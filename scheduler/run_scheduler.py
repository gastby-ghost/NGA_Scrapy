import subprocess
import time
import signal
import sys
import logging
import yaml
import os
from datetime import datetime, timedelta
from collections import deque
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.events import EVENT_JOB_EXECUTED, EVENT_JOB_ERROR
from pytz import timezone

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥è¿›ç¨‹é”æ¨¡å—
try:
    from NGA_Scrapy.utils.process_lock import ProcessLock, get_spider_status
except ImportError:
    logger = logging.getLogger(__name__)
    logger.error("æ— æ³•å¯¼å…¥ process_lock æ¨¡å—")
    sys.exit(1)

# è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆscheduler çš„çˆ¶ç›®å½•ï¼‰
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)

# å¯¼å…¥é‚®ä»¶é€šçŸ¥æ¨¡å—
try:
    from email_notifier import EmailNotifier, StatisticsCollector
except ImportError:
    logger = logging.getLogger(__name__)
    logger.warning("æ— æ³•å¯¼å…¥ email_notifier æ¨¡å—ï¼Œé‚®ä»¶é€šçŸ¥åŠŸèƒ½å°†ä¸å¯ç”¨")
    # åˆ›å»ºç©ºçš„ç±»ä»¥é¿å…å¯¼å…¥é”™è¯¯
    class EmailNotifier:
        def __init__(self, **kwargs):
            pass
        def send_alert(self, title, message, details=None):
            pass
        def send_statistics_report(self, stats):
            return False

    class StatisticsCollector:
        def collect_statistics(self, start_date, end_date):
            return None

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scheduler.log'),
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)

# å…¨å±€å˜é‡
spider_process = None
process_lock = None
shutdown_requested = False
email_notifier = None
stats_collector = None
consecutive_failures = 0
error_rates = deque(maxlen=100)  # ä¿å­˜æœ€è¿‘100æ¬¡æ‰§è¡Œçš„é”™è¯¯ç‡
spider_start_time = None
first_run_completed = False  # æ ‡è®°æ˜¯å¦å·²å®Œæˆç¬¬ä¸€æ¬¡çˆ¬è™«

# åŠ è½½é…ç½®æ–‡ä»¶
def load_config():
    """åŠ è½½é‚®ä»¶é…ç½®æ–‡ä»¶"""
    global email_notifier, stats_collector

    config_file = os.path.join(os.path.dirname(__file__), 'email_config.yaml')

    if not os.path.exists(config_file):
        logger.warning(f"é‚®ä»¶é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
        logger.info("è¯·å¤åˆ¶ email_config.yaml å¹¶é…ç½®ä½ çš„é‚®ç®±ä¿¡æ¯")
        return None

    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)

        # åˆ›å»ºé‚®ä»¶å‘é€å™¨
        if config.get('notifications', {}).get('enable_statistics_report') or \
           config.get('notifications', {}).get('enable_error_alerts'):

            email_notifier = EmailNotifier(
                smtp_server=config['smtp_server'],
                smtp_port=config['smtp_port'],
                username=config['username'],
                password=config['password'],
                from_email=config['from_email'],
                to_emails=config['to_emails'],
                use_tls=config.get('use_tls', True)
            )
            logger.info("é‚®ä»¶é€šçŸ¥å™¨åˆå§‹åŒ–æˆåŠŸ")

        # åˆ›å»ºç»Ÿè®¡æ”¶é›†å™¨
        stats_collector = StatisticsCollector()
        logger.info("ç»Ÿè®¡æ”¶é›†å™¨åˆå§‹åŒ–æˆåŠŸ")

        return config

    except Exception as e:
        logger.exception(f"åŠ è½½é…ç½®æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return None

def signal_handler(signum, frame):
    """ä¿¡å·å¤„ç†å™¨ï¼šä¼˜é›…å…³é—­è°ƒåº¦å™¨å’Œå­è¿›ç¨‹"""
    global shutdown_requested, process_lock
    logger.info(f"æ”¶åˆ°ä¿¡å· {signum}ï¼Œå¼€å§‹ä¼˜é›…å…³é—­...")
    shutdown_requested = True

    # ç»ˆæ­¢çˆ¬è™«å­è¿›ç¨‹
    if spider_process and spider_process.poll() is None:
        logger.info("æ­£åœ¨ç»ˆæ­¢çˆ¬è™«å­è¿›ç¨‹...")
        spider_process.terminate()

        # ç­‰å¾…è¿›ç¨‹ç»“æŸï¼Œæœ€å¤šç­‰å¾…10ç§’
        try:
            spider_process.wait(timeout=10)
            logger.info("çˆ¬è™«å­è¿›ç¨‹å·²ç»ˆæ­¢")
        except subprocess.TimeoutExpired:
            logger.warning("çˆ¬è™«å­è¿›ç¨‹æœªåœ¨10ç§’å†…ç»ˆæ­¢ï¼Œå¼ºåˆ¶æ€æ­»")
            spider_process.kill()
            spider_process.wait()

    # é‡Šæ”¾è¿›ç¨‹é”
    if process_lock:
        try:
            logger.info("æ­£åœ¨é‡Šæ”¾è¿›ç¨‹é”...")
            process_lock.release()
            logger.info("âœ… è¿›ç¨‹é”å·²é‡Šæ”¾")
        except Exception as e:
            logger.error(f"é‡Šæ”¾è¿›ç¨‹é”æ—¶å‡ºé”™: {e}")

    # å…³é—­è°ƒåº¦å™¨
    scheduler.shutdown(wait=False)
    logger.info("è°ƒåº¦å™¨å·²å…³é—­")
    sys.exit(0)

def run_spider():
    """æ‰§è¡Œçˆ¬è™«ä»»åŠ¡ï¼Œå¸¦é‡è¯•æœºåˆ¶å’ŒçŠ¶æ€ç›‘æ§"""
    global spider_process, process_lock, consecutive_failures, spider_start_time, error_rates, first_run_completed

    # æ£€æŸ¥æ˜¯å¦æœ‰å…³é—­è¯·æ±‚
    if shutdown_requested:
        logger.info("è·³è¿‡çˆ¬è™«æ‰§è¡Œï¼ˆå·²è¯·æ±‚å…³é—­ï¼‰")
        return

    # æ£€æŸ¥è¿›ç¨‹é”ï¼Œé˜²æ­¢å¹¶å‘è¿è¡Œ
    spider_status = get_spider_status()
    if spider_status['running']:
        duration = spider_status.get('duration', 0)
        pid = spider_status.get('pid')
        logger.warning(f"æ£€æµ‹åˆ°çˆ¬è™«å®ä¾‹æ­£åœ¨è¿è¡Œ (PID: {pid}, è¿è¡Œæ—¶é•¿: {duration:.1f}ç§’)ï¼Œè·³è¿‡æœ¬æ¬¡æ‰§è¡Œ")
        return

    # æ£€æŸ¥å½“å‰è¿›ç¨‹ç®¡ç†çš„çˆ¬è™«æ˜¯å¦è¿˜åœ¨è¿è¡Œ
    if spider_process and spider_process.poll() is None:
        logger.warning(f"å½“å‰è°ƒåº¦çš„çˆ¬è™«ä»åœ¨è¿è¡Œ (PID: {spider_process.pid})ï¼Œè·³è¿‡æœ¬æ¬¡æ‰§è¡Œ")
        return

    spider_start_time = datetime.now()
    execution_success = False
    error_output = []
    spider_stats = None

    try:
        logger.info("=" * 60)
        logger.info(f"å¼€å§‹æ‰§è¡Œçˆ¬è™«ä»»åŠ¡ - {datetime.now()}")
        logger.info("=" * 60)

        # è·å–è¿›ç¨‹é”
        lock_timeout = 7200  # 2å°æ—¶è¶…æ—¶
        process_lock = ProcessLock(timeout=lock_timeout)

        if not process_lock.acquire(blocking=False):
            logger.error("æ— æ³•è·å–è¿›ç¨‹é”ï¼Œå¯èƒ½æœ‰å…¶ä»–çˆ¬è™«å®ä¾‹æ­£åœ¨è¿è¡Œ")
            consecutive_failures += 1
            check_error_alerts(-1, ["æ— æ³•è·å–è¿›ç¨‹é”"])
            return

        logger.info("âœ… æˆåŠŸè·å–è¿›ç¨‹é”ï¼Œå¼€å§‹å¯åŠ¨çˆ¬è™«")

        # ä½¿ç”¨Popenä»£æ›¿run()ï¼Œé¿å…é˜»å¡
        # ä½¿ç”¨settings_cloudé…ç½®ï¼ˆäº‘æœåŠ¡å™¨ä¼˜åŒ–å‚æ•°ï¼‰
        spider_process = subprocess.Popen(
            ["scrapy", "crawl", "nga", "-s", "SETTINGS_MODULE=settings_cloud"],
            cwd=PROJECT_ROOT,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1
        )

        # å®æ—¶è¾“å‡ºçˆ¬è™«æ—¥å¿—
        logger.info(f"çˆ¬è™«è¿›ç¨‹å·²å¯åŠ¨ï¼ŒPID: {spider_process.pid}")

        # ç­‰å¾…è¿›ç¨‹å®Œæˆå¹¶è®°å½•è¾“å‡ºï¼ŒåŒæ—¶è§£æç»Ÿè®¡ä¿¡æ¯
        output_lines = []
        stats_dict_str = ""  # ç”¨äºç´¯ç§¯ç»Ÿè®¡å­—å…¸çš„å­—ç¬¦ä¸²
        in_stats_section = False  # æ˜¯å¦åœ¨ç»Ÿè®¡å­—å…¸éƒ¨åˆ†

        for line in spider_process.stdout:
            line_stripped = line.strip()
            output_lines.append(line_stripped)

            if line_stripped:
                logger.info(f"[Spider] {line_stripped}")

            # æ£€æµ‹ç»Ÿè®¡ä¿¡æ¯éƒ¨åˆ†çš„å¼€å§‹
            if "Dumping Scrapy stats:" in line_stripped:
                in_stats_section = True
                continue

            # å¦‚æœåœ¨ç»Ÿè®¡éƒ¨åˆ†ï¼Œç´¯ç§¯å­—å…¸å­—ç¬¦ä¸²
            if in_stats_section:
                stats_dict_str += line_stripped

                # å¦‚æœé‡åˆ°å®Œæ•´çš„å­—å…¸ï¼ˆä»¥}ç»“å°¾ï¼‰ï¼Œå°è¯•è§£æ
                if line_stripped.endswith("}"):
                    try:
                        import ast
                        spider_stats = ast.literal_eval(stats_dict_str)
                        logger.info(f"âœ… æˆåŠŸè§£æçˆ¬è™«ç»Ÿè®¡ä¿¡æ¯:")
                        logger.info(f"   - æŠ“å–é¡¹ç›®æ•°: {spider_stats.get('item_scraped_count', 0)}")
                        logger.info(f"   - ä¸‹è½½é¡µé¢æ•°: {spider_stats.get('downloader/response_count', 0)}")
                        logger.info(f"   - è¿è¡Œæ—¶é—´: {spider_stats.get('elapsed_time_seconds', 0):.2f}ç§’")
                        logger.info(f"   - å®ŒæˆçŠ¶æ€: {spider_stats.get('finish_reason', 'unknown')}")
                    except (ValueError, SyntaxError) as e:
                        logger.warning(f"âš ï¸ ç»Ÿè®¡ä¿¡æ¯è§£æå¤±è´¥: {e}")
                        logger.debug(f"åŸå§‹æ•°æ®: {stats_dict_str}")

                    # é‡ç½®çŠ¶æ€
                    in_stats_section = False
                    stats_dict_str = ""

        # ç­‰å¾…è¿›ç¨‹ç»“æŸ
        return_code = spider_process.wait()

        if return_code == 0:
            logger.info("=" * 60)
            logger.info(f"çˆ¬è™«ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ - {datetime.now()}")
            logger.info("=" * 60)
            execution_success = True
            consecutive_failures = 0  # é‡ç½®è¿ç»­å¤±è´¥è®¡æ•°

            # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡æˆåŠŸæ‰§è¡Œï¼Œå‘é€ç»Ÿè®¡æŠ¥å‘Š
            if not first_run_completed:
                logger.info("ç¬¬ä¸€æ¬¡çˆ¬è™«æ‰§è¡ŒæˆåŠŸï¼Œå¼€å§‹å‘é€ç»Ÿè®¡æŠ¥å‘Š...")
                send_statistics_report()
                first_run_completed = True
        else:
            logger.error(f"çˆ¬è™«ä»»åŠ¡æ‰§è¡Œå¤±è´¥ï¼Œè¿”å›ç : {return_code}")
            consecutive_failures += 1
            error_output = output_lines[-20:]  # ä¿å­˜æœ€å20è¡Œè¾“å‡ºä½œä¸ºé”™è¯¯ä¿¡æ¯

            if output_lines:
                logger.error("æœ€åè¾“å‡º:")
                for line in output_lines[-10:]:
                    logger.error(f"  {line}")

        spider_process = None

        # é‡Šæ”¾è¿›ç¨‹é”
        if process_lock:
            process_lock.release()
            process_lock = None
            logger.info("âœ… è¿›ç¨‹é”å·²é‡Šæ”¾")

        # å°è¯•ä»æ—¥å¿—æ–‡ä»¶ä¸­è·å–ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ›´å¯é çš„æ–¹æ³•ï¼‰
        if not spider_stats:
            spider_stats = _parse_stats_from_log()
            if spider_stats:
                logger.info(f"âœ… ä»æ—¥å¿—æ–‡ä»¶ä¸­æˆåŠŸè§£æçˆ¬è™«ç»Ÿè®¡ä¿¡æ¯:")
                logger.info(f"   - æŠ“å–é¡¹ç›®æ•°: {spider_stats.get('item_scraped_count', 0)}")
                logger.info(f"   - ä¸‹è½½é¡µé¢æ•°: {spider_stats.get('downloader/response_count', 0)}")
                logger.info(f"   - è¿è¡Œæ—¶é—´: {spider_stats.get('elapsed_time_seconds', 0):.2f}ç§’")
                logger.info(f"   - å®ŒæˆçŠ¶æ€: {spider_stats.get('finish_reason', 'unknown')}")

        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯åˆ°æ–‡ä»¶
        if spider_stats:
            _save_spider_statistics(spider_stats, return_code, execution_success)
        else:
            logger.warning("âš ï¸ æœªè·å–åˆ°çˆ¬è™«ç»Ÿè®¡ä¿¡æ¯")

        # è®¡ç®—é”™è¯¯ç‡ï¼ˆç®€åŒ–è®¡ç®—ï¼šåŸºäºè¿”å›ç ï¼‰
        error_rate = 0 if return_code == 0 else 100
        error_rates.append(error_rate)

        # æ£€æŸ¥é”™è¯¯å‘Šè­¦æ¡ä»¶
        check_error_alerts(return_code, error_output)

    except Exception as e:
        logger.exception(f"æ‰§è¡Œçˆ¬è™«ä»»åŠ¡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        consecutive_failures += 1
        error_output = [str(e)]
        spider_process = None

        # ç¡®ä¿é‡Šæ”¾è¿›ç¨‹é”
        if process_lock:
            try:
                process_lock.release()
                logger.info("âœ… å¼‚å¸¸æƒ…å†µä¸‹å·²é‡Šæ”¾è¿›ç¨‹é”")
            except Exception as lock_error:
                logger.error(f"é‡Šæ”¾è¿›ç¨‹é”æ—¶å‡ºé”™: {lock_error}")
            finally:
                process_lock = None

        # è®°å½•é”™è¯¯ç‡
        error_rates.append(100)

        # æ£€æŸ¥é”™è¯¯å‘Šè­¦æ¡ä»¶
        check_error_alerts(-1, error_output)


def _parse_stats_from_log():
    """ä»æ—¥å¿—æ–‡ä»¶ä¸­è§£ææœ€æ–°çš„çˆ¬è™«ç»Ÿè®¡ä¿¡æ¯"""
    try:
        import re
        import ast

        log_file = os.path.join(PROJECT_ROOT, "nga_spider.log")
        if not os.path.exists(log_file):
            logger.debug(f"æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {log_file}")
            return None

        # è¯»å–æ—¥å¿—æ–‡ä»¶çš„æœ€å200è¡Œï¼ˆé¿å…è¯»å–æ•´ä¸ªå¤§æ–‡ä»¶ï¼‰
        with open(log_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        # å¯»æ‰¾æœ€åçš„ "Dumping Scrapy stats:" è¡Œ
        stats_line_idx = None
        for i in range(len(lines) - 1, -1, -1):
            if "Dumping Scrapy stats:" in lines[i]:
                stats_line_idx = i
                break

        if stats_line_idx is None:
            logger.debug("æœªæ‰¾åˆ°ç»Ÿè®¡ä¿¡æ¯è¡Œ")
            return None

        # æ”¶é›†ä»ç»Ÿè®¡è¡Œå¼€å§‹çš„æ‰€æœ‰è¡Œï¼Œç›´åˆ°é‡åˆ°ç©ºè¡Œæˆ–éå­—å…¸æ ¼å¼
        stats_lines = []
        for i in range(stats_line_idx, len(lines)):
            line = lines[i].strip()
            if not line:
                break
            stats_lines.append(line)

        # åˆå¹¶æ‰€æœ‰è¡Œ
        stats_text = " ".join(stats_lines)

        # æå–å­—å…¸éƒ¨åˆ†ï¼ˆå¯»æ‰¾ä»¥ { å¼€å§‹ä»¥ } ç»“æŸçš„éƒ¨åˆ†ï¼‰
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å®Œæ•´çš„å­—å…¸
        dict_pattern = r"\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}"
        match = re.search(dict_pattern, stats_text)

        if match:
            stats_dict_str = match.group(0)
            try:
                # æ¸…ç†å­—ç¬¦ä¸²ï¼Œæ›¿æ¢ Python ç‰¹å®šçš„ç±»å‹æ ‡è®°
                stats_dict_str = stats_dict_str.replace("datetime.datetime", "")
                stats_dict_str = stats_dict_str.replace("datetime.timezone", "")
                stats_dict_str = re.sub(r"tzinfo=[^,)]*", "", stats_dict_str)

                # å°è¯•è§£æ
                stats_dict = ast.literal_eval(stats_dict_str)
                logger.debug(f"âœ… æˆåŠŸä»æ—¥å¿—è§£æç»Ÿè®¡ä¿¡æ¯")
                return stats_dict
            except (ValueError, SyntaxError) as e:
                logger.debug(f"âš ï¸ ç»Ÿè®¡ä¿¡æ¯è§£æå¤±è´¥: {e}")
                logger.debug(f"åŸå§‹æ•°æ®: {stats_dict_str[:200]}")
                return None
        else:
            logger.debug("æœªæ‰¾åˆ°ç»Ÿè®¡å­—å…¸æ ¼å¼")
            return None

    except Exception as e:
        logger.debug(f"è§£ææ—¥å¿—æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return None

def _save_spider_statistics(stats: dict, return_code: int, success: bool):
    """ä¿å­˜çˆ¬è™«ç»Ÿè®¡ä¿¡æ¯åˆ°æ–‡ä»¶"""
    try:
        import os
        import json
        from datetime import datetime

        # åˆ›å»ºç»Ÿè®¡ç›®å½•
        stats_dir = os.path.join(SCRIPT_DIR, "stats")
        os.makedirs(stats_dir, exist_ok=True)

        # ç”Ÿæˆæ–‡ä»¶åï¼ˆåŒ…å«æ—¶é—´æˆ³ï¼‰
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        stats_file = os.path.join(stats_dir, f"spider_stats_{timestamp}.json")

        # å‡†å¤‡è¦ä¿å­˜çš„æ•°æ®
        stats_data = {
            'timestamp': datetime.now().isoformat(),
            'return_code': return_code,
            'success': success,
            'spider_stats': stats,
            'summary': {
                'items_scraped': stats.get('item_scraped_count', 0),
                'pages_crawled': stats.get('downloader/response_count', 0),
                'runtime_seconds': stats.get('elapsed_time_seconds', 0),
                'response_bytes': stats.get('downloader/response_bytes', 0),
                'finish_reason': stats.get('finish_reason', 'unknown'),
                'success_rate': 100.0 if success else 0.0,
            }
        }

        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats_data, f, ensure_ascii=False, indent=2)

        logger.info(f"âœ… ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {stats_file}")

    except Exception as e:
        logger.exception(f"âŒ ä¿å­˜ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")

def check_error_alerts(return_code: int, error_output: list):
    """æ£€æŸ¥æ˜¯å¦éœ€è¦å‘é€é”™è¯¯å‘Šè­¦"""
    config = scheduler.config if hasattr(scheduler, 'config') else None

    if not config or not config.get('notifications', {}).get('enable_error_alerts', False):
        return

    notifications_config = config['notifications']

    # æ£€æŸ¥è¿ç»­å¤±è´¥æ¬¡æ•°
    threshold = notifications_config.get('consecutive_failures_threshold', 3)
    if consecutive_failures >= threshold:
        message = f"çˆ¬è™«è¿ç»­å¤±è´¥ {consecutive_failures} æ¬¡"
        details = "\n".join(error_output) if error_output else "æ— è¯¦ç»†é”™è¯¯ä¿¡æ¯"

        logger.error(f"å‘é€å‘Šè­¦é‚®ä»¶: {message}")
        if email_notifier:
            email_notifier.send_alert("çˆ¬è™«è¿ç»­å¤±è´¥", message, details)

    # æ£€æŸ¥è¶…æ—¶
    if spider_start_time:
        timeout_minutes = notifications_config.get('spider_timeout_minutes', 60)
        if (datetime.now() - spider_start_time).total_seconds() > timeout_minutes * 60:
            message = f"çˆ¬è™«è¿è¡Œæ—¶é—´è¶…è¿‡ {timeout_minutes} åˆ†é’Ÿ"
            details = f"å¼€å§‹æ—¶é—´: {spider_start_time}"

            logger.error(f"å‘é€å‘Šè­¦é‚®ä»¶: {message}")
            if email_notifier:
                email_notifier.send_alert("çˆ¬è™«è¿è¡Œè¶…æ—¶", message, details)

    # æ£€æŸ¥é”™è¯¯ç‡
    error_rate_config = notifications_config.get('error_rate_alert', {})
    if error_rate_config.get('enabled', False) and error_rates:
        # è®¡ç®—æœ€è¿‘Næ¬¡æ‰§è¡Œçš„å¹³å‡é”™è¯¯ç‡
        recent_rates = list(error_rates)
        avg_error_rate = sum(recent_rates) / len(recent_rates)
        threshold_percent = error_rate_config.get('threshold_percent', 20)

        if avg_error_rate >= threshold_percent:
            message = f"çˆ¬è™«é”™è¯¯ç‡è¾¾åˆ° {avg_error_rate:.1f}%ï¼Œè¶…è¿‡é˜ˆå€¼ {threshold_percent}%"
            details = f"æœ€è¿‘ {len(recent_rates)} æ¬¡æ‰§è¡Œçš„å¹³å‡é”™è¯¯ç‡: {avg_error_rate:.2f}%"

            logger.error(f"å‘é€å‘Šè­¦é‚®ä»¶: {message}")
            if email_notifier:
                email_notifier.send_alert("çˆ¬è™«é”™è¯¯ç‡è¿‡é«˜", message, details)

def send_statistics_report():
    """å‘é€ç»Ÿè®¡æŠ¥å‘Š"""
    global stats_collector

    if not email_notifier or not stats_collector:
        logger.warning("é‚®ä»¶é€šçŸ¥å™¨æˆ–ç»Ÿè®¡æ”¶é›†å™¨æœªåˆå§‹åŒ–ï¼Œè·³è¿‡ç»Ÿè®¡æŠ¥å‘Š")
        return

    config = scheduler.config if hasattr(scheduler, 'config') else None
    if not config or not config.get('notifications', {}).get('enable_statistics_report', False):
        return

    try:
        logger.info("å¼€å§‹ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š...")

        # è·å–æœ€è¿‘3å¤©çš„ç»Ÿè®¡æ•°æ®
        end_date = datetime.now()
        start_date = end_date - timedelta(days=3)

        stats = stats_collector.collect_statistics(start_date, end_date)

        if not stats:
            logger.warning("æœªèƒ½æ”¶é›†åˆ°ç»Ÿè®¡æ•°æ®")
            return

        # å‘é€é‚®ä»¶
        success = email_notifier.send_statistics_report(stats)

        if success:
            logger.info("ç»Ÿè®¡æŠ¥å‘Šå·²å‘é€")
        else:
            logger.error("å‘é€ç»Ÿè®¡æŠ¥å‘Šå¤±è´¥")

    except Exception as e:
        logger.exception(f"å‘é€ç»Ÿè®¡æŠ¥å‘Šæ—¶å‘ç”Ÿé”™è¯¯: {e}")

def job_listener(event):
    """ç›‘å¬ä»»åŠ¡æ‰§è¡Œäº‹ä»¶"""
    if hasattr(event, 'exception') and event.exception:
        logger.error(f"ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {event.exception}")
    else:
        logger.info(f"ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ: {event.job_id}")

if __name__ == '__main__':
    # å¯åŠ¨æ—¶æ¸…ç©ºæ—¥å¿—æ–‡ä»¶
    log_file = os.path.join(os.path.dirname(__file__), 'scheduler.log')
    if os.path.exists(log_file):
        open(log_file, 'w', encoding='utf-8').close()
        print(f"å·²æ¸…ç©ºæ—¥å¿—æ–‡ä»¶: {log_file}")

    # æ£€æµ‹æ˜¯å¦åœ¨screenä¸­è¿è¡Œ
    in_screen = os.environ.get('STY') is not None
    if in_screen:
        print("\n" + "=" * 60)
        print("ğŸ“º æ£€æµ‹åˆ°åœ¨Screenä¼šè¯ä¸­è¿è¡Œ")
        print("   ä¼šè¯åç§°:", os.environ.get('STY', 'æœªçŸ¥'))
        print("   æç¤º: æŒ‰ Ctrl+\\ å¯ä¼˜é›…é€€å‡º")
        print("=" * 60 + "\n")

    # æ³¨å†Œä¿¡å·å¤„ç†å™¨
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    # åŠ è½½é…ç½®
    config = load_config()

    # æ˜¾ç¤ºå¯åŠ¨ä¿¡æ¯
    logger.info("=" * 60)
    logger.info("NGA çˆ¬è™«è°ƒåº¦å™¨å¯åŠ¨")
    logger.info("=" * 60)
    logger.info("é…ç½®ä¿¡æ¯:")
    logger.info("  - æ—¶åŒº: Asia/Shanghai")
    logger.info("  - æ‰§è¡Œé—´éš”: 30 åˆ†é’Ÿ")
    logger.info("  - å¯åŠ¨æ—¶é—´: {}".format(datetime.now()))
    logger.info("  - æ—¥å¿—æ–‡ä»¶: scheduler.log")

    if config:
        logger.info("  - é‚®ä»¶é€šçŸ¥: å·²å¯ç”¨")
        logger.info("  - ç»Ÿè®¡æŠ¥å‘Š: {}".format(
            "å¯ç”¨" if config.get('notifications', {}).get('enable_statistics_report') else "ç¦ç”¨"
        ))
        if config.get('notifications', {}).get('enable_statistics_report'):
            logger.info("  - é¦–æ¬¡ç»Ÿè®¡æŠ¥å‘Š: å°†åœ¨ç¬¬ä¸€æ¬¡çˆ¬è™«æˆåŠŸåå‘é€")
        logger.info("  - é”™è¯¯å‘Šè­¦: {}".format(
            "å¯ç”¨" if config.get('notifications', {}).get('enable_error_alerts') else "ç¦ç”¨"
        ))
    else:
        logger.info("  - é‚®ä»¶é€šçŸ¥: æœªé…ç½®")

    logger.info("=" * 60)
    logger.info("æŒ‰ Ctrl+C ä¼˜é›…é€€å‡º")
    logger.info("=" * 60)

    # åˆ›å»ºè°ƒåº¦å™¨
    tz = timezone('Asia/Shanghai')
    scheduler = BackgroundScheduler(timezone=tz)
    scheduler.config = config  # å°†é…ç½®é™„åŠ åˆ°è°ƒåº¦å™¨å¯¹è±¡

    # æ·»åŠ çˆ¬è™«ä»»åŠ¡
    scheduler.add_job(
        run_spider,
        'interval',
        minutes=30,
        id='nga_spider_job',
        next_run_time=datetime.now(tz),
        max_instances=1,
        coalesce=True,
        misfire_grace_time=300
    )

    # æ·»åŠ ç»Ÿè®¡æŠ¥å‘Šä»»åŠ¡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if config and config.get('notifications', {}).get('enable_statistics_report', False):
        interval_days = config.get('notifications', {}).get('statistics_report_interval_days', 3)
        report_time = config.get('notifications', {}).get('statistics_report_time', '09:00')

        # è§£ææ—¶é—´
        hour, minute = map(int, report_time.split(':'))

        scheduler.add_job(
            send_statistics_report,
            'cron',
            hour=hour,
            minute=minute,
            id='statistics_report_job',
            day_of_week='*/{}'.format(interval_days),  # æ¯3å¤©æ‰§è¡Œä¸€æ¬¡
            timezone=tz
        )
        logger.info(f"ç»Ÿè®¡æŠ¥å‘Šä»»åŠ¡å·²æ·»åŠ : æ¯ {interval_days} å¤©æ‰§è¡Œä¸€æ¬¡ï¼Œæ—¶é—´: {report_time}")
        logger.info("é¦–æ¬¡ç»Ÿè®¡æŠ¥å‘Šå°†åœ¨ç¬¬ä¸€æ¬¡çˆ¬è™«æˆåŠŸåè‡ªåŠ¨å‘é€")

    # æ·»åŠ ä»»åŠ¡ç›‘å¬å™¨
    scheduler.add_listener(job_listener, EVENT_JOB_EXECUTED | EVENT_JOB_ERROR)

    # å¯åŠ¨è°ƒåº¦å™¨
    scheduler.start()

    # å®šæœŸæ¸…ç†è¿‡æœŸé”
    def cleanup_stale_locks():
        """å®šæœŸæ¸…ç†è¿‡æœŸçš„é”æ–‡ä»¶"""
        try:
            lock = ProcessLock(timeout=3600)  # 1å°æ—¶è¶…æ—¶
            if lock._cleanup_stale_lock():
                logger.info("âœ… æ¸…ç†è¿‡æœŸé”å®Œæˆ")
        except Exception as e:
            logger.error(f"æ¸…ç†è¿‡æœŸé”æ—¶å‡ºé”™: {e}")

    # æ·»åŠ é”æ¸…ç†ä»»åŠ¡ï¼ˆæ¯10åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡ï¼‰
    scheduler.add_job(
        cleanup_stale_locks,
        'interval',
        minutes=10,
        id='cleanup_locks_job',
        max_instances=1
    )
    logger.info("é”æ¸…ç†ä»»åŠ¡å·²æ·»åŠ ï¼šæ¯10åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡")

    # ä¸»å¾ªç¯
    try:
        while True:
            if not shutdown_requested:
                jobs = scheduler.get_jobs()
                next_run = None
                for job in jobs:
                    if job.id == 'nga_spider_job':
                        next_run = job.next_run_time
                        break

                # è·å–çˆ¬è™«çŠ¶æ€ä¿¡æ¯
                spider_status = get_spider_status()
                if spider_process and spider_process.poll() is None:
                    logger.debug(f"è°ƒåº¦å™¨è¿è¡Œä¸­ | çˆ¬è™«PID: {spider_process.pid} | ä¸‹æ¬¡æ‰§è¡Œ: {next_run}")
                elif spider_status['running']:
                    logger.debug(f"è°ƒåº¦å™¨è¿è¡Œä¸­ | å¤–éƒ¨çˆ¬è™«PID: {spider_status.get('pid')} | è¿è¡Œæ—¶é•¿: {spider_status.get('duration', 0):.1f}ç§’ | ä¸‹æ¬¡æ‰§è¡Œ: {next_run}")
                else:
                    logger.debug(f"è°ƒåº¦å™¨è¿è¡Œä¸­ | ä¸‹æ¬¡æ‰§è¡Œ: {next_run}")

            time.sleep(30)

    except KeyboardInterrupt:
        logger.info("æ”¶åˆ°é”®ç›˜ä¸­æ–­ä¿¡å·")
        signal_handler(signal.SIGINT, None)
