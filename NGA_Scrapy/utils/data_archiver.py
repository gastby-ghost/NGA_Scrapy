#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®å½’æ¡£ç®¡ç†æ¨¡å—

è‡ªåŠ¨æ¸…ç†å’Œå½’æ¡£è¿‡æœŸæ•°æ®ï¼Œä¿æŒæ•°æ®åº“æ€§èƒ½ï¼Œä¼˜åŒ–å­˜å‚¨ç©ºé—´ã€‚
æ¯æœˆæ‰§è¡Œä¸€æ¬¡ï¼Œå½’æ¡£1ä¸ªæœˆæœªæ›´æ–°çš„ä¸»é¢˜åŠå…¶å¯¹åº”å›å¤å’Œç”¨æˆ·ã€‚

ä¸»è¦åŠŸèƒ½:
1. æŒ‰ä¸»é¢˜å…³è”å½’æ¡£ï¼ˆä¸»é¢˜+å›å¤+ç”¨æˆ·ï¼‰
2. æ•°æ®ç”Ÿå‘½å‘¨æœŸç®¡ç†
3. å½’æ¡£æ€§èƒ½ä¼˜åŒ–
4. æ•°æ®æ¢å¤æœºåˆ¶

ä½œè€…: Claude Code
æ—¥æœŸ: 2025-12-07
"""

import os
import json
import logging
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Set
from pathlib import Path
from threading import Lock


class DataArchiver:
    """æ•°æ®å½’æ¡£ç®¡ç†å™¨

    æ¯æœˆå½’æ¡£1ä¸ªæœˆæœªæ›´æ–°çš„ä¸»é¢˜åŠå…¶å…³è”çš„å›å¤å’Œç”¨æˆ·
    """

    def __init__(self, db_session, archive_dir='./archive', config=None):
        """åˆå§‹åŒ–æ•°æ®å½’æ¡£å™¨

        Args:
            db_session: SQLAlchemyä¼šè¯
            archive_dir: å½’æ¡£å­˜å‚¨ç›®å½•
            config: é…ç½®å­—å…¸
        """
        self.db_session = db_session
        self.archive_dir = Path(archive_dir)
        self.config = config or {}

        self.logger = logging.getLogger(__name__)

        # ç¡®ä¿å½’æ¡£ç›®å½•å­˜åœ¨
        self.archive_dir.mkdir(parents=True, exist_ok=True)

        # å½’æ¡£é…ç½® - ç®€åŒ–ä¸ºæŒ‰ä¸»é¢˜å…³è”å½’æ¡£
        self.archive_config = {
            'enabled': self.config.get('enabled', True),
            # ä¸»é¢˜å½’æ¡£é˜ˆå€¼ï¼š1ä¸ªæœˆï¼ˆ30å¤©ï¼‰æœªæ›´æ–°
            'archive_threshold_days': self.config.get('archive_threshold_days', 30),
            # å½’æ¡£æ–‡ä»¶ä¿ç•™æœŸï¼ˆå¤©ï¼‰
            'archive_retention_days': self.config.get('archive_retention_days', 365),
            # æ¯æ‰¹å½’æ¡£çš„ä¸»é¢˜æ•°é‡
            'archive_batch_size': self.config.get('archive_batch_size', 500),
        }

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'archived_topics': 0,
            'archived_replies': 0,
            'archived_users': 0,
            'archive_operations': 0,
            'total_archived_size': 0,  # MB
            'last_archive_time': None,
        }

        self.lock = Lock()

    def get_topics_to_archive(self) -> List[str]:
        """è·å–éœ€è¦å½’æ¡£çš„ä¸»é¢˜TIDåˆ—è¡¨

        æŸ¥æ‰¾1ä¸ªæœˆæœªæ›´æ–°çš„ä¸»é¢˜

        Returns:
            List[str]: éœ€è¦å½’æ¡£çš„ä¸»é¢˜TIDåˆ—è¡¨
        """
        try:
            from ..models import Topic

            threshold_days = self.archive_config['archive_threshold_days']
            cutoff_date = datetime.now() - timedelta(days=threshold_days)
            batch_size = self.archive_config['archive_batch_size']

            # æŸ¥è¯¢1ä¸ªæœˆæœªæ›´æ–°çš„ä¸»é¢˜
            topics = (
                self.db_session.query(Topic.tid)
                .filter(Topic.last_reply_date < cutoff_date.strftime('%Y-%m-%d %H:%M:%S'))
                .limit(batch_size)
                .all()
            )

            tids = [topic.tid for topic in topics]
            self.logger.info(f"æ‰¾åˆ° {len(tids)} ä¸ªéœ€è¦å½’æ¡£çš„ä¸»é¢˜ï¼ˆ{threshold_days}å¤©æœªæ›´æ–°ï¼‰")
            return tids

        except Exception as e:
            self.logger.error(f"è·å–å½’æ¡£ä¸»é¢˜å¤±è´¥: {e}")
            return []

    def archive_topics_with_related(self, tids: List[str]) -> Dict[str, int]:
        """å½’æ¡£ä¸»é¢˜åŠå…¶å…³è”çš„å›å¤å’Œç”¨æˆ·

        Args:
            tids: ä¸»é¢˜TIDåˆ—è¡¨

        Returns:
            Dict[str, int]: å½’æ¡£ç»“æœç»Ÿè®¡
        """
        if not tids:
            return {'topics': 0, 'replies': 0, 'users': 0, 'failed': 0}

        try:
            start_time = time.time()

            # 1. å¯¼å‡ºä¸»é¢˜æ•°æ®
            topics_data = self._export_topics(tids)
            if not topics_data:
                return {'topics': 0, 'replies': 0, 'users': 0, 'failed': len(tids)}

            # 2. è·å–å¹¶å¯¼å‡ºå…³è”çš„å›å¤
            replies_data = self._export_replies_by_tids(tids)

            # 3. æ”¶é›†å¹¶å¯¼å‡ºå…³è”çš„ç”¨æˆ·
            user_ids = self._collect_user_ids(topics_data, replies_data)
            users_data = self._export_users(user_ids)

            # 4. åˆ›å»ºå½’æ¡£æ–‡ä»¶
            archive_file = self._create_archive_file()
            if not archive_file:
                return {'topics': 0, 'replies': 0, 'users': 0, 'failed': len(tids)}

            # 5. ä¿å­˜å½’æ¡£æ•°æ®
            archive_data = {
                'archive_time': datetime.now().isoformat(),
                'archive_type': 'monthly_topic_archive',
                'threshold_days': self.archive_config['archive_threshold_days'],
                'topics': topics_data,
                'replies': replies_data,
                'users': users_data,
                'summary': {
                    'topic_count': len(topics_data.get('data', [])),
                    'reply_count': len(replies_data.get('data', [])),
                    'user_count': len(users_data.get('data', [])),
                }
            }

            try:
                with open(archive_file, 'w', encoding='utf-8') as f:
                    json.dump(archive_data, f, ensure_ascii=False, indent=2)
            except Exception as e:
                self.logger.error(f"ä¿å­˜å½’æ¡£æ–‡ä»¶å¤±è´¥: {e}")
                return {'topics': 0, 'replies': 0, 'users': 0, 'failed': len(tids)}

            # 6. ä»æ•°æ®åº“åˆ é™¤å·²å½’æ¡£æ•°æ®
            deleted_topics = self._delete_topics(tids)
            deleted_replies = self._delete_replies_by_tids(tids)
            # ç”¨æˆ·ä¸åˆ é™¤ï¼Œåªå½’æ¡£å¤‡ä»½

            elapsed = time.time() - start_time

            # æ›´æ–°ç»Ÿè®¡
            with self.lock:
                self.stats['archived_topics'] += deleted_topics
                self.stats['archived_replies'] += deleted_replies
                self.stats['archived_users'] += len(users_data.get('data', []))
                self.stats['archive_operations'] += 1
                self.stats['last_archive_time'] = datetime.now().isoformat()
                self.stats['total_archived_size'] += os.path.getsize(archive_file) / 1024 / 1024

            self.logger.info(
                f"ğŸ“¦ å½’æ¡£å®Œæˆ: {deleted_topics}ä¸ªä¸»é¢˜, {deleted_replies}ä¸ªå›å¤, "
                f"{len(users_data.get('data', []))}ä¸ªç”¨æˆ·å¤‡ä»½, è€—æ—¶ {elapsed:.2f}s"
            )

            return {
                'topics': deleted_topics,
                'replies': deleted_replies,
                'users': len(users_data.get('data', [])),
                'failed': len(tids) - deleted_topics,
                'archive_file': str(archive_file)
            }

        except Exception as e:
            self.logger.error(f"å½’æ¡£æ“ä½œå¤±è´¥: {e}")
            return {'topics': 0, 'replies': 0, 'users': 0, 'failed': len(tids)}

    def _export_topics(self, tids: List[str]) -> Dict:
        """å¯¼å‡ºä¸»é¢˜æ•°æ®"""
        try:
            from ..models import Topic

            topics = (
                self.db_session.query(Topic)
                .filter(Topic.tid.in_(tids))
                .all()
            )

            return {
                'export_type': 'topic',
                'count': len(topics),
                'data': [
                    {
                        'tid': topic.tid,
                        'title': topic.title,
                        'poster_id': topic.poster_id,
                        'post_time': topic.post_time,
                        're_num': topic.re_num,
                        'sampling_time': topic.sampling_time,
                        'last_reply_date': topic.last_reply_date,
                        'partition': topic.partition,
                    }
                    for topic in topics
                ]
            }

        except Exception as e:
            self.logger.error(f"å¯¼å‡ºä¸»é¢˜æ•°æ®å¤±è´¥: {e}")
            return {}

    def _export_replies_by_tids(self, tids: List[str]) -> Dict:
        """æ ¹æ®ä¸»é¢˜TIDå¯¼å‡ºå…³è”çš„å›å¤"""
        try:
            from ..models import Reply

            replies = (
                self.db_session.query(Reply)
                .filter(Reply.tid.in_(tids))
                .all()
            )

            return {
                'export_type': 'reply',
                'count': len(replies),
                'data': [
                    {
                        'rid': reply.rid,
                        'tid': reply.tid,
                        'parent_rid': reply.parent_rid,
                        'content': reply.content,
                        'recommendvalue': reply.recommendvalue,
                        'poster_id': reply.poster_id,
                        'post_time': reply.post_time,
                        'image_urls': reply.image_urls,
                        'image_paths': reply.image_paths,
                        'sampling_time': reply.sampling_time,
                    }
                    for reply in replies
                ]
            }

        except Exception as e:
            self.logger.error(f"å¯¼å‡ºå›å¤æ•°æ®å¤±è´¥: {e}")
            return {}

    def _collect_user_ids(self, topics_data: Dict, replies_data: Dict) -> Set[str]:
        """æ”¶é›†ä¸»é¢˜å’Œå›å¤ä¸­çš„ç”¨æˆ·ID"""
        user_ids = set()

        # ä»ä¸»é¢˜ä¸­æ”¶é›†
        for topic in topics_data.get('data', []):
            if topic.get('poster_id'):
                user_ids.add(topic['poster_id'])

        # ä»å›å¤ä¸­æ”¶é›†
        for reply in replies_data.get('data', []):
            if reply.get('poster_id'):
                user_ids.add(reply['poster_id'])

        return user_ids

    def _export_users(self, user_ids: Set[str]) -> Dict:
        """å¯¼å‡ºç”¨æˆ·æ•°æ®ï¼ˆä»…å¤‡ä»½ï¼Œä¸åˆ é™¤ï¼‰"""
        if not user_ids:
            return {'export_type': 'user', 'count': 0, 'data': []}

        try:
            from ..models import User

            users = (
                self.db_session.query(User)
                .filter(User.uid.in_(list(user_ids)))
                .all()
            )

            return {
                'export_type': 'user',
                'count': len(users),
                'data': [
                    {
                        'uid': user.uid,
                        'name': user.name,
                        'user_group': user.user_group,
                        'prestige': user.prestige,
                        'reg_date': user.reg_date,
                        'history_re_num': user.history_re_num,
                    }
                    for user in users
                ]
            }

        except Exception as e:
            self.logger.error(f"å¯¼å‡ºç”¨æˆ·æ•°æ®å¤±è´¥: {e}")
            return {}

    def _create_archive_file(self) -> Optional[Path]:
        """åˆ›å»ºå½’æ¡£æ–‡ä»¶"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            archive_file = self.archive_dir / f"monthly_archive_{timestamp}.json"
            return archive_file
        except Exception as e:
            self.logger.error(f"åˆ›å»ºå½’æ¡£æ–‡ä»¶å¤±è´¥: {e}")
            return None

    def _delete_topics(self, tids: List[str]) -> int:
        """åˆ é™¤ä¸»é¢˜"""
        try:
            from ..models import Topic

            deleted = (
                self.db_session.query(Topic)
                .filter(Topic.tid.in_(tids))
                .delete(synchronize_session=False)
            )
            self.db_session.commit()
            return deleted

        except Exception as e:
            self.logger.error(f"åˆ é™¤ä¸»é¢˜å¤±è´¥: {e}")
            self.db_session.rollback()
            return 0

    def _delete_replies_by_tids(self, tids: List[str]) -> int:
        """åˆ é™¤ä¸»é¢˜å…³è”çš„å›å¤"""
        try:
            from ..models import Reply

            deleted = (
                self.db_session.query(Reply)
                .filter(Reply.tid.in_(tids))
                .delete(synchronize_session=False)
            )
            self.db_session.commit()
            return deleted

        except Exception as e:
            self.logger.error(f"åˆ é™¤å›å¤å¤±è´¥: {e}")
            self.db_session.rollback()
            return 0

    def auto_archive(self) -> Dict[str, int]:
        """è‡ªåŠ¨æ‰§è¡Œæœˆåº¦æ•°æ®å½’æ¡£

        Returns:
            Dict[str, int]: å½’æ¡£ç»Ÿè®¡
        """
        if not self.archive_config['enabled']:
            self.logger.info("æ•°æ®å½’æ¡£å·²ç¦ç”¨")
            return {'topics': 0, 'replies': 0, 'users': 0, 'failed': 0}

        self.logger.info("ğŸ”„ å¼€å§‹æœˆåº¦æ•°æ®å½’æ¡£...")

        # è·å–éœ€è¦å½’æ¡£çš„ä¸»é¢˜
        tids = self.get_topics_to_archive()

        if not tids:
            self.logger.info("æ²¡æœ‰éœ€è¦å½’æ¡£çš„æ•°æ®")
            return {'topics': 0, 'replies': 0, 'users': 0, 'failed': 0}

        # æ‰§è¡Œå…³è”å½’æ¡£
        result = self.archive_topics_with_related(tids)

        return result

    def restore_archive(self, archive_file: str) -> Dict[str, int]:
        """ä»å½’æ¡£æ–‡ä»¶æ¢å¤æ•°æ®

        Args:
            archive_file: å½’æ¡£æ–‡ä»¶è·¯å¾„

        Returns:
            Dict[str, int]: æ¢å¤ç»“æœç»Ÿè®¡
        """
        try:
            if not os.path.exists(archive_file):
                self.logger.error(f"å½’æ¡£æ–‡ä»¶ä¸å­˜åœ¨: {archive_file}")
                return {'topics': 0, 'replies': 0, 'users': 0, 'failed': 0}

            with open(archive_file, 'r', encoding='utf-8') as f:
                archive_data = json.load(f)

            restored_topics = 0
            restored_replies = 0
            restored_users = 0

            # æ¢å¤ä¸»é¢˜
            for topic in archive_data.get('topics', {}).get('data', []):
                try:
                    self._restore_topic(topic)
                    restored_topics += 1
                except Exception as e:
                    self.logger.error(f"æ¢å¤ä¸»é¢˜å¤±è´¥: {e}")

            # æ¢å¤å›å¤
            for reply in archive_data.get('replies', {}).get('data', []):
                try:
                    self._restore_reply(reply)
                    restored_replies += 1
                except Exception as e:
                    self.logger.error(f"æ¢å¤å›å¤å¤±è´¥: {e}")

            # æ¢å¤ç”¨æˆ·ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            for user in archive_data.get('users', {}).get('data', []):
                try:
                    self._restore_user(user)
                    restored_users += 1
                except Exception as e:
                    self.logger.error(f"æ¢å¤ç”¨æˆ·å¤±è´¥: {e}")

            self.db_session.commit()

            self.logger.info(
                f"ğŸ”„ æ¢å¤å®Œæˆ: {restored_topics}ä¸ªä¸»é¢˜, "
                f"{restored_replies}ä¸ªå›å¤, {restored_users}ä¸ªç”¨æˆ·"
            )

            return {
                'topics': restored_topics,
                'replies': restored_replies,
                'users': restored_users,
                'failed': 0
            }

        except Exception as e:
            self.logger.error(f"æ¢å¤å½’æ¡£æ•°æ®å¤±è´¥: {e}")
            self.db_session.rollback()
            return {'topics': 0, 'replies': 0, 'users': 0, 'failed': 1}

    def _restore_topic(self, topic_data: Dict):
        """æ¢å¤ä¸»é¢˜æ•°æ®"""
        from ..models import Topic
        topic = Topic(**topic_data)
        self.db_session.merge(topic)

    def _restore_reply(self, reply_data: Dict):
        """æ¢å¤å›å¤æ•°æ®"""
        from ..models import Reply
        reply = Reply(**reply_data)
        self.db_session.merge(reply)

    def _restore_user(self, user_data: Dict):
        """æ¢å¤ç”¨æˆ·æ•°æ®"""
        from ..models import User
        user = User(**user_data)
        self.db_session.merge(user)

    def cleanup_old_archives(self, retention_days: int = None) -> int:
        """æ¸…ç†è¿‡æœŸçš„å½’æ¡£æ–‡ä»¶

        Args:
            retention_days: å½’æ¡£æ–‡ä»¶ä¿ç•™å¤©æ•°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼

        Returns:
            int: æ¸…ç†çš„æ–‡ä»¶æ•°é‡
        """
        try:
            if retention_days is None:
                retention_days = self.archive_config['archive_retention_days']

            cutoff_date = datetime.now() - timedelta(days=retention_days)
            cleaned_count = 0

            for archive_file in self.archive_dir.glob("*.json"):
                file_mtime = datetime.fromtimestamp(archive_file.stat().st_mtime)

                if file_mtime < cutoff_date:
                    archive_file.unlink()
                    cleaned_count += 1
                    self.logger.debug(f"åˆ é™¤è¿‡æœŸå½’æ¡£æ–‡ä»¶: {archive_file}")

            if cleaned_count > 0:
                self.logger.info(f"ğŸ§¹ æ¸…ç†äº† {cleaned_count} ä¸ªè¿‡æœŸå½’æ¡£æ–‡ä»¶")

            return cleaned_count

        except Exception as e:
            self.logger.error(f"æ¸…ç†å½’æ¡£æ–‡ä»¶å¤±è´¥: {e}")
            return 0

    def get_archive_stats(self) -> Dict:
        """è·å–å½’æ¡£ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            total_size = sum(
                f.stat().st_size for f in self.archive_dir.glob("*.json")
            ) / 1024 / 1024  # MB

            return {
                'archived_topics': self.stats['archived_topics'],
                'archived_replies': self.stats['archived_replies'],
                'archived_users': self.stats['archived_users'],
                'archive_operations': self.stats['archive_operations'],
                'total_archived_size_mb': f"{self.stats['total_archived_size']:.2f}",
                'current_archive_size_mb': f"{total_size:.2f}",
                'last_archive_time': self.stats['last_archive_time'],
                'archive_files': len(list(self.archive_dir.glob("*.json"))),
                'config': self.archive_config,
            }

    def generate_archive_report(self) -> str:
        """ç”Ÿæˆå½’æ¡£æŠ¥å‘Š"""
        stats = self.get_archive_stats()

        report = []
        report.append("=" * 60)
        report.append("ğŸ“¦ æœˆåº¦æ•°æ®å½’æ¡£æŠ¥å‘Š")
        report.append("=" * 60)
        report.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")

        report.append("ğŸ“Š å½’æ¡£ç»Ÿè®¡:")
        report.append(f"  å·²å½’æ¡£ä¸»é¢˜: {stats['archived_topics']:,} ä¸ª")
        report.append(f"  å·²å½’æ¡£å›å¤: {stats['archived_replies']:,} ä¸ª")
        report.append(f"  å·²å¤‡ä»½ç”¨æˆ·: {stats['archived_users']:,} ä¸ª")
        report.append(f"  å½’æ¡£æ“ä½œ: {stats['archive_operations']} æ¬¡")
        report.append("")

        report.append("ğŸ’¾ å­˜å‚¨ç»Ÿè®¡:")
        report.append(f"  å†å²å½’æ¡£å¤§å°: {stats['total_archived_size_mb']} MB")
        report.append(f"  å½“å‰å½’æ¡£å¤§å°: {stats['current_archive_size_mb']} MB")
        report.append(f"  å½’æ¡£æ–‡ä»¶æ•°: {stats['archive_files']} ä¸ª")
        report.append("")

        if stats['last_archive_time']:
            report.append(f"â° æœ€åå½’æ¡£æ—¶é—´: {stats['last_archive_time']}")
            report.append("")

        report.append("âš™ï¸ å½’æ¡£é…ç½®:")
        report.append(f"  å¯ç”¨çŠ¶æ€: {stats['config']['enabled']}")
        report.append(f"  å½’æ¡£é˜ˆå€¼: {stats['config']['archive_threshold_days']} å¤©æœªæ›´æ–°")
        report.append(f"  å½’æ¡£ä¿ç•™æœŸ: {stats['config']['archive_retention_days']} å¤©")
        report.append(f"  æ‰¹å¤„ç†å¤§å°: {stats['config']['archive_batch_size']} ä¸ªä¸»é¢˜/æ‰¹")
        report.append("")

        report.append("=" * 60)
        return "\n".join(report)


# ä¾¿æ·å‡½æ•°
def create_data_archiver(db_session, archive_dir='./archive', config=None):
    """åˆ›å»ºæ•°æ®å½’æ¡£å™¨å®ä¾‹"""
    return DataArchiver(db_session, archive_dir, config)


def run_monthly_archive(db_session, archive_dir='./archive', config=None):
    """ä¾¿æ·å‡½æ•°ï¼šæ‰§è¡Œæœˆåº¦å½’æ¡£"""
    archiver = DataArchiver(db_session, archive_dir, config)
    return archiver.auto_archive()


if __name__ == "__main__":
    from sqlalchemy import create_engine
    from sqlalchemy.orm import sessionmaker

    engine = create_engine('sqlite:///:memory:')
    Session = sessionmaker(bind=engine)
    session = Session()

    archiver = DataArchiver(session, './test_archive', {
        'enabled': True,
        'archive_threshold_days': 30,
    })

    print(archiver.generate_archive_report())
