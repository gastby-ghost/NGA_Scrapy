#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æŸ¥è¯¢ä¼˜åŒ–å™¨æ¨¡å—

æä¾›é«˜æ•ˆçš„æ•°æ®åº“æŸ¥è¯¢ç­–ç•¥ï¼ŒåŒ…æ‹¬EXISTSæ›¿ä»£INæŸ¥è¯¢ã€å¢é‡åŒæ­¥æœºåˆ¶ç­‰ã€‚

ä¸»è¦åŠŸèƒ½:
1. EXISTSæŸ¥è¯¢ä¼˜åŒ–ï¼ˆæ›¿ä»£INæŸ¥è¯¢ï¼‰
2. å¢é‡åŒæ­¥æœºåˆ¶
3. æŸ¥è¯¢è®¡åˆ’åˆ†æ
4. æ—¶é—´æˆ³ä¼˜åŒ–æŸ¥è¯¢
5. æ‰¹é‡å­˜åœ¨æ€§æ£€æŸ¥

ä½œè€…: Claude Code
æ—¥æœŸ: 2025-12-07
"""

import time
import logging
from sqlalchemy import exists, and_, or_
from sqlalchemy.orm import Session
from sqlalchemy.exc import SQLAlchemyError


class QueryOptimizer:
    """æŸ¥è¯¢ä¼˜åŒ–å™¨"""

    def __init__(self, db_session: Session, logger=None):
        """åˆå§‹åŒ–æŸ¥è¯¢ä¼˜åŒ–å™¨

        Args:
            db_session: SQLAlchemyä¼šè¯
            logger: æ—¥å¿—è®°å½•å™¨
        """
        self.db_session = db_session
        self.logger = logger or logging.getLogger(__name__)

    def check_topics_exist_exists(self, tids):
        """ä½¿ç”¨EXISTSæŸ¥è¯¢æ£€æŸ¥ä¸»é¢˜æ˜¯å¦å­˜åœ¨ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰

        å¯¹äºå¤§é‡TIDï¼ŒEXISTSæŸ¥è¯¢æ¯”INæŸ¥è¯¢æ›´é«˜æ•ˆï¼Œç‰¹åˆ«æ˜¯å½“è¡¨æœ‰ç´¢å¼•æ—¶ã€‚

        Args:
            tids: ä¸»é¢˜IDåˆ—è¡¨

        Returns:
            set: å­˜åœ¨çš„ä¸»é¢˜IDé›†åˆ
        """
        if not tids:
            return set()

        try:
            from ..models import Topic
            start_time = time.time()

            # ä½¿ç”¨EXISTSå­æŸ¥è¯¢æ£€æŸ¥æ¯ä¸ªTIDæ˜¯å¦å­˜åœ¨
            # è¿™ç§æ–¹å¼å¯¹äºå¤§é‡æ•°æ®æ›´é«˜æ•ˆ
            existing_tids = set()

            # å¯¹äºå°‘é‡TIDï¼Œä½¿ç”¨INæŸ¥è¯¢
            if len(tids) <= 10:
                topics = self.db_session.query(Topic.tid).filter(Topic.tid.in_(tids)).all()
                existing_tids = {topic.tid for topic in topics}
            else:
                # å¯¹äºå¤§é‡TIDï¼Œä½¿ç”¨åˆ†æ‰¹EXISTSæŸ¥è¯¢
                batch_size = 100
                for i in range(0, len(tids), batch_size):
                    batch_tids = tids[i:i + batch_size]

                    # æ„å»ºEXISTSæŸ¥è¯¢
                    for tid in batch_tids:
                        exists_query = self.db_session.query(exists().where(Topic.tid == tid)).scalar()
                        if exists_query:
                            existing_tids.add(tid)

            elapsed = time.time() - start_time
            self.logger.debug(
                f"ğŸ” [EXISTSæŸ¥è¯¢] æ£€æŸ¥{len(tids)}ä¸ªä¸»é¢˜å­˜åœ¨æ€§ï¼Œ"
                f"æ‰¾åˆ°{len(existing_tids)}ä¸ªå­˜åœ¨ï¼Œè€—æ—¶{elapsed:.3f}s"
            )

            return existing_tids

        except SQLAlchemyError as e:
            self.logger.error(f"EXISTSæŸ¥è¯¢å‡ºé”™: {e}")
            return set()
        except Exception as e:
            self.logger.error(f"EXISTSæŸ¥è¯¢å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
            return set()

    def get_updated_topics_since(self, last_sync_time, limit=1000):
        """è·å–æŒ‡å®šæ—¶é—´åæ›´æ–°çš„ä¸»é¢˜ï¼ˆå¢é‡åŒæ­¥ï¼‰

        Args:
            last_sync_time: ä¸Šæ¬¡åŒæ­¥æ—¶é—´ï¼ˆå­—ç¬¦ä¸²æ ¼å¼ï¼‰
            limit: æœ€å¤§è¿”å›æ•°é‡

        Returns:
            list: æ›´æ–°çš„ä¸»é¢˜åˆ—è¡¨
        """
        if not last_sync_time:
            return []

        try:
            from ..models import Topic
            start_time = time.time()

            # ä½¿ç”¨ç´¢å¼•ä¼˜åŒ–æŸ¥è¯¢ï¼ˆlast_reply_dateå­—æ®µæœ‰ç´¢å¼•ï¼‰
            # åªè·å–æ¯”ä¸Šæ¬¡åŒæ­¥æ—¶é—´æ›´æ–°çš„ä¸»é¢˜
            topics = (
                self.db_session.query(Topic)
                .filter(Topic.last_reply_date > last_sync_time)
                .order_by(Topic.last_reply_date.desc())
                .limit(limit)
                .all()
            )

            elapsed = time.time() - start_time
            self.logger.info(
                f"ğŸ“ˆ [å¢é‡åŒæ­¥] æŸ¥è¯¢{limit}ä¸ªæœ€æ–°æ›´æ–°ä¸»é¢˜ï¼Œ"
                f"æ‰¾åˆ°{len(topics)}ä¸ªï¼Œè€—æ—¶{elapsed:.3f}s"
            )

            return topics

        except SQLAlchemyError as e:
            self.logger.error(f"å¢é‡åŒæ­¥æŸ¥è¯¢å‡ºé”™: {e}")
            return []
        except Exception as e:
            self.logger.error(f"å¢é‡åŒæ­¥æŸ¥è¯¢å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
            return []

    def get_topic_count_by_time_range(self, start_time, end_time=None):
        """æŒ‰æ—¶é—´èŒƒå›´è·å–ä¸»é¢˜æ•°é‡ç»Ÿè®¡

        Args:
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´ï¼ŒNoneè¡¨ç¤ºåˆ°ç°åœ¨

        Returns:
            int: ä¸»é¢˜æ•°é‡
        """
        try:
            from ..models import Topic

            query = self.db_session.query(Topic).filter(Topic.post_time >= start_time)

            if end_time:
                query = query.filter(Topic.post_time <= end_time)

            count = query.count()

            self.logger.debug(
                f"ğŸ“Š [æ—¶é—´ç»Ÿè®¡] {start_time} åˆ° {end_time or 'ç°åœ¨'} "
                f"å…±{count}ä¸ªä¸»é¢˜"
            )

            return count

        except SQLAlchemyError as e:
            self.logger.error(f"æ—¶é—´èŒƒå›´ç»Ÿè®¡æŸ¥è¯¢å‡ºé”™: {e}")
            return 0
        except Exception as e:
            self.logger.error(f"æ—¶é—´èŒƒå›´ç»Ÿè®¡æŸ¥è¯¢å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
            return 0

    def batch_check_and_update(self, tids, update_callback=None):
        """æ‰¹é‡æ£€æŸ¥å¹¶æ›´æ–°ä¸»é¢˜ï¼ˆé«˜çº§ä¼˜åŒ–ï¼‰

        ç»“åˆEXISTSæŸ¥è¯¢ã€å¢é‡æ›´æ–°å’Œæ‰¹é‡å¤„ç†ï¼Œæä¾›æœ€é«˜æ•ˆçš„æ•°æ®åŒæ­¥æ–¹æ¡ˆã€‚

        Args:
            tids: ä¸»é¢˜IDåˆ—è¡¨
            update_callback: æ›´æ–°å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶(topic, is_new)å‚æ•°

        Returns:
            dict: {
                'existing': existing_tids_set,
                'new': new_tids_set,
                'updated': updated_topics_list,
                'skipped': skipped_tids_list
            }
        """
        if not tids:
            return {
                'existing': set(),
                'new': set(),
                'updated': [],
                'skipped': []
            }

        try:
            from ..models import Topic
            import time

            start_time = time.time()

            # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨EXISTSæŸ¥è¯¢æ£€æŸ¥å­˜åœ¨æ€§
            existing_tids = self.check_topics_exist_exists(tids)
            new_tids = set(tids) - existing_tids

            # ç¬¬äºŒæ­¥ï¼šè·å–éœ€è¦æ›´æ–°çš„ä¸»é¢˜ï¼ˆå·²å­˜åœ¨ä½†å¯èƒ½æœ‰æ›´æ–°ï¼‰
            topics_to_update = []
            if existing_tids:
                # æ‰¹é‡è·å–å·²å­˜åœ¨ä¸»é¢˜çš„è¯¦ç»†ä¿¡æ¯
                batch_size = 100
                for i in range(0, len(existing_tids), batch_size):
                    batch = list(existing_tids)[i:i + batch_size]
                    topics = (
                        self.db_session.query(Topic)
                        .filter(Topic.tid.in_(batch))
                        .all()
                    )
                    topics_to_update.extend(topics)

            # ç¬¬ä¸‰æ­¥ï¼šæ‰§è¡Œæ›´æ–°å›è°ƒ
            updated_topics = []
            skipped_tids = []

            if update_callback:
                for topic in topics_to_update:
                    try:
                        is_updated = update_callback(topic)
                        if is_updated:
                            updated_topics.append(topic)
                    except Exception as e:
                        self.logger.error(f"æ›´æ–°å›è°ƒæ‰§è¡Œå¤±è´¥ (tid={topic.tid}): {e}")
                        skipped_tids.append(topic.tid)

            elapsed = time.time() - start_time

            # ç»Ÿè®¡ä¿¡æ¯
            self.logger.info(
                f"ğŸ”„ [æ‰¹é‡ä¼˜åŒ–] å¤„ç†{len(tids)}ä¸ªä¸»é¢˜: "
                f"å­˜åœ¨{len(existing_tids)}ä¸ª, æ–°å¢{len(new_tids)}ä¸ª, "
                f"æ›´æ–°{len(updated_topics)}ä¸ª, è·³è¿‡{len(skipped_tids)}ä¸ª, "
                f"è€—æ—¶{elapsed:.3f}s"
            )

            return {
                'existing': existing_tids,
                'new': new_tids,
                'updated': updated_topics,
                'skipped': skipped_tids
            }

        except Exception as e:
            self.logger.error(f"æ‰¹é‡æ£€æŸ¥æ›´æ–°å¤±è´¥: {e}")
            return {
                'existing': set(),
                'new': set(),
                'updated': [],
                'skipped': tids
            }

    def analyze_query_plan(self, query):
        """åˆ†ææŸ¥è¯¢è®¡åˆ’ï¼ˆPostgreSQLç‰¹æœ‰ï¼‰

        Args:
            query: SQLAlchemyæŸ¥è¯¢å¯¹è±¡

        Returns:
            dict: æŸ¥è¯¢è®¡åˆ’ä¿¡æ¯
        """
        try:
            # è·å–åŸç”ŸSQLè¯­å¥
            sql = str(query.statement.compile(
                compile_kwargs={"literal_binds": True}
            ))

            # æ‰§è¡ŒEXPLAIN ANALYZE
            explain_query = f"EXPLAIN ANALYZE {sql}"
            result = self.db_session.execute(explain_query)

            plan_info = []
            for row in result:
                plan_info.append(row[0])

            self.logger.debug(f"ğŸ“‹ [æŸ¥è¯¢è®¡åˆ’] {sql}")
            self.logger.debug(f"ğŸ“Š [æ‰§è¡Œè®¡åˆ’]\n" + "\n".join(plan_info))

            return {
                'sql': sql,
                'plan': plan_info,
                'cost': self._extract_cost(plan_info)
            }

        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢è®¡åˆ’åˆ†æå¤±è´¥: {e}")
            return None

    def _extract_cost(self, plan_info):
        """ä»æ‰§è¡Œè®¡åˆ’ä¸­æå–æˆæœ¬ä¿¡æ¯"""
        try:
            for line in plan_info:
                if 'Planning Time:' in line:
                    return {'planning_time': line}
                elif 'Execution Time:' in line:
                    return {'execution_time': line}
            return {}
        except Exception:
            return {}

    def optimize_batch_query(self, tids, use_exists=True, use_cache=True):
        """ä¼˜åŒ–çš„æ‰¹é‡æŸ¥è¯¢å…¥å£

        è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æŸ¥è¯¢ç­–ç•¥ï¼š
        1. å°‘é‡æ•°æ®ï¼ˆ<10ï¼‰ï¼šINæŸ¥è¯¢
        2. ä¸­ç­‰æ•°æ®ï¼ˆ10-1000ï¼‰ï¼šåˆ†æ‰¹INæŸ¥è¯¢
        3. å¤§é‡æ•°æ®ï¼ˆ>1000ï¼‰ï¼šEXISTSæŸ¥è¯¢

        Args:
            tids: ä¸»é¢˜IDåˆ—è¡¨
            use_exists: æ˜¯å¦ä½¿ç”¨EXISTSæŸ¥è¯¢ï¼ˆå¤§é‡æ•°æ®æ—¶ï¼‰
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜

        Returns:
            dict: æŸ¥è¯¢ç»“æœç»Ÿè®¡
        """
        if not tids:
            return {'strategy': 'none', 'count': 0, 'time': 0}

        start_time = time.time()

        # æ ¹æ®æ•°æ®é‡é€‰æ‹©ç­–ç•¥
        if len(tids) < 10:
            strategy = 'in_query'
        elif len(tids) <= 1000:
            strategy = 'batch_in_query'
        elif use_exists:
            strategy = 'exists_query'
        else:
            strategy = 'batch_in_query'

        # æ‰§è¡ŒæŸ¥è¯¢
        if strategy == 'exists_query':
            existing = self.check_topics_exist_exists(tids)
            count = len(existing)
        else:
            # ä½¿ç”¨INæŸ¥è¯¢
            from ..models import Topic
            topics = self.db_session.query(Topic).filter(Topic.tid.in_(tids)).all()
            count = len(topics)

        elapsed = time.time() - start_time

        self.logger.info(
            f"ğŸ¯ [æŸ¥è¯¢ä¼˜åŒ–] ä½¿ç”¨ç­–ç•¥: {strategy}, "
            f"æŸ¥è¯¢{len(tids)}ä¸ªä¸»é¢˜, æ‰¾åˆ°{count}ä¸ª, è€—æ—¶{elapsed:.3f}s"
        )

        return {
            'strategy': strategy,
            'count': count,
            'time': elapsed,
            'tids': tids
        }


# ä¾¿æ·å‡½æ•°
def create_query_optimizer(db_session, logger=None):
    """åˆ›å»ºæŸ¥è¯¢ä¼˜åŒ–å™¨å®ä¾‹"""
    return QueryOptimizer(db_session, logger)


def batch_exists_query(db_session, tids, logger=None):
    """ä¾¿æ·å‡½æ•°ï¼šæ‰¹é‡EXISTSæŸ¥è¯¢"""
    optimizer = QueryOptimizer(db_session, logger)
    return optimizer.check_topics_exist_exists(tids)


def incremental_sync(db_session, last_sync_time, logger=None):
    """ä¾¿æ·å‡½æ•°ï¼šå¢é‡åŒæ­¥"""
    optimizer = QueryOptimizer(db_session, logger)
    return optimizer.get_updated_topics_since(last_sync_time)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    from sqlalchemy import create_engine
    from sqlalchemy.orm import sessionmaker

    # åˆ›å»ºæµ‹è¯•æ•°æ®åº“
    engine = create_engine('sqlite:///:memory:')
    Session = sessionmaker(bind=engine)

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    from ..models import Base, Topic
    Base.metadata.create_all(engine)

    session = Session()

    # æ·»åŠ æµ‹è¯•æ•°æ®
    for i in range(100):
        topic = Topic(
            tid=f'test_{i}',
            title=f'Test Topic {i}',
            post_time='2025-01-01 00:00:00',
            last_reply_date='2025-01-01 00:00:00',
            re_num=0
        )
        session.add(topic)

    session.commit()

    # æµ‹è¯•EXISTSæŸ¥è¯¢
    optimizer = QueryOptimizer(session)
    test_tids = [f'test_{i}' for i in range(50)]
    existing = optimizer.check_topics_exist_exists(test_tids)
    print(f"å­˜åœ¨çš„ä¸»é¢˜: {len(existing)}/{len(test_tids)}")

    # æµ‹è¯•æŸ¥è¯¢è®¡åˆ’åˆ†æ
    query = session.query(Topic).filter(Topic.tid.in_(test_tids))
    plan = optimizer.analyze_query_plan(query)
    print(f"æŸ¥è¯¢è®¡åˆ’: {plan}")
