#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½ç›‘æ§æ¨¡å—

æä¾›å®æ—¶æŸ¥è¯¢æ€§èƒ½ç›‘æ§ã€æ…¢æŸ¥è¯¢å‘Šè­¦ã€æ€§èƒ½ç»Ÿè®¡ç­‰åŠŸèƒ½ã€‚

ä¸»è¦åŠŸèƒ½:
1. æŸ¥è¯¢è€—æ—¶ç»Ÿè®¡ï¼ˆå¹³å‡ã€95åˆ†ä½ã€99åˆ†ä½ï¼‰
2. æ…¢æŸ¥è¯¢å‘Šè­¦å’Œæ—¥å¿—
3. æŸ¥è¯¢ååé‡ç›‘æ§
4. æ€§èƒ½æŠ¥å‘Šç”Ÿæˆ
5. å†å²æ€§èƒ½æ•°æ®åˆ†æ

ä½œè€…: Claude Code
æ—¥æœŸ: 2025-12-07
"""

import time
import json
import os
from datetime import datetime, timedelta
from collections import defaultdict, deque
from threading import Lock
import logging


class QueryMonitor:
    """æŸ¥è¯¢æ€§èƒ½ç›‘æ§å™¨"""

    def __init__(self, log_file='query_performance.log'):
        self.logger = logging.getLogger(__name__)
        self.log_file = log_file
        self.lock = Lock()

        # æŸ¥è¯¢ç»Ÿè®¡æ•°æ®
        self.query_stats = {
            'total_queries': 0,
            'total_time': 0.0,
            'min_time': float('inf'),
            'max_time': 0.0,
            'query_times': deque(maxlen=1000),  # ä¿å­˜æœ€è¿‘1000æ¬¡æŸ¥è¯¢è€—æ—¶
            'slow_queries': deque(maxlen=100),  # ä¿å­˜æœ€è¿‘100æ¬¡æ…¢æŸ¥è¯¢
            'batch_stats': defaultdict(int),  # æ‰¹æ¬¡å¤§å°ç»Ÿè®¡
            'hourly_stats': defaultdict(lambda: {'count': 0, 'total_time': 0.0}),
        }

        # æ…¢æŸ¥è¯¢é˜ˆå€¼ï¼ˆç§’ï¼‰
        self.slow_query_threshold = 0.5  # 500ms
        self.critical_slow_threshold = 2.0  # 2000ms

        # æ€§èƒ½å‘Šè­¦é…ç½®
        self.alert_config = {
            'enable_alerts': True,
            'alert_cooldown': 300,  # 5åˆ†é’Ÿå†…ä¸é‡å¤å‘Šè­¦
            'last_alert_time': 0,
        }

    def record_query(self, query_time, query_type='batch', batch_size=None, topic_count=None):
        """è®°å½•ä¸€æ¬¡æŸ¥è¯¢çš„æ€§èƒ½æ•°æ®

        Args:
            query_time: æŸ¥è¯¢è€—æ—¶ï¼ˆç§’ï¼‰
            query_type: æŸ¥è¯¢ç±»å‹ï¼ˆ'batch', 'single', 'exists'ç­‰ï¼‰
            batch_size: æ‰¹æ¬¡å¤§å°
            topic_count: æŸ¥è¯¢çš„ä¸»é¢˜æ•°é‡
        """
        with self.lock:
            # æ›´æ–°åŸºç¡€ç»Ÿè®¡
            self.query_stats['total_queries'] += 1
            self.query_stats['total_time'] += query_time
            self.query_stats['min_time'] = min(self.query_stats['min_time'], query_time)
            self.query_stats['max_time'] = max(self.query_stats['max_time'], query_time)

            # ä¿å­˜æŸ¥è¯¢è€—æ—¶
            self.query_stats['query_times'].append(query_time)

            # è®°å½•æ‰¹æ¬¡å¤§å°ç»Ÿè®¡
            if batch_size:
                self.query_stats['batch_stats'][batch_size] += 1

            # è®°å½•æ¯å°æ—¶ç»Ÿè®¡
            now = datetime.now()
            hour_key = now.strftime('%Y-%m-%d %H:00')
            self.query_stats['hourly_stats'][hour_key]['count'] += 1
            self.query_stats['hourly_stats'][hour_key]['total_time'] += query_time

            # æ…¢æŸ¥è¯¢æ£€æŸ¥
            if query_time > self.slow_query_threshold:
                self._record_slow_query(query_time, query_type, batch_size, topic_count)

    def _record_slow_query(self, query_time, query_type, batch_size, topic_count):
        """è®°å½•æ…¢æŸ¥è¯¢"""
        slow_query_info = {
            'timestamp': datetime.now().isoformat(),
            'query_time': query_time,
            'query_type': query_type,
            'batch_size': batch_size,
            'topic_count': topic_count,
        }

        self.query_stats['slow_queries'].append(slow_query_info)

        # æ…¢æŸ¥è¯¢å‘Šè­¦
        if self.alert_config['enable_alerts']:
            self._send_alert(slow_query_info)

    def _send_alert(self, slow_query_info):
        """å‘é€æ…¢æŸ¥è¯¢å‘Šè­¦"""
        now = time.time()

        # å†·å´æœŸæ£€æŸ¥
        if now - self.alert_config['last_alert_time'] < self.alert_config['alert_cooldown']:
            return

        self.alert_config['last_alert_time'] = now

        # ç”Ÿæˆå‘Šè­¦æ¶ˆæ¯
        alert_msg = (
            f"âš ï¸ [æ…¢æŸ¥è¯¢å‘Šè­¦] æ£€æµ‹åˆ°æ…¢æŸ¥è¯¢ï¼\n"
            f"  æŸ¥è¯¢è€—æ—¶: {slow_query_info['query_time']:.3f}s\n"
            f"  æŸ¥è¯¢ç±»å‹: {slow_query_info['query_type']}\n"
            f"  æ‰¹æ¬¡å¤§å°: {slow_query_info.get('batch_size', 'N/A')}\n"
            f"  ä¸»é¢˜æ•°é‡: {slow_query_info.get('topic_count', 'N/A')}\n"
            f"  æ—¶é—´: {slow_query_info['timestamp']}\n"
        )

        self.logger.warning(alert_msg)

        # å†™å…¥æ—¥å¿—æ–‡ä»¶
        self._write_to_log(alert_msg)

    def _write_to_log(self, message):
        """å†™å…¥æ—¥å¿—æ–‡ä»¶"""
        try:
            with open(self.log_file, 'a', encoding='utf-8') as f:
                f.write(f"[{datetime.now().isoformat()}] {message}\n")
        except Exception as e:
            self.logger.error(f"å†™å…¥æ€§èƒ½æ—¥å¿—å¤±è´¥: {e}")

    def get_stats(self):
        """è·å–å½“å‰æ€§èƒ½ç»Ÿè®¡"""
        with self.lock:
            stats = self.query_stats.copy()

            # è®¡ç®—å¹³å‡è€—æ—¶
            if stats['total_queries'] > 0:
                stats['avg_time'] = stats['total_time'] / stats['total_queries']
            else:
                stats['avg_time'] = 0

            # è®¡ç®—åˆ†ä½æ•°
            if stats['query_times']:
                query_times_list = sorted(stats['query_times'])
                stats['p50_time'] = self._percentile(query_times_list, 50)
                stats['p95_time'] = self._percentile(query_times_list, 95)
                stats['p99_time'] = self._percentile(query_times_list, 99)
            else:
                stats['p50_time'] = stats['p95_time'] = stats['p99_time'] = 0

            return stats

    def _percentile(self, data, percentile):
        """è®¡ç®—ç™¾åˆ†ä½æ•°"""
        if not data:
            return 0

        index = (percentile / 100.0) * (len(data) - 1)
        if index.is_integer():
            return data[int(index)]
        else:
            lower = data[int(index)]
            upper = data[int(index) + 1]
            return lower + (upper - lower) * (index - int(index))

    def generate_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        stats = self.get_stats()

        report = []
        report.append("=" * 80)
        report.append("ğŸ“Š æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½æŠ¥å‘Š")
        report.append("=" * 80)
        report.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")

        # åŸºç¡€ç»Ÿè®¡
        report.append("ğŸ“ˆ åŸºç¡€ç»Ÿè®¡:")
        report.append(f"  æ€»æŸ¥è¯¢æ¬¡æ•°: {stats['total_queries']:,}")
        report.append(f"  æ€»è€—æ—¶: {stats['total_time']:.3f}s")
        report.append(f"  å¹³å‡è€—æ—¶: {stats['avg_time']:.3f}s")
        report.append(f"  æœ€å°è€—æ—¶: {stats['min_time']:.3f}s")
        report.append(f"  æœ€å¤§è€—æ—¶: {stats['max_time']:.3f}s")
        report.append("")

        # åˆ†ä½æ•°ç»Ÿè®¡
        report.append("ğŸ“Š åˆ†ä½æ•°ç»Ÿè®¡:")
        report.append(f"  50åˆ†ä½: {stats['p50_time']:.3f}s")
        report.append(f"  95åˆ†ä½: {stats['p95_time']:.3f}s")
        report.append(f"  99åˆ†ä½: {stats['p99_time']:.3f}s")
        report.append("")

        # æ‰¹æ¬¡å¤§å°ç»Ÿè®¡
        if stats['batch_stats']:
            report.append("ğŸ“¦ æ‰¹æ¬¡å¤§å°ç»Ÿè®¡:")
            for batch_size, count in sorted(stats['batch_stats'].items()):
                report.append(f"  æ‰¹æ¬¡å¤§å° {batch_size}: {count} æ¬¡")
            report.append("")

        # æ…¢æŸ¥è¯¢ç»Ÿè®¡
        slow_query_count = len(stats['slow_queries'])
        if slow_query_count > 0:
            report.append(f"âš ï¸ æ…¢æŸ¥è¯¢ç»Ÿè®¡ (>{self.slow_query_threshold}s): {slow_query_count} æ¬¡")
            report.append("  æœ€è¿‘5æ¬¡æ…¢æŸ¥è¯¢:")
            for i, sq in enumerate(list(stats['slow_queries'])[-5:], 1):
                report.append(
                    f"    {i}. {sq['query_time']:.3f}s - "
                    f"{sq['query_type']} - {sq['timestamp']}"
                )
            report.append("")
        else:
            report.append("âœ… æ— æ…¢æŸ¥è¯¢è®°å½•")
            report.append("")

        # æ¯å°æ—¶ç»Ÿè®¡
        if stats['hourly_stats']:
            report.append("â° æœ€è¿‘6å°æ—¶ç»Ÿè®¡:")
            for hour, hour_stats in sorted(stats['hourly_stats'].items())[-6:]:
                avg_time = hour_stats['total_time'] / hour_stats['count'] if hour_stats['count'] > 0 else 0
                report.append(
                    f"  {hour}: {hour_stats['count']} æ¬¡æŸ¥è¯¢, "
                    f"å¹³å‡ {avg_time:.3f}s, æ€»è®¡ {hour_stats['total_time']:.3f}s"
                )
            report.append("")

        # æ€§èƒ½è¯„ä¼°
        report.append("ğŸ¯ æ€§èƒ½è¯„ä¼°:")
        if stats['avg_time'] < 0.1:
            report.append("  âœ… æŸ¥è¯¢æ€§èƒ½ä¼˜ç§€ (å¹³å‡ < 100ms)")
        elif stats['avg_time'] < 0.5:
            report.append("  âœ… æŸ¥è¯¢æ€§èƒ½è‰¯å¥½ (å¹³å‡ < 500ms)")
        elif stats['avg_time'] < 1.0:
            report.append("  âš ï¸ æŸ¥è¯¢æ€§èƒ½ä¸€èˆ¬ (å¹³å‡ < 1s)")
        else:
            report.append("  ğŸš¨ æŸ¥è¯¢æ€§èƒ½è¾ƒå·® (å¹³å‡ >= 1s)")

        if stats['p95_time'] > 1.0:
            report.append("  âš ï¸ 95åˆ†ä½è€—æ—¶è¾ƒé«˜ï¼Œå»ºè®®ä¼˜åŒ–")

        report.append("")
        report.append("=" * 80)

        return "\n".join(report)

    def save_stats_to_file(self, filename=None):
        """ä¿å­˜ç»Ÿè®¡æ•°æ®åˆ°æ–‡ä»¶"""
        if filename is None:
            filename = f"query_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        stats = self.get_stats()

        # è½¬æ¢dequeä¸ºlistä»¥ä¾¿JSONåºåˆ—åŒ–
        stats['query_times'] = list(stats['query_times'])
        stats['slow_queries'] = list(stats['slow_queries'])

        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(stats, f, ensure_ascii=False, indent=2)
            self.logger.info(f"æ€§èƒ½ç»Ÿè®¡æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
            return filename
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ€§èƒ½ç»Ÿè®¡æ•°æ®å¤±è´¥: {e}")
            return None

    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡æ•°æ®"""
        with self.lock:
            self.query_stats = {
                'total_queries': 0,
                'total_time': 0.0,
                'min_time': float('inf'),
                'max_time': 0.0,
                'query_times': deque(maxlen=1000),
                'slow_queries': deque(maxlen=100),
                'batch_stats': defaultdict(int),
                'hourly_stats': defaultdict(lambda: {'count': 0, 'total_time': 0.0}),
            }
            self.logger.info("æ€§èƒ½ç»Ÿè®¡æ•°æ®å·²é‡ç½®")


# å…¨å±€ç›‘æ§å®ä¾‹
_query_monitor = None


def get_monitor():
    """è·å–å…¨å±€ç›‘æ§å®ä¾‹"""
    global _query_monitor
    if _query_monitor is None:
        _query_monitor = QueryMonitor()
    return _query_monitor


def query_timer(query_type='batch', batch_size=None, topic_count=None):
    """æŸ¥è¯¢è®¡æ—¶è£…é¥°å™¨

    ä½¿ç”¨ç¤ºä¾‹:
        @query_timer('batch', batch_size=100)
        def batch_query():
            # æ‰§è¡ŒæŸ¥è¯¢
            pass
    """
    def decorator(func):
        def wrapper(*args, **kwargs):
            monitor = get_monitor()
            start_time = time.time()

            try:
                result = func(*args, **kwargs)
                return result
            finally:
                elapsed = time.time() - start_time
                monitor.record_query(
                    query_time=elapsed,
                    query_type=query_type,
                    batch_size=batch_size,
                    topic_count=topic_count
                )

        return wrapper
    return decorator


# ä¾¿æ·å‡½æ•°
def record_batch_query(query_time, batch_size, topic_count):
    """è®°å½•æ‰¹æ¬¡æŸ¥è¯¢"""
    get_monitor().record_query(query_time, 'batch', batch_size, topic_count)


def record_single_query(query_time, topic_count):
    """è®°å½•å•æ¡æŸ¥è¯¢"""
    get_monitor().record_query(query_time, 'single', batch_size=1, topic_count=topic_count)


def record_exists_query(query_time, topic_count):
    """è®°å½•EXISTSæŸ¥è¯¢"""
    get_monitor().record_query(query_time, 'exists', batch_size=1, topic_count=topic_count)


def get_performance_report():
    """è·å–æ€§èƒ½æŠ¥å‘Š"""
    return get_monitor().generate_report()


def save_performance_stats(filename=None):
    """ä¿å­˜æ€§èƒ½ç»Ÿè®¡"""
    return get_monitor().save_stats_to_file(filename)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    import random

    monitor = QueryMonitor()

    # æ¨¡æ‹ŸæŸ¥è¯¢
    print("æ¨¡æ‹ŸæŸ¥è¯¢æ€§èƒ½ç›‘æ§...")
    for i in range(100):
        query_time = random.uniform(0.01, 0.5)
        monitor.record_query(query_time, 'batch', 100, 100)

    # ç”ŸæˆæŠ¥å‘Š
    print(monitor.generate_report())

    # ä¿å­˜ç»Ÿè®¡
    monitor.save_stats_to_file('test_query_stats.json')
