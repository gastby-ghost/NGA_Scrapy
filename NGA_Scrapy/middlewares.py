import scrapy
import json
import os
import time
import threading
import uuid
from queue import Queue, Empty
from typing import Optional, Dict, List, Callable, Any
from playwright.sync_api import TimeoutError as PlaywrightTimeoutError, sync_playwright
from scrapy.exceptions import NotConfigured
from scrapy import signals
from NGA_Scrapy.utils.proxy_manager import get_proxy_manager
from NGA_Scrapy.utils.ban_detector import BanDetector
from NGA_Scrapy.utils.instance_manager import BrowserInstanceManager


# ========== é…ç½®å¸¸é‡ ==========
DEFAULT_HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'Accept-Encoding': 'gzip, deflate, br',
    'Cache-Control': 'no-cache',
    'Pragma': 'no-cache',
    'Sec-Fetch-Dest': 'document',
    'Sec-Fetch-Mode': 'navigate',
    'Sec-Fetch-Site': 'none',
    'Sec-Fetch-User': '?1',
    'Sec-Ch-Ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
    'Sec-Ch-Ua-Mobile': '?0',
    'Sec-Ch-Ua-Platform': '"Linux"',
    'Upgrade-Insecure-Requests': '1'
}

BROWSER_ARGS = [
    '--disable-gpu',
    '--no-sandbox',
    '--disable-dev-shm-usage',
    '--disable-extensions',
    '--disable-infobars',
    '--disable-notifications',
]

DEFAULT_VIEWPORT = {'width': 1920, 'height': 1080}
REQUEST_TIMEOUT = 60
NAV_TIMEOUT = 15000
LOAD_TIMEOUT = 5000


# ========== å·¥å…·ç±» ==========
class PerformanceStats:
    """ç®€åŒ–çš„æ€§èƒ½ç»Ÿè®¡å·¥å…·ç±»"""
    def __init__(self):
        self._lock = threading.Lock()
        self._start_time = time.time()
        self.reset()

    def reset(self):
        """é‡ç½®ç»Ÿè®¡æ•°æ®"""
        with self._lock:
            self.request_count = 0
            self.success_count = 0
            self.failed_count = 0
            self.total_page_time = 0.0
            self.max_page_time = 0.0
            self.min_page_time = float('inf')
            self.browser_recycles = 0
            self.timeout_errors = 0

    def log_request(self, success: bool, duration: float):
        """è®°å½•è¯·æ±‚ç»Ÿè®¡"""
        with self._lock:
            self.request_count += 1
            if success:
                self.success_count += 1
                self.total_page_time += duration
                self.max_page_time = max(self.max_page_time, duration)
                self.min_page_time = min(self.min_page_time, duration)
            else:
                self.failed_count += 1

    def log_recycle(self):
        """è®°å½•å®ä¾‹å›æ”¶"""
        with self._lock:
            self.browser_recycles += 1

    def log_timeout(self):
        """è®°å½•è¶…æ—¶äº‹ä»¶"""
        with self._lock:
            self.timeout_errors += 1

    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        with self._lock:
            uptime = time.time() - self._start_time
            avg_time = (self.total_page_time / self.success_count) if self.success_count > 0 else 0

            return {
                'uptime': f"{uptime:.1f}s",
                'requests': self.request_count,
                'success_rate': f"{(self.success_count / self.request_count * 100):.2f}%" if self.request_count > 0 else "0%",
                'avg_page_time': f"{avg_time:.3f}s",
                'max_page_time': f"{self.max_page_time:.3f}s",
                'min_page_time': f"{self.min_page_time:.3f}s" if self.min_page_time != float('inf') else "N/A",
                'browser_recycles': self.browser_recycles,
                'timeout_errors': self.timeout_errors,
                'req_per_minute': f"{(self.request_count / (uptime / 60)):.2f}" if uptime > 0 else "N/A"
            }


# ========== Cookieç®¡ç† ==========
class CookieManager:
    """Cookieç®¡ç†å™¨ - ç‹¬ç«‹èŒè´£"""
    def __init__(self, logger):
        self.logger = logger
        self.cookies = None

    def load(self, cookies_file: str = 'cookies.txt') -> Optional[List[Dict]]:
        """åŠ è½½å¹¶é¢„å¤„ç†cookies"""
        if not os.path.exists(cookies_file):
            self.logger.info(f"Cookies file not found: {cookies_file}")
            return None

        try:
            with open(cookies_file, 'r', encoding='utf-8') as f:
                cookies = json.load(f)

            processed = []
            for c in cookies:
                try:
                    expiry = c.get('expiry') or c.get('expirationDate')
                    if expiry is None:
                        expiry = int(time.time()) + 3600
                    elif isinstance(expiry, float):
                        expiry = int(expiry)

                    domain = c.get('domain', '.ngabbs.com')
                    if not domain.startswith('.'):
                        domain = f'.{domain}'

                    cookie_dict = {
                        'name': c['name'],
                        'value': c['value'],
                        'domain': domain,
                        'path': c.get('path', '/'),
                        'expires': expiry,
                        'httpOnly': c.get('httpOnly', False),
                        'secure': c.get('secure', False),
                        'sameSite': 'Lax'
                    }

                    processed.append({k: v for k, v in cookie_dict.items() if v is not None})
                except KeyError:
                    self.logger.warning(f"Skip invalid cookie: missing required field")
                    continue

            self.cookies = processed
            return processed
        except json.JSONDecodeError as e:
            self.logger.error(f"Invalid JSON in cookies file: {e}")
            return None
        except Exception as e:
            self.logger.error(f"Failed to load cookies: {e}")
            return None

    def save_if_updated(self, context):
        """æ£€æŸ¥å¹¶ä¿å­˜æ›´æ–°åçš„cookies"""
        try:
            current_cookies = context.cookies()
            nga_passport = next(
                (c for c in current_cookies if c['name'] == 'ngaPassportUid'),
                None
            )

            if nga_passport:
                old_passport = next(
                    (c for c in self.cookies if c['name'] == 'ngaPassportUid'),
                    None
                ) if self.cookies else None

                needs_update = (
                    not old_passport or
                    old_passport['value'] != nga_passport['value'] or
                    old_passport.get('expires', 0) != nga_passport.get('expires', 0)
                )

                if needs_update:
                    with open('cookies.txt', 'w', encoding='utf-8') as f:
                        json.dump(current_cookies, f, ensure_ascii=False, indent=2)

                    self.cookies = current_cookies
                    expires_time = time.strftime(
                        '%Y-%m-%d %H:%M:%S',
                        time.localtime(nga_passport['expires'])
                    )
                    self.logger.info(
                        f"âœ“ Updated ngaPassportUid: {nga_passport['value'][:20]}..., "
                        f"expires: {expires_time}"
                    )
        except Exception as e:
            self.logger.error(f"Auto-update cookies failed: {e}")


# ========== é¡µé¢è·å–å™¨ ==========
class PageFetcher:
    """é¡µé¢è·å–å™¨ - ç‹¬ç«‹èŒè´£"""
    def __init__(self, logger):
        self.logger = logger

    def fetch(self, browser_pool: List, url: str, cookies: Optional[List],
              browser_index: int, referer: str = 'https://bbs.nga.cn/') -> Dict:
        """è·å–é¡µé¢å†…å®¹"""
        browser, context = browser_pool[browser_index % len(browser_pool)]
        page = context.new_page()

        try:
            self.logger.debug(f"Loading page: {url} (browser {browser_index})")

            if cookies:
                self.logger.debug(f"Setting {len(cookies)} cookies")
                context.clear_cookies()
                context.add_cookies(cookies)
                time.sleep(0.1)

            page.set_extra_http_headers({'Referer': referer})

            self.logger.debug(f"Navigating to: {url}")
            nav_start = time.time()
            page.goto(url, wait_until="domcontentloaded", timeout=NAV_TIMEOUT)
            nav_time = time.time() - nav_start
            self.logger.debug(f"Navigation complete: {nav_time:.2f}s")

            page.wait_for_load_state("domcontentloaded", timeout=LOAD_TIMEOUT)

            return {
                'url': page.url,
                'content': page.content(),
                'success': True,
                'nav_time': nav_time
            }
        except Exception as e:
            self.logger.error(f"Page load failed: {url}, error: {type(e).__name__}: {str(e)}")
            raise
        finally:
            page.close()


# ========== Playwrightå·¥ä½œçº¿ç¨‹ ==========
class PlaywrightWorker:
    """Playwrightä¸“ç”¨å·¥ä½œçº¿ç¨‹ï¼ˆè´Ÿè´£æµè§ˆå™¨æ± ç®¡ç†ï¼‰"""
    def __init__(self, max_browsers: int, proxy_manager=None, logger=None):
        self.max_browsers = max_browsers
        self.proxy_manager = proxy_manager
        self.logger = logger or self._default_logger
        self._workers = []
        self._task_queue = Queue()
        self._result_map = {}
        self._condition = threading.Condition(threading.Lock())
        self._stop_event = threading.Event()
        self._initialized = threading.Event()
        self._browser_pool = []
        self._playwright = None
        self._start_worker()

    def _default_logger(self):
        import logging
        return logging.getLogger('PlaywrightWorker')

    def _start_worker(self):
        """å¯åŠ¨å·¥ä½œçº¿ç¨‹"""
        worker = threading.Thread(
            target=self._playwright_worker_loop,
            name="PlaywrightWorker",
            daemon=False  # éå®ˆæŠ¤çº¿ç¨‹ï¼Œç¡®ä¿æ­£ç¡®å…³é—­
        )
        worker.start()
        self._workers.append(worker)

        # ç­‰å¾…åˆå§‹åŒ–å®Œæˆ
        self._initialized.wait(timeout=10)

    def _create_browser_pool(self):
        """åœ¨å·¥ä½œçº¿ç¨‹ä¸­åˆ›å»ºæµè§ˆå™¨æ± """
        playwright = sync_playwright().start()

        for _ in range(self.max_browsers):
            browser = playwright.chromium.launch(
                headless=True,
                args=BROWSER_ARGS
            )

            context_kwargs = {
                'viewport': DEFAULT_VIEWPORT,
                'java_script_enabled': True,
                'ignore_https_errors': True,
                'permissions': [],
                'extra_http_headers': DEFAULT_HEADERS.copy()
            }

            if self.proxy_manager:
                proxy_dict = self.proxy_manager.get_random_proxy(mark_used=False)
                if proxy_dict and proxy_dict.get('proxy'):
                    proxy_server = proxy_dict['proxy']
                    proxy_config = {
                        'server': proxy_server,
                        'bypass': 'localhost;127.0.0.1;*.nga.cn;*.ngabbs.com'
                    }

                    if 'username' in proxy_dict and 'password' in proxy_dict:
                        proxy_config['username'] = proxy_dict['username']
                        proxy_config['password'] = proxy_dict['password']

                    context_kwargs['proxy'] = proxy_config
                    self.logger.debug(f"Using proxy: {proxy_server}")

            context = browser.new_context(**context_kwargs)
            self._browser_pool.append((browser, context))

        return playwright

    def _playwright_worker_loop(self):
        """Playwrightå·¥ä½œçº¿ç¨‹ä¸»å¾ªç¯"""
        try:
            self.logger.info("Initializing Playwright worker...")
            self._playwright = self._create_browser_pool()
            self.logger.info(f"Browser pool initialized: {len(self._browser_pool)} instances")

            # é€šçŸ¥åˆå§‹åŒ–å®Œæˆ
            self._initialized.set()

            # å·¥ä½œçº¿ç¨‹ä¸»å¾ªç¯
            last_queue_check = time.time()
            while not self._stop_event.is_set():
                try:
                    request_id, task_func, args, kwargs, result_event = \
                        self._task_queue.get(timeout=0.1)

                    if self._stop_event.is_set():
                        if result_event:
                            result_event.set()
                        break

                    # ã€è¯Šæ–­æ—¥å¿—ã€‘è®°å½•ä»»åŠ¡é˜Ÿåˆ—çŠ¶æ€
                    current_time = time.time()
                    if current_time - last_queue_check > 10:  # æ¯10ç§’è®°å½•ä¸€æ¬¡
                        queue_size = self._task_queue.qsize()
                        self.logger.info(f"ğŸ“Š [å·¥ä½œçº¿ç¨‹è¯Šæ–­] ä»»åŠ¡é˜Ÿåˆ—å¤§å°: {queue_size}")
                        last_queue_check = current_time

                    task_start = time.time()
                    try:
                        # åœ¨å·¥ä½œçº¿ç¨‹ä¸­æ‰§è¡Œä»»åŠ¡
                        task_name = getattr(task_func, '__name__', 'unknown_task')
                        self.logger.debug(f"ğŸ”„ [å·¥ä½œçº¿ç¨‹] å¼€å§‹æ‰§è¡Œä»»åŠ¡: {task_name}")
                        
                        result = task_func(self._browser_pool, *args, **kwargs)
                        
                        task_duration = time.time() - task_start
                        self.logger.debug(f"âœ… [å·¥ä½œçº¿ç¨‹] ä»»åŠ¡å®Œæˆ: {task_name}, è€—æ—¶: {task_duration:.2f}s")
                        
                        with self._condition:
                            self._result_map[request_id] = ('success', result, None)
                    except Exception as e:
                        task_duration = time.time() - task_start
                        self.logger.error(f"âŒ [å·¥ä½œçº¿ç¨‹] ä»»åŠ¡å¤±è´¥: {task_name}, è€—æ—¶: {task_duration:.2f}s, é”™è¯¯: {e}")
                        import traceback
                        with self._condition:
                            self._result_map[request_id] = ('error', None, (type(e), e, traceback.format_exc()))
                    finally:
                        with self._condition:
                            self._condition.notify_all()

                    if result_event:
                        result_event.set()
                except Empty:
                    continue
                except Exception as e:
                    self.logger.error(f"Worker error: {e}")

            # æ¸…ç†èµ„æº
            self.logger.info("Closing browser instances...")
            for browser, context in self._browser_pool:
                try:
                    context.close()
                    browser.close()
                except Exception as e:
                    self.logger.warning(f"Error closing browser: {e}")

            if self._playwright:
                self._playwright.stop()

            self.logger.info("Playwright worker stopped")

        except Exception as e:
            self.logger.error(f"Playwright worker failed: {e}")
            self._initialized.set()  # ç¡®ä¿é€šçŸ¥ç­‰å¾…è€…
            raise

    def execute(self, task_func: Callable, *args, **kwargs):
        """åœ¨å·¥ä½œçº¿ç¨‹ä¸­æ‰§è¡Œä»»åŠ¡"""
        if self._stop_event.is_set():
            raise InterruptedError("Playwright worker is shutting down")

        request_id = str(uuid.uuid4())
        result_event = threading.Event()
        task_name = getattr(task_func, '__name__', str(task_func))

        with self._condition:
            self._result_map[request_id] = None

        self._task_queue.put((request_id, task_func, args, kwargs, result_event))
        self.logger.debug(f"Task queued: {task_name}")

        # ç­‰å¾…ç»“æœ
        if not result_event.wait(timeout=REQUEST_TIMEOUT):
            with self._condition:
                self._result_map.pop(request_id, None)
            raise TimeoutError(f"Task timeout: {task_name}")

        with self._condition:
            status, result, error = self._result_map.pop(request_id, ('timeout', None, 'Timeout'))

            if status == 'success':
                self.logger.debug(f"Task completed: {task_name}")
                return result
            elif status == 'error':
                exc_type, exc_value, _ = error
                self.logger.error(f"Task failed: {task_name}, {exc_type.__name__}: {exc_value}")
                raise exc_type(exc_value)
            else:
                self.logger.error(f"Task timeout: {task_name}")
                raise TimeoutError(f"Task timeout: {task_name}")

    def shutdown(self, timeout: int = 10):
        """å…³é—­å·¥ä½œçº¿ç¨‹"""
        self.logger.info("Shutting down Playwright worker...")
        self._stop_event.set()

        for worker in self._workers:
            worker.join(timeout=timeout)

        self.logger.info("Playwright worker stopped")


# ========== æµè§ˆå™¨æ±  ==========
class BrowserPool:
    """æµè§ˆå™¨è¿æ¥æ± ï¼ˆä½¿ç”¨PlaywrightWorkerï¼‰"""
    def __init__(self, max_browsers: int = 4, proxy_manager=None, logger=None):
        self.max_browsers = max_browsers
        self.proxy_manager = proxy_manager
        self.logger = logger or self._default_logger
        self.stats = PerformanceStats()
        self._page_fetcher = PageFetcher(self.logger)
        self._playwright_worker = PlaywrightWorker(max_browsers, proxy_manager, logger)

    def _default_logger(self):
        import logging
        return logging.getLogger('BrowserPool')

    def fetch_page(self, url: str, cookies: Optional[List],
                   browser_index: int) -> Dict:
        """è·å–é¡µé¢"""
        def _fetch_task(browser_pool):
            return self._page_fetcher.fetch(browser_pool, url, cookies, browser_index)

        return self._playwright_worker.execute(_fetch_task)

    def log_pool_status(self):
        """è®°å½•è¿æ¥æ± çŠ¶æ€"""
        stats = self.stats.get_stats()
        report = "\n".join([f"{k}: {v}" for k, v in stats.items()])
        self.logger.info(f"Performance stats:\n{report}")

    def close(self):
        """å…³é—­æµè§ˆå™¨æ± """
        self.logger.info("Closing browser pool...")
        self._playwright_worker.shutdown()
        self.log_pool_status()
        self.logger.info("Browser pool closed")


# ========== Playwrightä¸­é—´ä»¶ ==========
class PlaywrightMiddleware:
    """ä¼˜åŒ–çš„Playwrightä¸­é—´ä»¶"""
    def __init__(self):
        self.browser_pool = None
        self.logger = None
        self.cookie_manager = None
        self.proxy_manager = None
        self.ban_detector = None
        self.instance_manager = None
        self.last_stat_time = time.time()
        self._browser_index = 0
        self._failed_browsers = {}
        self._lock = threading.Lock()

    @classmethod
    def from_crawler(cls, crawler):
        if not crawler.settings.getbool('PLAYWRIGHT_ENABLED', True):
            raise NotConfigured('Playwright middleware not enabled')

        middleware = cls()
        crawler.signals.connect(middleware.spider_opened, signal=signals.spider_opened)
        crawler.signals.connect(middleware.spider_closed, signal=signals.spider_closed)
        return middleware

    def spider_opened(self, spider):
        """Spiderå¯åŠ¨å¤„ç†"""
        self.logger = spider.logger
        self.logger.info("=" * 60)
        self.logger.info("Playwright middleware starting")
        self.logger.info("=" * 60)

        # åˆå§‹åŒ–Cookieç®¡ç†å™¨
        self.cookie_manager = CookieManager(self.logger)
        self.cookie_manager.load()

        # åˆå§‹åŒ–å°ç¦æ£€æµ‹å™¨
        self.ban_detector = BanDetector(
            logger=self.logger,
            ban_threshold=spider.settings.getint('BAN_THRESHOLD', 3),
            recovery_time=spider.settings.getint('BAN_RECOVERY_TIME', 1800)
        )

        # åˆå§‹åŒ–å®ä¾‹ç®¡ç†å™¨
        def replace_instance(failed_id: int) -> int:
            return self._replace_browser_instance(failed_id)

        self.instance_manager = BrowserInstanceManager(
            max_instances=spider.settings.getint('PLAYWRIGHT_POOL_SIZE', 4),
            ban_detector=self.ban_detector,
            replacement_callback=replace_instance,
            proxy_manager=self.proxy_manager,
            logger=self.logger
        )

        self.instance_manager.start()
        self.logger.info("Ban detection and instance manager started")

        # åˆå§‹åŒ–ä»£ç†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        proxy_enabled = spider.settings.getbool('PROXY_ENABLED', False)
        if proxy_enabled:
            self._init_proxy(spider)
        else:
            self.logger.info("Proxy disabled")

        self.logger.info("=" * 60)

    def _init_proxy(self, spider):
        """åˆå§‹åŒ–ä»£ç†ç®¡ç†å™¨"""
        try:
            self.logger.info("Initializing proxy manager...")
            self.proxy_manager = get_proxy_manager()
            proxies = self.proxy_manager.get_proxies(force_refresh=True)

            if proxies:
                self.logger.info(f"Loaded {len(proxies)} proxies")
            else:
                self.logger.warning("No proxies loaded")

            status = self.proxy_manager.get_pool_status()
            self.logger.info("Proxy pool status:")
            for key, value in status.items():
                self.logger.info(f"  - {key}: {value}")
        except FileNotFoundError:
            self.logger.error("Proxy config file not found")
            self.proxy_manager = None
        except ValueError as e:
            self.logger.error(f"Proxy config validation error: {e}")
            self.proxy_manager = None
        except Exception as e:
            self.logger.error(f"Proxy manager init failed: {e}")
            self.proxy_manager = None

    def spider_closed(self, spider, reason):
        """Spiderå…³é—­å¤„ç†"""
        self.logger.info(f"Spider closed: {reason}")

        if self.instance_manager:
            self.instance_manager.stop()

        if self.browser_pool:
            self.browser_pool.close()

    def process_request(self, request, spider):
        """å¤„ç†è¯·æ±‚"""
        self.logger = spider.logger

        # ã€æ–°å¢è°ƒè¯•ã€‘è®°å½•æ¯ä¸ªè¢«è°ƒç”¨çš„è¯·æ±‚
        if 'read.php' in request.url:
            self.logger.debug(f"ğŸ” [Middleware] æ”¶åˆ°è¯·æ±‚: {request.url}, Priority: {request.priority}")
            # ã€è¯Šæ–­æ—¥å¿—ã€‘æ£€æŸ¥è°ƒåº¦é˜Ÿåˆ—çŠ¶æ€
            if hasattr(spider.crawler.engine, 'scheduler') and hasattr(spider.crawler.engine.scheduler, 'queue'):
                queue_size = len(spider.crawler.engine.scheduler.queue)
                self.logger.info(f"ğŸ“Š [é˜Ÿåˆ—è¯Šæ–­] å½“å‰è°ƒåº¦é˜Ÿåˆ—é•¿åº¦: {queue_size}")

        # ç»Ÿè®¡æŠ¥å‘Š
        if time.time() - self.last_stat_time > 300:
            if self.browser_pool:
                self.browser_pool.log_pool_status()
            if self.instance_manager:
                self.logger.info(self.instance_manager.get_status_report())
            self.last_stat_time = time.time()

        # è·³è¿‡å›¾ç‰‡è¯·æ±‚
        if 'jpg' in request.url:
            return None

        try:
            # ç¡®ä¿æµè§ˆå™¨æ± å·²åˆå§‹åŒ–
            if not self.browser_pool:
                self.browser_pool = BrowserPool(
                    max_browsers=spider.settings.getint('PLAYWRIGHT_POOL_SIZE', 4),
                    proxy_manager=self.proxy_manager,
                    logger=self.logger
                )

            # è·å–å¯ç”¨æµè§ˆå™¨å®ä¾‹
            browser_index = self._select_browser_instance()
            if browser_index is None:
                self.logger.error("No available browser instances")
                return None

            # å°è¯•è·å–é¡µé¢ï¼ˆæœ€å¤š3æ¬¡ï¼‰
            for attempt in range(3):
                try:
                    # ã€è¯Šæ–­æ—¥å¿—ã€‘è®°å½•é¡µé¢è·å–å¼€å§‹
                    self.logger.info(f"ğŸš€ [é¡µé¢è·å–] å¼€å§‹è·å–é¡µé¢: {request.url[:80]}... (æµè§ˆå™¨å®ä¾‹: {browser_index}, å°è¯•: {attempt + 1}/3)")
                    
                    start_time = time.time()
                    result = self.browser_pool.fetch_page(
                        request.url, self.cookie_manager.cookies, browser_index
                    )
                    response_time = time.time() - start_time

                    # è®°å½•æˆåŠŸ
                    self.browser_pool.stats.log_request(True, response_time)
                    if self.instance_manager:
                        self.instance_manager.report_success(browser_index, response_time)

                    self.logger.info(f"âœ… [é¡µé¢è·å–æˆåŠŸ] {request.url[:80]}... ({response_time:.2f}s)")

                    return scrapy.http.HtmlResponse(
                        url=result['url'],
                        body=result['content'].encode('utf-8'),
                        encoding='utf-8',
                        request=request,
                        status=200  # æ˜ç¡®è®¾ç½®çŠ¶æ€ç 
                    )

                except (PlaywrightTimeoutError, Exception) as e:
                    error_type = type(e).__name__

                    # æŠ¥å‘Šå¤±è´¥
                    is_banned = False
                    if self.instance_manager:
                        is_banned = self.instance_manager.report_failure(browser_index, e)

                    with self._lock:
                        self._failed_browsers[browser_index] = time.time()

                    self.logger.warning(
                        f"Browser {browser_index} failed (attempt {attempt + 1}/3): "
                        f"{error_type}: {str(e)[:100]}..."
                    )

                    if is_banned:
                        self.logger.warning(f"Browser {browser_index} banned, will be replaced")

                    # é€‰æ‹©ä¸‹ä¸€ä¸ªå®ä¾‹
                    browser_index = self._select_browser_instance()
                    if browser_index is None:
                        break

                    # å°è¯•ä¸‹ä¸€ä¸ªå®ä¾‹
                    continue

            # æ‰€æœ‰å°è¯•éƒ½å¤±è´¥
            self.browser_pool.stats.log_timeout()
            return scrapy.http.Response(
                url=request.url,
                status=408,
                body=b'',
                headers={'Retry-After': '30'}
            )

        except Exception as e:
            self.logger.error(f"Request processing failed: {e}")
            return None

    def _select_browser_instance(self) -> Optional[int]:
        """é€‰æ‹©å¯ç”¨çš„æµè§ˆå™¨å®ä¾‹"""
        pool_size = self.browser_pool.max_browsers if self.browser_pool else 1

        # é¦–å…ˆå°è¯•ä½¿ç”¨å®ä¾‹ç®¡ç†å™¨æ¨èçš„å®ä¾‹
        if self.instance_manager:
            selected = self.instance_manager.get_available_instance_id()
            if selected is not None and selected not in self.ban_detector.browser_instances:
                proxy_addr = None
                if self.proxy_manager:
                    try:
                        proxy_dict = self.proxy_manager.get_random_proxy(mark_used=False)
                        proxy_addr = proxy_dict.get('proxy') if proxy_dict else None
                    except:
                        pass
                self.instance_manager.register_instance(selected, proxy_addr)

                if not self.ban_detector.is_instance_banned(selected):
                    # æ£€æŸ¥å¤±è´¥é»‘åå•
                    with self._lock:
                        if selected not in self._failed_browsers or \
                           time.time() - self._failed_browsers[selected] >= 300:
                            return selected

        # è½®è¯¢é€‰æ‹©
        for _ in range(pool_size):
            idx = self._browser_index % pool_size
            self._browser_index += 1

            # æ£€æŸ¥å°ç¦çŠ¶æ€
            if self.ban_detector.is_instance_banned(idx):
                continue

            # æ£€æŸ¥å¤±è´¥é»‘åå•
            with self._lock:
                if idx in self._failed_browsers:
                    if time.time() - self._failed_browsers[idx] < 300:
                        continue
                    else:
                        del self._failed_browsers[idx]

            # æ³¨å†Œå®ä¾‹
            if self.instance_manager and idx not in self.ban_detector.browser_instances:
                proxy_addr = None
                if self.proxy_manager:
                    try:
                        proxy_dict = self.proxy_manager.get_random_proxy(mark_used=False)
                        proxy_addr = proxy_dict.get('proxy') if proxy_dict else None
                    except:
                        pass
                self.instance_manager.register_instance(idx, proxy_addr)

            return idx

        return None

    def _replace_browser_instance(self, failed_instance_id: int) -> Optional[int]:
        """æ›¿æ¢å¤±è´¥çš„æµè§ˆå™¨å®ä¾‹"""
        self.logger.info(f"Replacing browser instance {failed_instance_id}")
        return (failed_instance_id + 1000) % 10000

    def close_spider(self, spider):
        """å…³é—­Spider"""
        if self.browser_pool:
            self.browser_pool.close()
