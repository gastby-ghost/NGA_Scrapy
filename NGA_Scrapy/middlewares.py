import scrapy
import json
import os
import time
import threading
import signal
import sys
from queue import Queue, Empty
from contextlib import contextmanager
from datetime import datetime
from playwright.sync_api import TimeoutError as PlaywrightTimeoutError
from playwright.sync_api import sync_playwright
from scrapy.exceptions import NotConfigured
from typing import Optional, Dict, List, Tuple
from scrapy.downloadermiddlewares.retry import RetryMiddleware
from scrapy.utils.response import response_status_message
import random
from concurrent.futures import ThreadPoolExecutor, Future
import uuid
from scrapy import signals
from NGA_Scrapy.utils.proxy_manager import get_proxy_manager

class PerformanceStats:
    """æ€§èƒ½ç»Ÿè®¡å·¥å…·ç±»"""
    def __init__(self):
        self.start_time = time.time()
        self.request_count = 0
        self.success_count = 0
        self.failed_count = 0
        self.total_page_time = 0
        self.max_page_time = 0
        self.min_page_time = float('inf')
        self.browser_recycles = 0
        self.timeout_errors = 0
        self._lock = threading.Lock()

    def log_request(self, success: bool, duration: float):
        """è®°å½•è¯·æ±‚ç»Ÿè®¡"""
        with self._lock:
            self.request_count += 1
            if success:
                self.success_count += 1
                self.total_page_time += duration
                self.max_page_time = max(self.max_page_time, duration)
                self.min_page_time = min(self.min_page_time, duration)
            else:
                self.failed_count += 1

    def log_recycle(self):
        """è®°å½•å®ä¾‹å›æ”¶"""
        with self._lock:
            self.browser_recycles += 1

    def log_timeout(self):
        """è®°å½•è¶…æ—¶äº‹ä»¶"""
        with self._lock:
            self.timeout_errors += 1

    def get_stats(self) -> Dict:
        """è·å–å½“å‰ç»Ÿè®¡ä¿¡æ¯"""
        avg_time = (self.total_page_time / self.success_count) if self.success_count > 0 else 0
        uptime = time.time() - self.start_time
        return {
            'uptime': uptime,
            'requests': self.request_count,
            'success_rate': f"{(self.success_count / self.request_count * 100):.2f}%" if self.request_count > 0 else "0%",
            'avg_page_time': f"{avg_time:.3f}s",
            'max_page_time': f"{self.max_page_time:.3f}s",
            'min_page_time': f"{self.min_page_time:.3f}s" if self.min_page_time != float('inf') else "N/A",
            'browser_recycles': self.browser_recycles,
            'timeout_errors': self.timeout_errors,
            'req_per_minute': f"{(self.request_count / (uptime / 60)):.2f}" if uptime > 0 else "N/A"
        }

class BrowserPool:
    """Playwrightæµè§ˆå™¨è¿æ¥æ± ï¼ˆå¸¦æ€§èƒ½ç›‘æ§ï¼‰- çº¿ç¨‹å®‰å…¨ç‰ˆæœ¬"""
    def __init__(self, max_browsers: int = 4, spider_logger=None, proxy_manager=None):
        self.max_browsers = max_browsers
        self.logger = spider_logger
        self.stats = PerformanceStats()
        self.proxy_manager = proxy_manager
        self._request_queue = Queue()
        self._result_map = {}
        self._lock = threading.Lock()
        self._condition = threading.Condition(self._lock)
        self._playwright_thread = None
        self._stop_event = threading.Event()
        self._initialized = False
        self._init_condition = threading.Condition(self._lock)
        self._shutdown_handled = False

        # å¯åŠ¨Playwrightå·¥ä½œçº¿ç¨‹ï¼ˆæ”¹ä¸ºéå®ˆæŠ¤çº¿ç¨‹ä»¥ç¡®ä¿æ­£ç¡®å…³é—­ï¼‰
        self._start_playwright_thread()

        # ç­‰å¾…åˆå§‹åŒ–å®Œæˆ
        with self._init_condition:
            while not self._initialized:
                self._init_condition.wait(timeout=1)

    def _start_playwright_thread(self):
        """å¯åŠ¨Playwrightå·¥ä½œçº¿ç¨‹"""
        self._playwright_thread = threading.Thread(
            target=self._playwright_worker,
            name="PlaywrightWorker",
            daemon=False  # æ”¹ä¸ºéå®ˆæŠ¤çº¿ç¨‹ï¼Œç¡®ä¿æ­£ç¡®å…³é—­
        )
        self._playwright_thread.start()

    def _playwright_worker(self):
        """Playwrightå·¥ä½œçº¿ç¨‹"""
        try:
            self.logger.info("æ­£åœ¨åˆå§‹åŒ–Playwrightå·¥ä½œçº¿ç¨‹...")
            playwright = sync_playwright().start()
            self.logger.info("Playwrightåˆå§‹åŒ–å®Œæˆ")

            # åˆ›å»ºæµè§ˆå™¨æ± 
            browser_pool = []
            for _ in range(self.max_browsers):
                browser = playwright.chromium.launch(
                    headless=True,
                    args=[
                        '--disable-gpu',
                        '--no-sandbox',
                        '--disable-dev-shm-usage',
                        '--disable-extensions',
                        '--disable-infobars',
                        '--disable-notifications',
                    ]
                )

                # æ„å»ºcontextå‚æ•°
                context_kwargs = {
                    'viewport': {'width': 1920, 'height': 1080},
                    'java_script_enabled': True,
                    'ignore_https_errors': True,
                    'permissions': [],
                    'extra_http_headers': {
                        'User-Agent': 'Mozilla/5.0 (Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                        'Accept-Encoding': 'gzip, deflate, br',
                        'Cache-Control': 'no-cache',
                        'Pragma': 'no-cache',
                        'Sec-Fetch-Dest': 'document',
                        'Sec-Fetch-Mode': 'navigate',
                        'Sec-Fetch-Site': 'none',
                        'Sec-Fetch-User': '?1',
                        'Sec-Ch-Ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
                        'Sec-Ch-Ua-Mobile': '?0',
                        'Sec-Ch-Ua-Platform': '"Linux"',
                        'Upgrade-Insecure-Requests': '1'
                    }
                }

                # å¦‚æœå¯ç”¨äº†ä»£ç†ï¼Œè®¾ç½®ä»£ç†
                if self.proxy_manager:
                    self.logger.debug(f"ğŸ” è·å–éšæœºä»£ç† (æ± ä¸­æœ‰ {len(self.proxy_manager.proxy_pool)} ä¸ªä»£ç†)")
                    proxy_dict = self.proxy_manager.get_random_proxy()
                    if proxy_dict and proxy_dict.get('proxy'):
                        # æ„å»ºä»£ç†æœåŠ¡å™¨åœ°å€
                        proxy_server = proxy_dict['proxy']
                        auth_info = ""
                        if 'username' in proxy_dict and 'password' in proxy_dict:
                            auth_info = f" (è®¤è¯: {proxy_dict['username']})"

                        # æ„å»ºä»£ç†è®¾ç½®
                        proxy_config = {
                            'server': proxy_server,
                            'bypass': 'localhost;127.0.0.1;*.nga.cn;*.ngabbs.com'
                        }

                        # å¦‚æœæœ‰è®¤è¯ä¿¡æ¯ï¼Œæ·»åŠ è®¤è¯
                        if 'username' in proxy_dict and 'password' in proxy_dict:
                            proxy_config['username'] = proxy_dict['username']
                            proxy_config['password'] = proxy_dict['password']

                        context_kwargs['proxy'] = proxy_config
                        self.logger.info(f"ğŸŒ ä½¿ç”¨ä»£ç†: {proxy_server}{auth_info}")
                    else:
                        self.logger.warning("âš ï¸ æœªè·å–åˆ°å¯ç”¨ä»£ç†ï¼Œä½¿ç”¨ç›´è¿")

                context = browser.new_context(**context_kwargs)
                browser_pool.append((browser, context))

            self.logger.info(f"æµè§ˆå™¨æ± åˆå§‹åŒ–å®Œæˆï¼Œå…±{len(browser_pool)}ä¸ªå®ä¾‹")

            # é€šçŸ¥åˆå§‹åŒ–å®Œæˆ
            with self._init_condition:
                self._initialized = True
                self._init_condition.notify_all()

            # å·¥ä½œçº¿ç¨‹ä¸»å¾ªç¯
            while not self._stop_event.is_set():
                try:
                    # ä»é˜Ÿåˆ—è·å–ä»»åŠ¡ï¼Œä½¿ç”¨è¾ƒçŸ­çš„è¶…æ—¶ä»¥ä¾¿åŠæ—¶å“åº”åœæ­¢ä¿¡å·
                    request_id, task_func, args, kwargs, result_event = self._request_queue.get(timeout=0.1)

                    # æ£€æŸ¥åœæ­¢äº‹ä»¶ï¼ˆé¿å…åœ¨æ‰§è¡Œä»»åŠ¡æ—¶è¢«é˜»å¡ï¼‰
                    if self._stop_event.is_set():
                        # å¤„ç†é˜Ÿåˆ—ä¸­å‰©ä½™çš„ä»»åŠ¡
                        if result_event:
                            result_event.set()
                        break

                    # æ‰§è¡Œä»»åŠ¡
                    try:
                        result = task_func(browser_pool, *args, **kwargs)
                        with self._condition:
                            self._result_map[request_id] = ('success', result, None)
                    except Exception as e:
                        # ä¿å­˜å¼‚å¸¸å¯¹è±¡åŠå…¶ç±»å‹ä¿¡æ¯
                        import traceback
                        with self._condition:
                            self._result_map[request_id] = ('error', None, (type(e), e, traceback.format_exc()))
                    finally:
                        with self._condition:
                            self._condition.notify_all()

                    # é€šçŸ¥ä»»åŠ¡å®Œæˆ
                    result_event.set()

                except Empty:
                    # ç©ºé˜Ÿåˆ—ï¼Œæ£€æŸ¥åœæ­¢äº‹ä»¶
                    continue
                except Exception as e:
                    self.logger.error(f"å·¥ä½œçº¿ç¨‹å¤„ç†ä»»åŠ¡æ—¶å‡ºé”™: {str(e)}")

            # å¤„ç†é˜Ÿåˆ—ä¸­çš„å‰©ä½™ä»»åŠ¡ï¼ˆå¿«é€Ÿæ¸…ç†ï¼‰
            remaining_tasks = []
            while True:
                try:
                    request_id, task_func, args, kwargs, result_event = self._request_queue.get_nowait()
                    remaining_tasks.append((request_id, result_event))
                except Empty:
                    break

            # æ ‡è®°è¿™äº›ä»»åŠ¡ä¸ºå·²å–æ¶ˆ
            for request_id, result_event in remaining_tasks:
                with self._condition:
                    self._result_map[request_id] = ('canceled', None, 'ä»»åŠ¡è¢«å–æ¶ˆ')
                self._condition.notify_all()
                if result_event:
                    result_event.set()

            # æ¸…ç†èµ„æº
            self.logger.info("æ­£åœ¨å…³é—­æµè§ˆå™¨å®ä¾‹...")
            for browser, context in browser_pool:
                try:
                    context.close()
                    browser.close()
                except Exception as e:
                    self.logger.warning(f"å…³é—­èµ„æºæ—¶å‡ºé”™: {str(e)}")

            playwright.stop()
            self.logger.info("Playwrightå·¥ä½œçº¿ç¨‹å·²é€€å‡º")

        except Exception as e:
            self.logger.error(f"Playwrightå·¥ä½œçº¿ç¨‹åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            with self._init_condition:
                self._initialized = True
                self._init_condition.notify_all()

    def _execute_in_playwright_thread(self, task_func, *args, **kwargs):
        """åœ¨Playwrightå·¥ä½œçº¿ç¨‹ä¸­æ‰§è¡Œä»»åŠ¡"""
        # æ£€æŸ¥æ˜¯å¦å·²æ”¶åˆ°åœæ­¢ä¿¡å·
        if self._stop_event.is_set():
            self.logger.warning("âš ï¸ æµè§ˆå™¨æ± æ­£åœ¨å…³é—­ï¼Œä»»åŠ¡è¢«ä¸­æ–­")
            raise InterruptedError("æµè§ˆå™¨æ± æ­£åœ¨å…³é—­ï¼Œä»»åŠ¡è¢«ä¸­æ–­")

        request_id = str(uuid.uuid4())
        result_event = threading.Event()
        task_name = getattr(task_func, '__name__', str(task_func))

        self.logger.debug(f"ğŸ“¥ æ¥æ”¶ä»»åŠ¡: {task_name} (ID: {request_id})")

        with self._condition:
            self._result_map[request_id] = None

        # æ·»åŠ ä»»åŠ¡åˆ°é˜Ÿåˆ—
        self._request_queue.put((request_id, task_func, args, kwargs, result_event))
        self.logger.debug(f"âœ… ä»»åŠ¡å·²æ·»åŠ åˆ°é˜Ÿåˆ—: {task_name} (ID: {request_id})")

        # ç­‰å¾…ç»“æœï¼Œä½¿ç”¨è¾ƒçŸ­çš„è¶…æ—¶å¹¶æ£€æŸ¥åœæ­¢äº‹ä»¶
        timeout = 60
        wait_interval = 0.5  # åˆ†æ®µç­‰å¾…ä»¥ä¾¿åŠæ—¶å“åº”åœæ­¢ä¿¡å·
        elapsed = 0

        while elapsed < timeout:
            # æ£€æŸ¥åœæ­¢äº‹ä»¶
            if self._stop_event.is_set():
                self.logger.warning(f"âš ï¸ ä»»åŠ¡æ‰§è¡ŒæœŸé—´æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œå–æ¶ˆä»»åŠ¡: {task_name} (ID: {request_id})")
                with self._condition:
                    self._result_map.pop(request_id, None)
                raise InterruptedError("æµè§ˆå™¨æ± æ­£åœ¨å…³é—­ï¼Œä»»åŠ¡è¢«ä¸­æ–­")

            # ç­‰å¾…ä¸€å°æ®µæ—¶é—´
            if result_event.wait(timeout=wait_interval):
                break
            elapsed += wait_interval

            # æ¯10ç§’è¾“å‡ºä¸€æ¬¡ç­‰å¾…æ—¥å¿—
            if elapsed > 0 and elapsed % 10 == 0:
                self.logger.debug(f"â³ ä»»åŠ¡ä»åœ¨æ‰§è¡Œä¸­: {task_name} (ID: {request_id})ï¼Œå·²ç­‰å¾… {elapsed:.0f}s")

        with self._condition:
            status, result, error = self._result_map.pop(request_id, ('timeout', None, 'Timeout'))

            if status == 'success':
                self.logger.debug(f"âœ… ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ: {task_name} (ID: {request_id})ï¼Œè€—æ—¶ {elapsed:.2f}s")
                return result
            elif status == 'canceled':
                self.logger.warning(f"âŒ ä»»åŠ¡è¢«å–æ¶ˆ: {task_name} (ID: {request_id})")
                raise InterruptedError("ä»»åŠ¡è¢«å–æ¶ˆ")
            elif status == 'error':
                # é‡æ–°æŠ›å‡ºåŸå§‹å¼‚å¸¸ï¼ˆä¿æŒå¼‚å¸¸ç±»å‹ï¼‰
                exc_type, exc_value, exc_traceback = error
                self.logger.error(f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {task_name} (ID: {request_id})ï¼Œé”™è¯¯: {exc_type.__name__}: {exc_value}")
                raise exc_type(exc_value)
            else:
                self.logger.error(f"â° ä»»åŠ¡æ‰§è¡Œè¶…æ—¶: {task_name} (ID: {request_id})ï¼Œè¶…æ—¶æ—¶é—´ {timeout}s")
                raise TimeoutError('ä»»åŠ¡æ‰§è¡Œè¶…æ—¶')

    def execute(self, task_func, *args, **kwargs):
        """æ‰§è¡Œä¸€ä¸ªPlaywrightæ“ä½œ"""
        return self._execute_in_playwright_thread(task_func, *args, **kwargs)

    def log_pool_status(self):
        """è®°å½•å½“å‰è¿æ¥æ± çŠ¶æ€"""
        stats = self.stats.get_stats()
        status_report = "\n".join([f"{k}: {v}" for k, v in stats.items()])
        self.logger.info(f"æ€§èƒ½ç»Ÿè®¡æŠ¥å‘Š:\n{status_report}")

    def close(self, force=False):
        """å…³é—­æ‰€æœ‰èµ„æº

        Args:
            force: æ˜¯å¦å¼ºåˆ¶ç«‹å³å…³é—­ï¼ˆç”¨äºä¿¡å·å¤„ç†ï¼‰
        """
        self.logger.info("æ­£åœ¨å…³é—­æµè§ˆå™¨æ± ...")

        # å‘é€åœæ­¢ä¿¡å·
        self._stop_event.set()

        if self._playwright_thread and self._playwright_thread.is_alive():
            # å¢åŠ è¶…æ—¶æ—¶é—´ä»¥ç¡®ä¿ä¼˜é›…å…³é—­
            timeout = 5 if force else 30
            self.logger.info(f"ç­‰å¾…å·¥ä½œçº¿ç¨‹ç»“æŸï¼Œè¶…æ—¶æ—¶é—´: {timeout}ç§’...")

            # å‘é€SIGINTåˆ°å·¥ä½œçº¿ç¨‹ä»¥åŠ é€Ÿå…³é—­
            if force and hasattr(signal, 'pthread_sigmask'):
                try:
                    # å°è¯•ä¸­æ–­é˜Ÿåˆ—ç­‰å¾…
                    import ctypes
                    ctypes.pythonapi.PyThread_set_thread_name(
                        self._playwright_thread.ident, "PlaywrightWorker"
                    )
                except:
                    pass

            self._playwright_thread.join(timeout=timeout)

            # æ£€æŸ¥çº¿ç¨‹æ˜¯å¦ä»åœ¨è¿è¡Œ
            if self._playwright_thread.is_alive():
                self.logger.warning("å·¥ä½œçº¿ç¨‹æœªèƒ½åœ¨è¶…æ—¶æ—¶é—´å†…ç»“æŸï¼Œå¼ºåˆ¶å…³é—­")
                # å¼ºåˆ¶å…³é—­æµè§ˆå™¨å®ä¾‹
                # æ³¨æ„ï¼šç”±äºæµè§ˆå™¨å®ä¾‹åœ¨å·¥ä½œçº¿ç¨‹ä¸­ï¼Œè¿™é‡Œåªèƒ½è®°å½•è­¦å‘Š

        self.log_pool_status()
        self.logger.info("æµè§ˆå™¨æ± å·²å…³é—­")


class PlaywrightMiddleware:
    def __init__(self):
        self.browser_pool = None
        self.logger = None
        self.cookies = None
        self.proxy_manager = None
        self.last_stat_time = time.time()
        self._browser_index = 0  # ç”¨äºè½®è¯¢é€‰æ‹©æµè§ˆå™¨å®ä¾‹

    @classmethod
    def from_crawler(cls, crawler):
        if not crawler.settings.getbool('PLAYWRIGHT_ENABLED', True):
            raise NotConfigured('Playwright middleware not enabled')

        middleware = cls()
        crawler.signals.connect(middleware.spider_opened, signal=signals.spider_opened)
        crawler.signals.connect(middleware.spider_closed, signal=signals.spider_closed)
        return middleware

    def spider_opened(self, spider):
        """Spiderå¯åŠ¨æ—¶çš„å¤„ç†"""
        self.logger = spider.logger
        self.logger.info("=" * 60)
        self.logger.info("ğŸš€ Playwrightä¸­é—´ä»¶å·²å¯åŠ¨ï¼Œä¿¡å·å¤„ç†å™¨å·²æ³¨å†Œ")
        self.logger.info("=" * 60)

        # æ£€æŸ¥æ˜¯å¦å¯ç”¨ä»£ç†
        proxy_enabled = spider.settings.getbool('PROXY_ENABLED', False)
        self.logger.info(f"ğŸ” æ£€æŸ¥ä»£ç†è®¾ç½®: PROXY_ENABLED = {proxy_enabled}")

        if proxy_enabled:
            try:
                self.logger.info("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–ä»£ç†ç®¡ç†å™¨...")
                self.proxy_manager = get_proxy_manager()
                self.logger.info("âœ… ä»£ç†ç®¡ç†å™¨å·²åˆå§‹åŒ–")

                # è·å–åˆå§‹ä»£ç†åˆ—è¡¨
                self.logger.info("ğŸ”„ æ­£åœ¨è·å–åˆå§‹ä»£ç†åˆ—è¡¨...")
                proxies = self.proxy_manager.get_proxies(force_refresh=True)

                if proxies:
                    self.logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(proxies)} ä¸ªä»£ç†")
                    self.logger.info(f"ğŸ“‹ ä»£ç†åˆ—è¡¨: {', '.join(proxies[:5])}")
                    if len(proxies) > 5:
                        self.logger.info(f"   ... ç­‰å…± {len(proxies)} ä¸ªä»£ç†")
                else:
                    self.logger.warning("âš ï¸ æœªè·å–åˆ°ä»»ä½•ä»£ç†")

                # æ˜¾ç¤ºä»£ç†æ± çŠ¶æ€
                status = self.proxy_manager.get_pool_status()
                self.logger.info("ğŸ“Š ä»£ç†æ± åˆå§‹çŠ¶æ€:")
                for key, value in status.items():
                    self.logger.info(f"   - {key}: {value}")

                self.logger.info("=" * 60)

            except FileNotFoundError as e:
                self.logger.error("=" * 60)
                self.logger.error(f"âŒ ä»£ç†é…ç½®æ–‡ä»¶é”™è¯¯: {e}")
                self.logger.error("è¯·ç¡®ä¿ proxy_config.json æ–‡ä»¶å­˜åœ¨ä¸”é…ç½®æ­£ç¡®")
                self.logger.error("=" * 60)
                self.proxy_manager = None

            except ValueError as e:
                self.logger.error("=" * 60)
                self.logger.error(f"âŒ ä»£ç†é…ç½®éªŒè¯é”™è¯¯: {e}")
                self.logger.error("è¯·æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­çš„ trade_no å’Œ api_key æ˜¯å¦æ­£ç¡®")
                self.logger.error("=" * 60)
                self.proxy_manager = None

            except Exception as e:
                self.logger.error("=" * 60)
                self.logger.error(f"âŒ ä»£ç†ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
                self.logger.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
                self.logger.error("=" * 60)
                self.proxy_manager = None
        else:
            self.logger.info("â„¹ï¸  ä»£ç†æœªå¯ç”¨ (PROXY_ENABLED = False)")
            self.logger.info("=" * 60)

    def spider_closed(self, spider, reason):
        """Spiderå…³é—­æ—¶çš„å¤„ç†"""
        self.logger.info(f"Spiderå…³é—­åŸå› : {reason}")
        if self.browser_pool:
            self.logger.info("æ­£åœ¨é€šè¿‡ä¿¡å·å¤„ç†å™¨å…³é—­æµè§ˆå™¨æ± ...")
            self.browser_pool.close()


    def _load_cookies(self) -> None:
        """
        é¢„åŠ è½½cookiesï¼ˆå¸¦æ€§èƒ½ç›‘æ§å’Œé”™è¯¯å¤„ç†ï¼‰
        åŠŸèƒ½ç‰¹æ€§ï¼š
        1. æ–‡ä»¶å­˜åœ¨æ€§æ£€æŸ¥
        2. JSONæ ¼å¼éªŒè¯
        3. å­—æ®µå…¼å®¹æ€§å¤„ç†
        4. æ€§èƒ½ç›‘æ§
        5. è¯¦ç»†çš„é”™è¯¯å¤„ç†
        """
        start_time = time.time()
        cookies_file = 'cookies.txt'
        
        # 1. æ–‡ä»¶æ£€æŸ¥
        if not os.path.exists(cookies_file):
            self.logger.info(f"Cookiesæ–‡ä»¶æœªæ‰¾åˆ°: {cookies_file}")
        
        try:
            # 2. æ–‡ä»¶è¯»å–å’ŒJSONè§£æ
            with open(cookies_file, 'r', encoding='utf-8') as f:
                cookies = json.load(f)
            
            # 3. æ€§èƒ½ç›‘æ§ç‚¹
            parse_time = time.time()
            
            # 4. Cookiesè½¬æ¢å¤„ç†
            processed_cookies = []
            for c in cookies:
                try:
                    # å¤„ç†expiry/expirationDateå­—æ®µ
                    expiry = c.get('expiry') or c.get('expirationDate')
                    if expiry is None:
                        expiry = int(time.time()) + 3600  # é»˜è®¤1å°æ—¶è¿‡æœŸ
                    elif isinstance(expiry, float):
                        expiry = int(expiry)
                    
                    # å¤„ç†domainå­—æ®µ
                    domain = c.get('domain', '.ngabbs.com')
                    if not domain.startswith('.'):
                        domain = f'.{domain}'
                    
                    # æ„å»ºæ ‡å‡†cookieå­—å…¸
                    cookie_dict = {
                        'name': c['name'],
                        'value': c['value'],
                        'domain': domain,
                        'path': c.get('path', '/'),
                        'expires': expiry,
                        'httpOnly': c.get('httpOnly', False),
                        'secure': c.get('secure', False),
                        'sameSite': 'Lax'
                    }
                    
                    # ç§»é™¤Noneå€¼
                    cookie_dict = {k: v for k, v in cookie_dict.items() if v is not None}
                    processed_cookies.append(cookie_dict)
                
                except KeyError as e:
                    self.logger.info(f"[è­¦å‘Š] å¿½ç•¥æ— æ•ˆcookieæ¡ç›®ï¼Œç¼ºå°‘å¿…è¦å­—æ®µ: {e}")
                    continue
            
            # 5. è®¾ç½®å¤„ç†åçš„cookies
            self.cookies = processed_cookies
            
            
        except json.JSONDecodeError as e:
            self.logger.info(f"Cookiesæ–‡ä»¶JSONæ ¼å¼é”™è¯¯: {e}")
        except Exception as e:
            self.logger.info(f"åŠ è½½cookiesæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

    def _save_cookies_if_updated(self, context):
        """
        æ£€æŸ¥å¹¶ä¿å­˜æ›´æ–°åçš„cookies
        è¿™ä¸ªæ–¹æ³•ä¼šåœ¨æ¯æ¬¡æˆåŠŸè®¿é—®é¡µé¢åè¢«è°ƒç”¨ï¼Œè‡ªåŠ¨æ›´æ–°ngaPassportUidç­‰cookies
        """
        try:
            # ä»å½“å‰contextè·å–æ‰€æœ‰cookies
            current_cookies = context.cookies()

            # æŸ¥æ‰¾ngaPassportUidï¼ˆçŸ­æ•ˆcookieï¼‰
            nga_passport = next(
                (c for c in current_cookies if c['name'] == 'ngaPassportUid'),
                None
            )

            if nga_passport:
                # æŸ¥æ‰¾æ—§çš„ngaPassportUid
                old_nga_passport = next(
                    (c for c in self.cookies if c['name'] == 'ngaPassportUid'),
                    None
                )

                # å¦‚æœæ—§çš„ä¸å­˜åœ¨ï¼Œæˆ–å€¼/è¿‡æœŸæ—¶é—´ä¸åŒï¼Œåˆ™éœ€è¦æ›´æ–°
                needs_update = (
                    not old_nga_passport or
                    old_nga_passport['value'] != nga_passport['value'] or
                    old_nga_passport.get('expires', 0) != nga_passport.get('expires', 0)
                )

                if needs_update:
                    # ä¿å­˜æ‰€æœ‰cookiesåˆ°æ–‡ä»¶
                    with open('cookies.txt', 'w', encoding='utf-8') as f:
                        json.dump(current_cookies, f, ensure_ascii=False, indent=2)

                    # æ›´æ–°å†…å­˜ä¸­çš„cookiesç¼“å­˜
                    self.cookies = current_cookies

                    # è®°å½•æ—¥å¿—
                    expires_time = time.strftime(
                        '%Y-%m-%d %H:%M:%S',
                        time.localtime(nga_passport['expires'])
                    )
                    self.logger.info(
                        f"âœ“ å·²è‡ªåŠ¨æ›´æ–° ngaPassportUid - "
                        f"æ–°å€¼: {nga_passport['value'][:20]}..., "
                        f"è¿‡æœŸæ—¶é—´: {expires_time}"
                    )

        except Exception as e:
            self.logger.error(f"è‡ªåŠ¨æ›´æ–°cookieså¤±è´¥: {e}")

    def process_request(self, request, spider):
        if not self.logger:
            self.logger = spider.logger

        if not self.cookies:
            self._load_cookies()

        if not self.browser_pool:
            self.browser_pool = BrowserPool(
                max_browsers=spider.settings.getint('PLAYWRIGHT_POOL_SIZE', 4),
                spider_logger=spider.logger,
                proxy_manager=self.proxy_manager
            )

        # æ¯å°æ—¶è¾“å‡ºä¸€æ¬¡ç»Ÿè®¡æŠ¥å‘Š
        if time.time() - self.last_stat_time > 3600:
            self.browser_pool.log_pool_status()
            self.last_stat_time = time.time()

        if 'jpg' in request.url:
            return None

        try:
            # å®šä¹‰æµè§ˆå™¨ä»»åŠ¡å‡½æ•°
            def _fetch_page(browser_pool, url, cookies, browser_index):
                browser, context = browser_pool[browser_index % len(browser_pool)]
                page = context.new_page()

                try:
                    self.logger.debug(f"ğŸŒ å‡†å¤‡åŠ è½½é¡µé¢: {url} (ä½¿ç”¨æµè§ˆå™¨ {browser_index % len(browser_pool)})")

                    if cookies:
                        self.logger.debug(f"ğŸª è®¾ç½® Cookiesï¼Œå…± {len(cookies)} ä¸ª")
                        # å…ˆæ¸…é™¤æ—§çš„ cookies
                        context.clear_cookies()
                        # æ·»åŠ æ–°çš„ cookies
                        context.add_cookies(cookies)
                        # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿ cookies è®¾ç½®å®Œæˆ
                        time.sleep(0.1)

                    # è®¾ç½® Referer å¤´éƒ¨ï¼Œæ¨¡æ‹Ÿä»é¦–é¡µè·³è½¬
                    self.logger.debug(f"ğŸ“‹ è®¾ç½® Referer å¤´éƒ¨")
                    page.set_extra_http_headers({
                        'Referer': 'https://bbs.nga.cn/'
                    })

                    self.logger.debug(f"ğŸš€ å¼€å§‹å¯¼èˆªåˆ°é¡µé¢...")
                    nav_start = time.time()
                    page.goto(url, wait_until="domcontentloaded", timeout=15000)
                    nav_time = time.time() - nav_start
                    self.logger.debug(f"âœ… é¡µé¢å¯¼èˆªå®Œæˆï¼Œè€—æ—¶: {nav_time:.2f}sï¼ŒURL: {page.url}")

                    alert_start = time.time()
                    self._handle_alert(page)
                    alert_time = time.time() - alert_start

                    load_start = time.time()
                    page.wait_for_load_state("domcontentloaded", timeout=5000)
                    load_time = time.time() - load_start
                    self.logger.debug(f"â±ï¸ é¡µé¢åŠ è½½å®Œæˆ: nav={nav_time:.2f}s, alert={alert_time:.2f}s, wait={load_time:.2f}s")

                    # ã€å…³é”®æ”¹è¿›ã€‘åœ¨è¿”å›å‰è‡ªåŠ¨æ›´æ–°cookies
                    self.logger.debug(f"ğŸ’¾ æ›´æ–° Cookies...")
                    self._save_cookies_if_updated(context)

                    self.logger.debug(f"ğŸ“„ é¡µé¢å†…å®¹è·å–å®Œæˆï¼Œå­—èŠ‚æ•°: {len(page.content())}")

                    return {
                        'url': page.url,
                        'content': page.content(),
                        'success': True
                    }
                except Exception as e:
                    self.logger.error(f"âŒ é¡µé¢åŠ è½½å¤±è´¥: {url}ï¼Œé”™è¯¯: {type(e).__name__}: {str(e)}")
                    raise
                finally:
                    self.logger.debug(f"ğŸ”’ å…³é—­é¡µé¢å®ä¾‹")
                    page.close()

            # è½®è¯¢é€‰æ‹©æµè§ˆå™¨å®ä¾‹
            browser_index = self._browser_index
            self._browser_index = (self._browser_index + 1) % 1000000  # é˜²æ­¢æº¢å‡º

            # åœ¨Playwrightå·¥ä½œçº¿ç¨‹ä¸­æ‰§è¡Œé¡µé¢è·å–
            result = self.browser_pool.execute(
                _fetch_page,
                request.url,
                self.cookies,
                browser_index
            )

            self.browser_pool.stats.log_request(True, 1.0)

            return scrapy.http.HtmlResponse(
                url=result['url'],
                body=result['content'].encode('utf-8'),
                encoding='utf-8',
                request=request
            )
        except PlaywrightTimeoutError as e:
            # è¶…æ—¶é”™è¯¯ï¼Œè½¬æ¢ä¸ºå¯é‡è¯•çš„å“åº”ï¼ˆä¸æ˜¾ç¤ºä¸ºé”™è¯¯ï¼‰
            self.browser_pool.stats.log_timeout()
            # æ¯100æ¬¡è¶…æ—¶è¾“å‡ºä¸€æ¬¡ç»Ÿè®¡
            if self.browser_pool.stats.timeout_errors % 100 == 0:
                self.browser_pool.log_pool_status()
            # è¿”å› 408 çŠ¶æ€ç ï¼Œè®© RetryMiddleware è‡ªåŠ¨é‡è¯•
            return scrapy.http.Response(
                url=request.url,
                status=408,  # Request Timeout
                body=b'',
                headers={'Retry-After': '30'}  # å»ºè®® 30 ç§’åé‡è¯•
            )
        except Exception as e:
            # å…¶ä»–é”™è¯¯ï¼Œè®°å½•æ—¥å¿—
            spider.logger.warning(f"Playwrightè¯·æ±‚å¤„ç†å¤±è´¥ï¼ˆéè¶…æ—¶ï¼‰: {str(e)}")
            self.browser_pool.stats.log_request(False, 0)
            return None

    def _handle_alert(self, page):
        """å¤„ç†å¼¹çª—ï¼ˆå¸¦æ€§èƒ½ç›‘æ§ï¼‰"""
        def handle_dialog(dialog):
            start_time = time.time()
            dialog.accept()
            self.logger.debug(f"å¼¹çª—å¤„ç†è€—æ—¶: {time.time() - start_time:.4f}s - {dialog.message[:50]}...")
        
        page.on('dialog', handle_dialog)

    def close_spider(self, spider):
        if self.browser_pool:
            self.browser_pool.close()

