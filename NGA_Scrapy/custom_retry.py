# -*- coding: utf-8 -*-
"""
è‡ªå®šä¹‰é‡è¯•ä¸­é—´ä»¶ï¼Œæ”¯æŒå»¶è¿Ÿé‡è¯•å’ŒæŒ‡æ•°é€€é¿
"""
import time
import random
from scrapy.downloadermiddlewares.retry import RetryMiddleware
from scrapy.utils.response import response_status_text
from scrapy.core.downloader.handlers.http11 import TunnelError
from twisted.internet import defer
from twisted.internet.error import TimeoutError, DNSLookupError, \
    ConnectionRefusedError, ConnectionDone, TCPTimedOutError


class CustomRetryMiddleware(RetryMiddleware):
    """
    è‡ªå®šä¹‰é‡è¯•ä¸­é—´ä»¶
    - æ”¯æŒæŒ‡æ•°é€€é¿å»¶è¿Ÿ
    - å¯¹è¶…æ—¶é”™è¯¯æœ‰æ›´å¥½çš„å¤„ç†
    - å‡å°‘æ—¥å¿—å™ªéŸ³
    """

    def process_response(self, request, response, spider):
        if request.method in ('HEAD',):
            return response

        # è·å–é‡è¯•æ¬¡æ•°
        retry_count = request.meta.get('retry_count', 0)
        max_retry = request.meta.get('max_retry_times', self.max_retry_times)

        # è·å–çŠ¶æ€ç å’ŒåŸå› 
        return_code = response.status
        reason = response_status_text(return_code)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡è¯•
        if return_code in self.retry_http_codes or isinstance(reason, (TimeoutError, TCPTimedOutError)):
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°
            if retry_count >= max_retry:
                spider.logger.warning(
                    f"è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retry}): {request.url}"
                )
                return response

            # è®¡ç®—é‡è¯•å»¶è¿Ÿï¼ˆæŒ‡æ•°é€€é¿ + éšæœºæŠ–åŠ¨ï¼‰
            # åŸºç¡€å»¶è¿Ÿï¼š2^(retry_count) * 2 ç§’
            base_delay = (2 ** retry_count) * 2

            # éšæœºæŠ–åŠ¨ï¼šÂ±30%
            jitter = random.uniform(0.7, 1.3)
            delay = base_delay * jitter

            # æœ€å¤§å»¶è¿Ÿ 5 åˆ†é’Ÿ
            max_delay = 300
            final_delay = min(delay, max_delay)

            # è®¾ç½®é‡è¯•å»¶è¿Ÿ
            spider.logger.info(
                f"ğŸ”„ é‡è¯•å»¶è¿Ÿ {final_delay:.1f}s (ç¬¬{retry_count + 1}/{max_retry + 1}æ¬¡): {request.url}"
            )

            # åˆ›å»ºé‡è¯•è¯·æ±‚
            retryreq = request.copy()
            retryreq.meta['retry_count'] = retry_count + 1
            retryreq.dont_filter = True
            retryreq.priority = request.priority + self.priority_adjust

            # åœ¨å“åº”å¤´ä¸­è®¾ç½® Retry-Afterï¼ˆå¦‚æœè¿˜æ²¡æœ‰çš„è¯ï¼‰
            if 'Retry-After' not in retryreq.headers:
                retryreq.headers['Retry-After'] = str(int(final_delay))

            return retryreq

        return response

    def process_exception(self, request, exception, spider):
        # å¤„ç†è¶…æ—¶ã€è¿æ¥é”™è¯¯ç­‰å¼‚å¸¸
        if isinstance(exception, (TimeoutError, TCPTimedOutError, DNSLookupError,
                                  ConnectionRefusedError, ConnectionDone)):
            # æ£€æŸ¥é‡è¯•æ¬¡æ•°
            retry_count = request.meta.get('retry_count', 0)
            max_retry = request.meta.get('max_retry_times', self.max_retry_times)

            if retry_count >= max_retry:
                return

            # åˆ›å»ºé‡è¯•è¯·æ±‚
            retryreq = request.copy()
            retryreq.meta['retry_count'] = retry_count + 1
            retryreq.dont_filter = True
            retryreq.priority = request.priority + self.priority_adjust

            # è®¡ç®—é‡è¯•å»¶è¿Ÿ
            base_delay = (2 ** retry_count) * 2
            jitter = random.uniform(0.7, 1.3)
            final_delay = min(base_delay * jitter, 300)

            spider.logger.info(
                f"ğŸ”„ å¼‚å¸¸é‡è¯•å»¶è¿Ÿ {final_delay:.1f}s (ç¬¬{retry_count + 1}/{max_retry + 1}æ¬¡): {request.url}"
            )

            return retryreq

