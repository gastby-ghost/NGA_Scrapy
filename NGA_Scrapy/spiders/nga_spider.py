# # nga_spider.py
# ä»¥ä¸‹æ˜¯NGAçˆ¬è™«ä»£ç çš„åŠŸèƒ½å’Œç‰¹æ€§æ€»ç»“ï¼ŒåŸºäºè¿™äº›ä¿¡æ¯å¯ä»¥å®Œæ•´è¿˜åŸä»£ç ï¼š

# ### 1. æ ¸å¿ƒåŠŸèƒ½
# - **ä¸»é¢˜çˆ¬å–**ï¼šçˆ¬å–NGAè®ºå›æ°´åŒº(fid=-7)å‰10é¡µçš„ä¸»é¢˜åˆ—è¡¨
# - **å›å¤çˆ¬å–**ï¼šå¯¹æ¯ä¸ªä¸»é¢˜çˆ¬å–å…¶æ‰€æœ‰å›å¤å†…å®¹
# - **ç”¨æˆ·ä¿¡æ¯çˆ¬å–**ï¼šè·å–å‘å¸–ç”¨æˆ·çš„åŸºæœ¬ä¿¡æ¯
# - **å¢é‡çˆ¬å–**ï¼šé€šè¿‡æ¯”è¾ƒæ•°æ®åº“ä¸­çš„æœ€åå›å¤æ—¶é—´ï¼Œåªçˆ¬å–æ–°å†…å®¹

# ### 2. ä¸»è¦ç‰¹æ€§

# #### æ•°æ®åº“é›†æˆ
# - ä½¿ç”¨SQLAlchemy ORMè¿›è¡Œæ•°æ®åº“æ“ä½œ
# - æ”¯æŒä»å‘½ä»¤è¡Œä¼ å…¥æ•°æ®åº“URL(db_urlå‚æ•°)
# - ä½¿ç”¨scoped_sessionç¡®ä¿çº¿ç¨‹å®‰å…¨
# - è‡ªåŠ¨åˆå§‹åŒ–/å…³é—­æ•°æ®åº“è¿æ¥
# - ç¼“å­˜ä¸»é¢˜æœ€åå›å¤æ—¶é—´å‡å°‘æ•°æ®åº“æŸ¥è¯¢

# #### å¢é‡çˆ¬å–æœºåˆ¶
# - æ¯”è¾ƒç½‘é¡µæ—¶é—´ä¸æ•°æ®åº“æ—¶é—´(`is_newer`æ–¹æ³•)
# - è·³è¿‡å·²å­˜åœ¨çš„æ—§å›å¤
# - æ”¯æŒå¤šç§NGAæ—¶é—´æ ¼å¼è§£æ(`_parse_nga_time`æ–¹æ³•)

# #### æ•°æ®æå–
# - **ä¸»é¢˜ä¿¡æ¯**ï¼šæ ‡é¢˜ã€IDã€å‘å¸–äººã€å‘å¸–æ—¶é—´ã€å›å¤æ•°ã€æœ€åå›å¤æ—¶é—´ã€åˆ†åŒº
# - **å›å¤ä¿¡æ¯**ï¼šå†…å®¹ã€æ¨èå€¼ã€å›å¤æ—¶é—´ã€çˆ¶å›å¤ID
# - **ç”¨æˆ·ä¿¡æ¯**ï¼šç”¨æˆ·ç»„ã€æ³¨å†Œæ—¥æœŸç­‰

# #### æ€§èƒ½ä¼˜åŒ–
# - ä¸»é¢˜åˆ—è¡¨é¡µå¹¶è¡Œçˆ¬å–(10é¡µå¹¶å‘)
# - å›å¤åˆ†é¡µè‡ªåŠ¨å¤„ç†
# - æ•°æ®åº“æŸ¥è¯¢ç»“æœç¼“å­˜
# - è·³è¿‡æ— æ–°å›å¤çš„ä¸»é¢˜

# #### é”™è¯¯å¤„ç†
# - å…¨é¢çš„å¼‚å¸¸æ•è·(SQLAlchemyErrorç­‰)
# - æ—¶é—´è§£æå¤±è´¥å¤„ç†
# - æ•°æ®åº“è¿æ¥å¤±è´¥å¤„ç†
# - è¯¦ç»†çš„æ—¥å¿—è®°å½•

# #### ä»£ç ç»“æ„
# - ä½¿ç”¨Scrapyæ ‡å‡†Spiderç»“æ„
# - æ¨¡å—åŒ–è®¾è®¡(æ•°æ®åº“æ“ä½œåˆ†ç¦»)
# - æ¸…æ™°çš„å›è°ƒæ–¹æ³•é“¾:
#   `parse â†’ parse_topic_list â†’ parse_replies`

# ### 3. å…³é”®æ–¹æ³•

# 1. **æ•°æ®åº“ç›¸å…³**:
#    - `_init_db`: åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
#    - `get_last_reply_from_db`: æŸ¥è¯¢ä¸»é¢˜æœ€åå›å¤æ—¶é—´
#    - `close`: æ¸…ç†èµ„æº

# 2. **çˆ¬å–é€»è¾‘**:
#    - `parse`: å…¥å£ç‚¹ï¼Œç”Ÿæˆä¸»é¢˜åˆ—è¡¨é¡µè¯·æ±‚
#    - `parse_topic_list`: è§£æä¸»é¢˜åˆ—è¡¨
#    - `parse_replies`: è§£æå›å¤å†…å®¹åŠåˆ†é¡µ
#    - `parse_user`: è§£æç”¨æˆ·ä¿¡æ¯(å½“å‰è¢«æ³¨é‡Š)

# 3. **å·¥å…·æ–¹æ³•**:
#    - `is_newer`: æ—¶é—´æ¯”è¾ƒ
#    - `_parse_nga_time`: æ—¶é—´æ ¼å¼è§£æ
#    - `_now_time`: è·å–å½“å‰æ—¶é—´

# ### 4. æ•°æ®æ¨¡å‹
# ä½¿ç”¨ä¸‰ä¸ªScrapy Item:
# - `TopicItem`: å­˜å‚¨ä¸»é¢˜ä¿¡æ¯
# - `ReplyItem`: å­˜å‚¨å›å¤ä¿¡æ¯
# - `UserItem`: å­˜å‚¨ç”¨æˆ·ä¿¡æ¯

# ### 5. é…ç½®å‚æ•°
# - `name = 'nga'`
# - `allowed_domains = ['bbs.nga.cn']`
# - `start_urls`: æ°´åŒºé¦–é¡µ
# - æ”¯æŒä»å‘½ä»¤è¡Œä¼ å…¥`db_url`

# ### 6. æ€§èƒ½è€ƒè™‘
# - æ•°æ®åº“ä¼šè¯ç®¡ç†(scoped_session)
# - å‡å°‘ä¸å¿…è¦çš„è¯·æ±‚(é€šè¿‡æ—¶é—´æ¯”è¾ƒ)
# - å¹¶å‘çˆ¬å–å¤šä¸ªä¸»é¢˜é¡µ
# - ç¼“å­˜æœºåˆ¶å‡å°‘æ•°æ®åº“æŸ¥è¯¢

# ### 7. å¾…ä¼˜åŒ–ç‚¹
# - ç”¨æˆ·ä¿¡æ¯çˆ¬å–å½“å‰è¢«æ³¨é‡Š
# - å¯æ·»åŠ è¯·æ±‚å»¶è¿Ÿæ§åˆ¶
# - å¯å¢åŠ ä»£ç†æ”¯æŒ
# - å¯æ·»åŠ æ›´è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯

import scrapy
from scrapy import Request
from ..items import TopicItem, ReplyItem, UserItem
from urllib.parse import parse_qs, urljoin
import time
from datetime import datetime
from sqlalchemy.orm import scoped_session
from sqlalchemy.exc import SQLAlchemyError
from ..models import Base
from ..utils.monitoring import get_monitor, record_batch_query
from ..utils.cache_manager import get_cache_manager
from ..utils.query_optimizer import QueryOptimizer
import psutil
import os

class NgaSpider(scrapy.Spider):
    name = 'nga'
    allowed_domains = ['bbs.nga.cn']
    start_urls = ['https://bbs.nga.cn/thread.php?fid=-7']

    def __init__(self, *args, **kwargs):
        super(NgaSpider, self).__init__(*args, **kwargs)
        # ä¸å†åœ¨å¯åŠ¨æ—¶æ¸…ç©ºæ—¥å¿—æ–‡ä»¶ï¼Œè®©Scrapyçš„æ—¥å¿—è½®è½¬æœºåˆ¶å¤„ç†
        # è°ƒåº¦å™¨éœ€è¦è¯»å–æ—¥å¿—æ–‡ä»¶è·å–ç»Ÿè®¡ä¿¡æ¯ï¼Œæ¸…ç©ºä¼šå¯¼è‡´æ•°æ®ä¸¢å¤±

        # ç¼“å­˜ä¸»é¢˜çš„æœ€æ–°å›å¤æ—¶é—´ï¼Œå‡å°‘æ•°æ®åº“æŸ¥è¯¢
        self.topic_last_reply_cache = {}
        # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        self.cache_manager = get_cache_manager()
        # åˆå§‹åŒ–æŸ¥è¯¢ä¼˜åŒ–å™¨
        self.query_optimizer = None  # å°†åœ¨æ•°æ®åº“åˆå§‹åŒ–åè®¾ç½®
        # åˆå§‹åŒ–æ•°æ®å½’æ¡£å™¨ï¼ˆæœˆåº¦å½’æ¡£ï¼‰
        self.data_archiver = None  # å°†åœ¨æ•°æ®åº“åˆå§‹åŒ–åè®¾ç½®
        # æ•°æ®åº“ç›¸å…³å±æ€§
        self.db_session = None
        self.db_url = kwargs.get('db_url')  # å…è®¸ä»å‘½ä»¤è¡Œä¼ å…¥db_url
        self.process = psutil.Process(os.getpid())  # åˆå§‹åŒ–ç›‘æ§
    
    @classmethod
    def from_crawler(cls, crawler, *args, **kwargs):
        spider = super(NgaSpider, cls).from_crawler(crawler, *args, **kwargs)
        # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        spider._init_db()
        return spider
    
    def _init_db(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥"""
        from ..utils.db_utils import create_db_session
        from ..utils.data_archiver import DataArchiver
        try:
            # ä½¿ç”¨scoped_sessionåŒ…è£…ï¼Œç¡®ä¿çº¿ç¨‹å®‰å…¨
            session_factory = create_db_session(self.db_url)
            if session_factory is None:
                raise RuntimeError("æ— æ³•åˆ›å»ºæ•°æ®åº“ä¼šè¯å·¥å‚")

            self.db_session = scoped_session(lambda: session_factory)

            # åˆå§‹åŒ–æŸ¥è¯¢ä¼˜åŒ–å™¨
            self.query_optimizer = QueryOptimizer(self.db_session, self.logger)

            # åˆå§‹åŒ–æ•°æ®å½’æ¡£å™¨ï¼ˆæœˆåº¦å½’æ¡£ï¼‰
            if self.data_archiver is None:
                self.data_archiver = DataArchiver(
                    self.db_session,
                    archive_dir='./archive',
                    config={
                        'enabled': True,
                        'archive_threshold_days': 30,  # 30å¤©æœªæ›´æ–°åˆ™å½’æ¡£
                    }
                )

            self.logger.info("æ•°æ®åº“è¿æ¥å’Œä¼˜åŒ–ç»„ä»¶åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            self.logger.error(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def close(self, reason):
        """çˆ¬è™«å…³é—­æ—¶æ¸…ç†èµ„æº"""
        # æ‰§è¡Œæœˆåº¦æ•°æ®å½’æ¡£
        if hasattr(self, 'data_archiver') and self.data_archiver:
            try:
                self.logger.info("å¼€å§‹æ‰§è¡Œæœˆåº¦æ•°æ®å½’æ¡£...")
                archive_results = self.data_archiver.auto_archive()
                self.logger.info(f"å½’æ¡£ç»“æœ: {archive_results}")

                # æ¸…ç†è¿‡æœŸå½’æ¡£æ–‡ä»¶
                cleaned_count = self.data_archiver.cleanup_old_archives(retention_days=365)
                self.logger.info(f"æ¸…ç†äº† {cleaned_count} ä¸ªè¿‡æœŸå½’æ¡£æ–‡ä»¶")

            except Exception as e:
                self.logger.error(f"æ•°æ®å½’æ¡£å¤±è´¥: {e}")

        # å…³é—­æ•°æ®åº“ä¼šè¯
        if hasattr(self, 'db_session') and self.db_session:
            try:
                self.db_session.remove()
                self.logger.info("æ•°æ®åº“ä¼šè¯å·²å…³é—­")
            except Exception as e:
                self.logger.error(f"å…³é—­æ•°æ®åº“ä¼šè¯æ—¶å‡ºé”™: {e}")
        self.logger.info(f"å…³é—­çˆ¬è™«æ–¹å¼: {reason}")
        #super().close(reason)
    
    def print_stats(self):
        """æ‰“å°è¿›åº¦å’Œæ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        cpu = self.process.cpu_percent(interval=1)
        mem = self.process.memory_info().rss / 1024 / 1024
        self.logger.debug(f"ğŸ“Š CPU: {cpu}% | Memory: {mem:.2f} MB")

        # è·å–æ•°æ®åº“ç»Ÿè®¡
        if self.db_session:
            try:
                from ..models import Topic, Reply, User
                topic_count = self.db_session.query(Topic).count()
                reply_count = self.db_session.query(Reply).count()
                user_count = self.db_session.query(User).count()
                self.logger.debug(f"ğŸ“ˆ DBç»Ÿè®¡: ä¸»é¢˜={topic_count}, å›å¤={reply_count}, ç”¨æˆ·={user_count}")
            except Exception as e:
                self.logger.debug(f"âš ï¸ è·å–æ•°æ®åº“ç»Ÿè®¡å¤±è´¥: {e}")

    def parse(self, response):
        # è§£æä¸»é¢˜åˆ—è¡¨é¡µ
        pageNum = 11
        self.logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–NGAè®ºå›æ°´åŒºï¼Œå…±éœ€çˆ¬å– {pageNum-1} é¡µä¸»é¢˜åˆ—è¡¨")
        for page in range(1, pageNum):  # çˆ¬å–å‰pageNumé¡µ
            self.logger.debug(f"ğŸ“„ ç”Ÿæˆç¬¬ {page} é¡µä¸»é¢˜åˆ—è¡¨é¡µè¯·æ±‚")
            yield Request(
                url=f"https://bbs.nga.cn/thread.php?fid=-7&page={page}",
                callback=self.parse_topic_list,
                meta={'page': page}
            )

    def parse_topic_list(self, response):
        """ä¸¤é˜¶æ®µä¸»é¢˜åˆ—è¡¨è§£æï¼šé˜¶æ®µ1-æ”¶é›†æ‰€æœ‰ä¸»é¢˜ä¿¡æ¯"""
        # è§£æä¸»é¢˜åˆ—è¡¨
        page = response.meta.get('page', 'unknown')
        self.logger.info(f"ğŸ“ å¼€å§‹è§£æç¬¬ {page} é¡µä¸»é¢˜åˆ—è¡¨ (URL: {response.url})")

        # ğŸ” [DEBUG] æ·»åŠ è¯¦ç»†çš„é¡µé¢ä¿¡æ¯
        content_length = len(response.text)
        self.logger.info(f"ğŸ” [DEBUG] é¡µé¢å†…å®¹é•¿åº¦: {content_length} å­—ç¬¦")

        # ä¿å­˜é¡µé¢HTMLç”¨äºè°ƒè¯•
        self._save_response_html(response, page, content_length)

        # æ£€æŸ¥é¡µé¢æ˜¯å¦åŒ…å«NGAå†…å®¹
        if 'nga' not in response.text.lower() and 'bbs.nga.cn' not in response.url:
            self.logger.error(f"âŒ [DEBUG] é¡µé¢å¯èƒ½ä¸æ˜¯NGAå†…å®¹ï¼Œä¿å­˜HTMLæ–‡ä»¶ç”¨äºè°ƒè¯•")
            self._save_response_html(response, page, content_length, reason="not_nga_content")

        # æ£€æŸ¥æ˜¯å¦æœ‰åçˆ¬è™«æç¤º
        anti_bot_keywords = ['è®¿é—®è¿‡äºé¢‘ç¹', 'IPè¢«å°', 'éªŒè¯ç ', 'captcha', 'äººæœºéªŒè¯', 'æ‚¨çš„è®¿é—®å¼‚å¸¸']
        if any(keyword in response.text for keyword in anti_bot_keywords):
            self.logger.error(f"âŒ [DEBUG] æ£€æµ‹åˆ°åçˆ¬è™«æç¤º: {[kw for kw in anti_bot_keywords if kw in response.text]}")
            self._save_response_html(response, page, content_length, reason="anti_bot_detected")

        # æŸ¥æ‰¾ä¸»é¢˜è¡Œ
        rows = response.xpath('//*[contains(@class, "topicrow")]')
        self.logger.info(f"ğŸ“Š ç¬¬ {page} é¡µä¸»é¢˜åˆ—è¡¨å…±æ‰¾åˆ° {len(rows)} ä¸ªä¸»é¢˜")

        # é˜¶æ®µ1: æ”¶é›†æ‰€æœ‰ä¸»é¢˜ä¿¡æ¯
        topics_data = self._collect_topics_from_page(rows, page)

        if not topics_data:
            self.logger.warning(f"âš ï¸ ç¬¬ {page} é¡µæ²¡æœ‰æ”¶é›†åˆ°æœ‰æ•ˆä¸»é¢˜")
            self.logger.warning(f"âš ï¸ [DEBUG] è¯¦ç»†åˆ†æ:")
            self.logger.warning(f"  - åŸå§‹rowsæ•°é‡: {len(rows)}")
            self.logger.warning(f"  - é¡µé¢å†…å®¹é•¿åº¦: {content_length}")
            self.logger.warning(f"  - URL: {response.url}")

            # ğŸ” [DEBUG] å°è¯•åˆ†æé¡µé¢ç»“æ„
            self._analyze_page_structure(response, rows, page)

            # ä¿å­˜HTMLç”¨äºè°ƒè¯•
            self.logger.info(f"ğŸ’¾ [DEBUG] ä¿å­˜HTMLæ–‡ä»¶ç”¨äºè°ƒè¯•...")
            self._save_response_html(response, page, content_length, reason="no_topics_found")

            return

        # é˜¶æ®µ2: æ‰¹é‡æŸ¥è¯¢æ•°æ®åº“ä¿¡æ¯
        all_tids = list(topics_data.keys())
        self.logger.info(f"ğŸ—„ï¸ [DBè°ƒè¯•] ç¬¬{page}é¡µ: å‡†å¤‡æŸ¥è¯¢{len(all_tids)}ä¸ªä¸»é¢˜çš„æ•°æ®åº“è®°å½•")
        db_info = self.batch_query_topics_from_db(all_tids)
        self.logger.info(f"ğŸ—„ï¸ [DBè°ƒè¯•] ç¬¬{page}é¡µ: æ•°æ®åº“è¿”å›{len(db_info)}æ¡è®°å½•, æ–°ä¸»é¢˜æ•°: {len(all_tids) - len(db_info)}")

        # é˜¶æ®µ3: æ™ºèƒ½å†³ç­–å“ªäº›ä¸»é¢˜éœ€è¦çˆ¬å–å›å¤
        topics_to_crawl, topics_to_skip = self._decide_topics_to_crawl(topics_data, db_info)
        self.logger.info(f"ğŸ—„ï¸ [DBè°ƒè¯•] ç¬¬{page}é¡µå†³ç­–ç»“æœ: éœ€çˆ¬å–{len(topics_to_crawl)}ä¸ª, è·³è¿‡{len(topics_to_skip)}ä¸ª")

        # é˜¶æ®µ4: æ‰¹é‡ç”Ÿæˆæ•°æ®é¡¹å’Œè¯·æ±‚
        for item in self._process_topics_batch(topics_to_crawl, topics_to_skip, db_info):
            yield item

        self.logger.debug(f"ğŸ“„ ç¬¬ {page} é¡µå¤„ç†å®Œæˆ: æ€»è®¡{len(topics_data)}ä¸ªä¸»é¢˜, "
                        f"çˆ¬å–{len(topics_to_crawl)}ä¸ª, è·³è¿‡{len(topics_to_skip)}ä¸ª")

    def _save_response_html(self, response, page, content_length, reason=""):
        """ä¿å­˜å“åº”HTMLåˆ°è°ƒè¯•æ–‡ä»¶"""
        import os
        import time
        from urllib.parse import urlparse

        debug_dir = 'debug_html'
        if not os.path.exists(debug_dir):
            os.makedirs(debug_dir)

        timestamp = time.strftime("%Y%m%d_%H%M%S", time.localtime())
        url_hash = hash(response.url) % 10000
        filename = f"{timestamp}_page{page}_{url_hash}_{reason}.html"
        filepath = os.path.join(debug_dir, filename)

        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                debug_header = f"""
<!-- DEBUG INFO (NGA Spider) -->
<!-- Page: {page} -->
<!-- URL: {response.url} -->
<!-- Content Length: {content_length} -->
<!-- Timestamp: {timestamp} -->
<!-- Reason: {reason} -->
<!-- ======================= -->

"""
                f.write(debug_header)
                f.write(response.text)

            self.logger.info(f"ğŸ’¾ [DEBUG] HTMLå·²ä¿å­˜: {filepath}")
            return filepath
        except Exception as e:
            self.logger.error(f"âŒ [DEBUG] ä¿å­˜HTMLå¤±è´¥: {e}")
            return None

    def _analyze_page_structure(self, response, rows, page):
        """åˆ†æé¡µé¢ç»“æ„ï¼Œæ‰¾å‡ºä¸ºä»€ä¹ˆæ²¡æœ‰æ‰¾åˆ°ä¸»é¢˜"""
        try:
            # æ£€æŸ¥é¡µé¢æ ‡é¢˜
            title = response.xpath('//title/text()').get()
            self.logger.warning(f"  - é¡µé¢æ ‡é¢˜: {title}")

            # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯ä¿¡æ¯
            error_selectors = [
                '//*[contains(text(), "404")]',
                '//*[contains(text(), "é¡µé¢ä¸å­˜åœ¨")]',
                '//*[contains(text(), "è®¿é—®è¢«æ‹’ç»")]',
                '//*[contains(text(), "è¯·ç™»å½•")]',
            ]
            for selector in error_selectors:
                error_text = response.xpath(selector).get()
                if error_text:
                    self.logger.warning(f"  - å‘ç°é”™è¯¯ä¿¡æ¯: {error_text[:100]}")

            # æ£€æŸ¥æ‰€æœ‰classåŒ…å«rowçš„å…ƒç´ 
            all_rows = response.xpath('//*[contains(@class, "row")]')
            self.logger.warning(f"  - åŒ…å«'row' classçš„å…ƒç´ æ•°é‡: {len(all_rows)}")

            # æ£€æŸ¥æ‰€æœ‰tableè¡Œ
            table_rows = response.xpath('//tr')
            self.logger.warning(f"  - æ‰€æœ‰trå…ƒç´ æ•°é‡: {len(table_rows)}")

            # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•é“¾æ¥
            links = response.xpath('//a/@href').getall()
            nga_links = [link for link in links if 'nga' in link or 'tid=' in link]
            self.logger.warning(f"  - æ€»é“¾æ¥æ•°: {len(links)}, NGAç›¸å…³é“¾æ¥æ•°: {len(nga_links)}")

            # æ£€æŸ¥é¡µé¢æ˜¯å¦åŒ…å«é¢„æœŸçš„HTMLç»“æ„
            if 'topicrow' in response.text:
                self.logger.warning(f"  - é¡µé¢åŒ…å«'topicrow'å­—ç¬¦ä¸²ï¼Œä½†XPathæœªæ‰¾åˆ°å…ƒç´ ")
            else:
                self.logger.warning(f"  - é¡µé¢ä¸åŒ…å«'topicrow'å­—ç¬¦ä¸²")

            # æ£€æŸ¥æ˜¯å¦ä¸ºç©ºé¡µé¢æˆ–é‡å®šå‘
            if content_length < 500:
                self.logger.warning(f"  - é¡µé¢å†…å®¹è¿‡çŸ­ï¼Œå¯èƒ½æ˜¯ç©ºé¡µé¢æˆ–é‡å®šå‘é¡µé¢")

        except Exception as e:
            self.logger.error(f"  - åˆ†æé¡µé¢ç»“æ„æ—¶å‡ºé”™: {e}")

    def _collect_topics_from_page(self, rows, page):
        """é˜¶æ®µ1: ä»é¡µé¢æ”¶é›†æ‰€æœ‰ä¸»é¢˜çš„åŸºç¡€ä¿¡æ¯"""
        topics_data = {}
        idx = 0

        for idx, row in enumerate(rows, 1):
            self.logger.debug(f"ğŸ” æ”¶é›†ç¬¬ {page} é¡µç¬¬ {idx} ä¸ªä¸»é¢˜ä¿¡æ¯")

            # æå–åŸºç¡€ä¿¡æ¯
            topic_link = row.xpath('.//a[contains(@class, "topic")]/@href').get()
            if not topic_link or 'tid=' not in topic_link:
                continue

            tid = topic_link.split('tid=')[1].split('&')[0]
            title = row.xpath('.//a[contains(@class, "topic")]/text()').get()
            if title == 'å¸–å­å‘å¸ƒæˆ–å›å¤æ—¶é—´è¶…è¿‡é™åˆ¶':
                continue

            poster_id = row.xpath('.//*[@class="author"]/@title').re_first(r'ç”¨æˆ·ID (\d+)')
            poster_name = row.xpath('.//*[@class="author"]/text()').get()
            post_time = row.xpath('.//span[contains(@class, "postdate")]/@title').get()
            re_num = row.xpath('.//*[@class="replies"]/text()').get()

            # å¦‚æœä¸»é¢˜å‘å¸ƒæ—¶é—´ä¸ºNoneï¼Œä½¿ç”¨å½“å‰æ—¶é—´
            if not post_time:
                post_time = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
                self.logger.debug(f"ğŸ•’ ä¸»é¢˜ {tid} æ— æ³•è·å–å‘å¸ƒæ—¶é—´ï¼Œä½¿ç”¨å½“å‰æ—¶é—´: {post_time}")

            # æå–æœ€åå›å¤æ—¶é—´ï¼ˆå¤šç§æ–¹å¼ï¼‰
            last_reply_date = self._extract_last_reply_date(row)

            # è·å–åˆ†åŒºä¿¡æ¯
            partition = 'æ°´åŒº'
            partition_el = row.xpath('.//td[@class="c2"]/span[@class="titleadd2"]/a[@class="silver"]/text()')
            if partition_el:
                partition = partition_el.get()

            # å­˜å‚¨ä¸»é¢˜ä¿¡æ¯
            topics_data[tid] = {
                'title': title,
                'poster_id': poster_id,
                'poster_name': poster_name,
                'post_time': post_time,
                're_num': re_num,
                'last_reply_date': last_reply_date,
                'partition': partition,
                'row_index': idx,
                'page': page
            }

        self.logger.debug(f"ğŸ“‹ ç¬¬ {page} é¡µæ”¶é›†å®Œæˆï¼Œå…±æ”¶é›† {len(topics_data)} ä¸ªæœ‰æ•ˆä¸»é¢˜")
        return topics_data

    def _extract_last_reply_date(self, row):
        """æå–æœ€åå›å¤æ—¶é—´çš„å¤šç§æ–¹å¼"""
        last_reply_date = None

        # æ–¹å¼1: ä» .replydate çš„ title å±æ€§æå–
        last_reply_date = row.xpath('.//a[contains(@class, "replydate")]/@title').get()

        # æ–¹å¼2: ä» .replydate çš„æ–‡æœ¬å†…å®¹æå–ï¼ˆç›¸å¯¹æ—¶é—´ï¼‰
        if not last_reply_date:
            alt_date = row.xpath('.//a[contains(@class, "replydate")]/text()').get()
            if alt_date and alt_date not in ['åˆšæ‰', 'ä»Šå¤©', 'æ˜¨å¤©', 'å‰å¤©']:
                last_reply_date = alt_date

        # æ–¹å¼3: æŸ¥æ‰¾æ‰€æœ‰æœ‰titleå±æ€§çš„å…ƒç´ ï¼Œç­›é€‰å‡ºæ—¶é—´æ ¼å¼çš„
        if not last_reply_date:
            time_candidates = row.xpath('.//*[@title and string-length(@title) > 8]/@title').getall()
            for candidate in time_candidates:
                if self._is_nga_time_format(candidate):
                    last_reply_date = candidate
                    break

        # æ–¹å¼4: ä½¿ç”¨æ­£åˆ™ä»æ•´è¡Œæ–‡æœ¬ä¸­æå–æ—¶é—´
        if not last_reply_date:
            row_text = row.xpath('string(.)').get()
            last_reply_date = self._extract_time_from_text(row_text)

        # å¦‚æœç½‘é¡µæ—¶é—´ä¸ºNoneï¼Œä½¿ç”¨å½“å‰æ—¶é—´ä½œä¸ºfallback
        if not last_reply_date:
            last_reply_date = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())

        return last_reply_date

    def _decide_topics_to_crawl(self, topics_data, db_info):
        """é˜¶æ®µ3: æ™ºèƒ½å†³ç­–å“ªäº›ä¸»é¢˜éœ€è¦çˆ¬å–å›å¤"""
        topics_to_crawl = []
        topics_to_skip = []

        for tid, topic_info in topics_data.items():
            web_last_reply = topic_info['last_reply_date']
            db_topic_info = db_info.get(tid, {})
            db_last_reply = db_topic_info.get('last_reply_date')

            # æ›´æ–°ç¼“å­˜
            self.topic_last_reply_cache[tid] = db_last_reply

            # å†³ç­–é€»è¾‘ï¼šæ˜¯å¦éœ€è¦çˆ¬å–è¯¥ä¸»é¢˜çš„å›å¤
            should_crawl = self._should_crawl_topic_replies(tid, web_last_reply, db_last_reply, topic_info, db_topic_info)

            if should_crawl:
                topics_to_crawl.append((tid, topic_info, db_last_reply))
                self.logger.debug(f"âœ… ä¸»é¢˜ {tid} éœ€è¦çˆ¬å–å›å¤ (ç½‘é¡µ:{web_last_reply}, æ•°æ®åº“:{db_last_reply})")
            else:
                topics_to_skip.append((tid, topic_info, db_last_reply))
                self.logger.debug(f"â­ï¸  ä¸»é¢˜ {tid} è·³è¿‡å›å¤çˆ¬å– (ç½‘é¡µ:{web_last_reply}, æ•°æ®åº“:{db_last_reply})")

        return topics_to_crawl, topics_to_skip

    def _should_crawl_topic_replies(self, tid, web_last_reply, db_last_reply, topic_info, db_topic_info=None):
        """åˆ¤æ–­æ˜¯å¦éœ€è¦çˆ¬å–ä¸»é¢˜çš„å›å¤"""
        # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰è®°å½•ï¼Œéœ€è¦çˆ¬å–
        if not db_last_reply:
            return True

        # å¦‚æœç½‘é¡µæ—¶é—´æ¯”æ•°æ®åº“æ—¶é—´æ–°ï¼Œéœ€è¦çˆ¬å–
        if web_last_reply and self.is_newer(web_last_reply, db_last_reply):
            return True

        # å¦‚æœå›å¤æ•°é‡æœ‰å˜åŒ–ï¼Œå¯èƒ½éœ€è¦çˆ¬å–ï¼ˆå¯é€‰çš„å¯å‘å¼åˆ¤æ–­ï¼‰
        web_re_num = topic_info.get('re_num', '0') or '0'
        db_re_num = str(db_topic_info.get('re_num', 0)) if db_topic_info and db_topic_info.get('re_num') else '0'
        if web_re_num and db_re_num and web_re_num != db_re_num:
            self.logger.debug(f"ğŸ”¢ ä¸»é¢˜ {tid} å›å¤æ•°å˜åŒ–: ç½‘é¡µ{web_re_num} vs æ•°æ®åº“{db_re_num}")
            return True

        return False

    def _process_topics_batch(self, topics_to_crawl, topics_to_skip, db_info):
        """é˜¶æ®µ4: æ‰¹é‡å¤„ç†æ‰€æœ‰ä¸»é¢˜ï¼Œç”Ÿæˆæ•°æ®é¡¹å’Œè¯·æ±‚"""
        reply_requests_count = 0
        total_count = len(topics_to_crawl)

        # å¤„ç†éœ€è¦çˆ¬å–çš„ä¸»é¢˜
        # å…³é”®ä¿®å¤ï¼šæ·»åŠ å°å»¶è¿Ÿæ§åˆ¶ç”Ÿæˆé€Ÿåº¦ï¼Œé¿å…é˜Ÿåˆ—æ‹¥å¡
        # ç”Ÿæˆé€Ÿåº¦å¿…é¡» <= å¤„ç†é€Ÿåº¦ï¼Œå¦åˆ™ä¼šå †ç§¯
        for i, (tid, topic_info, db_last_reply) in enumerate(topics_to_crawl):
            # æ¯ç”Ÿæˆ8ä¸ªè¯·æ±‚æš‚åœ0.5ç§’ï¼ˆä¸å¹¶å‘æ•°åŒ¹é…ï¼‰ï¼Œè®©Scrapyæœ‰æ—¶é—´å¤„ç†
            if i > 0 and i % 8 == 0:
                self.logger.info(f"â±ï¸ [èŠ‚æµ] å·²ç”Ÿæˆ{i}/{total_count}ä¸ªè¯·æ±‚ï¼Œæš‚åœ0.5ç§’è®©è°ƒåº¦å™¨å¤„ç†...")
                time.sleep(0.5)
            # ç”ŸæˆTopicItem
            topic_item = TopicItem(
                tid=tid,
                title=topic_info['title'],
                poster_id=topic_info['poster_id'],
                post_time=topic_info['post_time'],
                re_num=topic_info['re_num'],
                sampling_time=self._now_time(),
                last_reply_date=topic_info['last_reply_date'],
                partition=topic_info['partition']
            )
            yield topic_item

            # ç”ŸæˆUserItem
            if topic_info['poster_id']:
                user_item = UserItem(
                    uid=topic_info['poster_id'],
                    name=topic_info['poster_name'] or '',
                    user_group='',
                    reg_date='',
                    prestige='',
                    history_re_num=''
                )
                yield user_item

            # ç”Ÿæˆå›å¤é¡µè¯·æ±‚ï¼ˆå¹¶å‘ç”± Scrapy çš„ CONCURRENT_REQUESTS æ§åˆ¶ï¼‰
            reply_request = Request(
                url=f"https://bbs.nga.cn/read.php?tid={tid}&page=999",
                callback=self.parse_replies,
                meta={'tid': tid, 'db_last_reply': db_last_reply},
                priority=100,
                dont_filter=False
            )
            self.logger.debug(f"ğŸ”„ æ­£åœ¨yieldè¯·æ±‚ {tid}...")
            yield reply_request
            reply_requests_count += 1
            self.logger.debug(f"âœ… æˆåŠŸyieldè¯·æ±‚ {tid}ï¼Œè®¡æ•°: {reply_requests_count}/{total_count}")
            self.logger.debug(f"ğŸš€ ä¸»é¢˜ {tid}: å·²ç”Ÿæˆå›å¤é¡µè¯·æ±‚ (ç¬¬{i+1}/{total_count}ä¸ª)")
            
            # ã€è¯Šæ–­æ—¥å¿—ã€‘æ¯ç”Ÿæˆ10ä¸ªè¯·æ±‚æ£€æŸ¥ä¸€æ¬¡é˜Ÿåˆ—çŠ¶æ€
            if (i + 1) % 10 == 0:
                if hasattr(self.crawler.engine, 'scheduler') and hasattr(self.crawler.engine.scheduler, 'queue'):
                    queue_size = len(self.crawler.engine.scheduler.queue)
                    self.logger.info(f"ğŸ“Š [ç”Ÿæˆè¯·æ±‚é˜Ÿåˆ—è¯Šæ–­] å·²ç”Ÿæˆ{i+1}ä¸ªè¯·æ±‚ï¼Œå½“å‰è°ƒåº¦é˜Ÿåˆ—é•¿åº¦: {queue_size}")

        self.logger.info(f"ğŸ—„ï¸ [DBè°ƒè¯•] æ‰¹å¤„ç†å®Œæˆ: ç”Ÿæˆ{reply_requests_count}ä¸ªå›å¤é¡µè¯·æ±‚, è·³è¿‡{len(topics_to_skip)}ä¸ªä¸»é¢˜")

        # é˜Ÿåˆ—çŠ¶æ€ç›‘æ§ - å…³é”®è°ƒè¯•ä¿¡æ¯
        if hasattr(self.crawler.engine, 'scheduler') and hasattr(self.crawler.engine.scheduler, 'queue'):
            queue_size = len(self.crawler.engine.scheduler.queue)
            self.logger.info(f"ğŸ“Š [é˜Ÿåˆ—ç›‘æ§] å½“å‰è°ƒåº¦é˜Ÿåˆ—é•¿åº¦: {queue_size}, ç”Ÿæˆè¯·æ±‚æ€»æ•°: {reply_requests_count}")
            if queue_size > 100:
                self.logger.warning(f"âš ï¸ [é˜Ÿåˆ—æ‹¥å¡] é˜Ÿåˆ—é•¿åº¦({queue_size})è¶…è¿‡100ï¼Œå¯èƒ½å¯¼è‡´å¤„ç†å»¶è¿Ÿï¼")
        else:
            self.logger.warning("âš ï¸ æ— æ³•è·å–è°ƒåº¦å™¨é˜Ÿåˆ—çŠ¶æ€")

        # å¤„ç†è·³è¿‡çš„ä¸»é¢˜ï¼ˆåªç”ŸæˆTopicItemï¼Œä¸ç”Ÿæˆè¯·æ±‚ï¼‰
        for tid, topic_info, db_last_reply in topics_to_skip:
            # å³ä½¿è·³è¿‡å›å¤çˆ¬å–ï¼Œä¹Ÿè¦æ›´æ–°ä¸»é¢˜ä¿¡æ¯ï¼ˆä¿æŒæ•°æ®æ–°é²œåº¦ï¼‰
            topic_item = TopicItem(
                tid=tid,
                title=topic_info['title'],
                poster_id=topic_info['poster_id'],
                post_time=topic_info['post_time'],
                re_num=topic_info['re_num'],
                sampling_time=self._now_time(),
                last_reply_date=topic_info['last_reply_date'],
                partition=topic_info['partition']
            )
            yield topic_item

            self.logger.debug(f"ğŸ“ ä¸»é¢˜ {tid}: å·²æ›´æ–°ä¸»é¢˜ä¿¡æ¯ï¼ˆè·³è¿‡å›å¤çˆ¬å–ï¼‰")

    def get_last_reply_from_db(self, tid):
        """ä»æ•°æ®åº“è·å–ä¸»é¢˜çš„æœ€åå›å¤æ—¶é—´"""
        if not hasattr(self, 'db_session') or not self.db_session:
            self.logger.error("æ•°æ®åº“ä¼šè¯æœªåˆå§‹åŒ–")
            return None

        try:
            from ..models import Topic  # å±€éƒ¨å¯¼å…¥é¿å…å¾ªç¯å¼•ç”¨
            topic = self.db_session.query(Topic).filter_by(tid=tid).first()
            return topic.last_reply_date if topic else None
        except SQLAlchemyError as e:
            self.logger.error(f"æŸ¥è¯¢æ•°æ®åº“å‡ºé”™: {e}")
            return None
        except Exception as e:
            self.logger.error(f"è·å–æœ€åå›å¤æ—¶é—´æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
            return None

    def batch_query_topics_from_db(self, tids, batch_size=100, use_cache=True, use_exists_optimization=True):
        """æ‰¹é‡æŸ¥è¯¢æ•°æ®åº“ä¸­å¤šä¸ªä¸»é¢˜çš„ä¿¡æ¯ï¼ˆç»ˆæä¼˜åŒ–ç‰ˆæœ¬ï¼‰

        Args:
            tids: ä¸»é¢˜IDåˆ—è¡¨
            batch_size: æ¯æ‰¹æŸ¥è¯¢çš„ä¸»é¢˜æ•°é‡ï¼Œé»˜è®¤100
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼Œé»˜è®¤True
            use_exists_optimization: æ˜¯å¦ä½¿ç”¨EXISTSä¼˜åŒ–ï¼Œé»˜è®¤True

        Returns:
            dict: {tid: {'last_reply_date': str, 'post_time': str, 're_num': int}}
        """
        if not hasattr(self, 'db_session') or not self.db_session:
            self.logger.error("æ•°æ®åº“ä¼šè¯æœªåˆå§‹åŒ–")
            return {}

        if not tids:
            return {}

        # å¯¼å…¥æ¨¡å—
        from ..models import Topic
        import time

        total_tids = len(tids)
        result = {}
        cached_count = 0
        db_query_count = 0
        query_strategy = 'in_query'  # é»˜è®¤æŸ¥è¯¢ç­–ç•¥

        # ç¼“å­˜é”®å‰ç¼€
        cache_prefix = 'topic_info:'

        # ç¬¬ä¸€æ­¥ï¼šä¼˜å…ˆä»ç¼“å­˜è·å–æ•°æ®
        if use_cache:
            for tid in tids:
                cache_key = f"{cache_prefix}{tid}"
                cached_data = self.cache_manager.get(cache_key)
                if cached_data is not None:
                    result[tid] = cached_data
                    cached_count += 1

            self.logger.debug(f"ğŸ’¾ [ç¼“å­˜] ä»ç¼“å­˜è·å– {cached_count}/{total_tids} ä¸ªä¸»é¢˜æ•°æ®")

        # ç¬¬äºŒæ­¥ï¼šæŸ¥è¯¢æ•°æ®åº“è·å–æœªç¼“å­˜çš„æ•°æ®
        if use_cache:
            # æ‰¾å‡ºæœªç¼“å­˜çš„TID
            uncached_tids = [tid for tid in tids if tid not in result]
        else:
            uncached_tids = tids

        if uncached_tids:
            # æ ¹æ®æ•°æ®é‡é€‰æ‹©æœ€ä¼˜æŸ¥è¯¢ç­–ç•¥
            if use_exists_optimization and len(uncached_tids) > 1000 and self.query_optimizer:
                # å¤§é‡æ•°æ®ï¼šä½¿ç”¨EXISTSæŸ¥è¯¢ä¼˜åŒ–
                query_strategy = 'exists_query'
                existing_tids = self.query_optimizer.check_topics_exist_exists(uncached_tids)

                # åªæŸ¥è¯¢å­˜åœ¨çš„ä¸»é¢˜
                if existing_tids:
                    existing_list = list(existing_tids)
                    for i in range(0, len(existing_list), batch_size):
                        batch_tids = existing_list[i:i + batch_size]
                        batch_num = i // batch_size + 1

                        query_start_time = time.time()
                        try:
                            topics = self.db_session.query(Topic).filter(Topic.tid.in_(batch_tids)).all()

                            batch_result_count = 0
                            for topic in topics:
                                topic_data = {
                                    'last_reply_date': topic.last_reply_date,
                                    'post_time': topic.post_time,
                                    're_num': topic.re_num
                                }
                                result[topic.tid] = topic_data
                                batch_result_count += 1

                                # å†™å…¥ç¼“å­˜
                                if use_cache:
                                    cache_key = f"{cache_prefix}{topic.tid}"
                                    self.cache_manager.set(cache_key, topic_data)

                            db_query_count += 1

                            batch_elapsed = time.time() - query_start_time
                            self.logger.debug(
                                f"âœ… [EXISTSä¼˜åŒ–] æ‰¹æ¬¡ {batch_num}: è€—æ—¶{batch_elapsed:.3f}s, "
                                f"è¿”å›{batch_result_count}æ¡è®°å½•"
                            )

                        except Exception as e:
                            self.logger.error(f"EXISTSä¼˜åŒ–æ‰¹æ¬¡ {batch_num} æŸ¥è¯¢å‡ºé”™: {e}")
                            continue

            else:
                # ä¸­å°æ•°æ®ï¼šä½¿ç”¨æ ‡å‡†åˆ†æ‰¹INæŸ¥è¯¢
                query_strategy = 'batch_in_query'
                total_batches = (len(uncached_tids) + batch_size - 1) // batch_size
                query_count = 0
                start_time = time.time()

                for i in range(0, len(uncached_tids), batch_size):
                    batch_tids = uncached_tids[i:i + batch_size]
                    batch_num = i // batch_size + 1

                    query_count += 1
                    query_start_time = time.time()

                    try:
                        # è®°å½•æŸ¥è¯¢æ—¥å¿—
                        if total_batches > 1:
                            self.logger.debug(f"ğŸ—„ï¸ [DBè°ƒè¯•] æ‰¹æ¬¡ {batch_num}/{total_batches}: æŸ¥è¯¢{len(batch_tids)}ä¸ªä¸»é¢˜")
                        else:
                            self.logger.debug(f"ğŸ—„ï¸ [DBè°ƒè¯•] æŸ¥è¯¢{len(batch_tids)}ä¸ªä¸»é¢˜")

                        # æ‰§è¡Œæ‰¹æ¬¡æŸ¥è¯¢
                        topics = self.db_session.query(Topic).filter(Topic.tid.in_(batch_tids)).all()

                        # å¤„ç†æŸ¥è¯¢ç»“æœ
                        batch_result_count = 0
                        for topic in topics:
                            topic_data = {
                                'last_reply_date': topic.last_reply_date,
                                'post_time': topic.post_time,
                                're_num': topic.re_num
                            }
                            result[topic.tid] = topic_data
                            batch_result_count += 1

                            # å†™å…¥ç¼“å­˜
                            if use_cache:
                                cache_key = f"{cache_prefix}{topic.tid}"
                                self.cache_manager.set(cache_key, topic_data)

                        db_query_count += 1

                        # è®°å½•å•æ‰¹æŸ¥è¯¢è€—æ—¶
                        batch_elapsed = time.time() - query_start_time

                        # æ…¢æŸ¥è¯¢å‘Šè­¦ï¼ˆ>500msï¼‰
                        if batch_elapsed > 0.5:
                            self.logger.warning(
                                f"âš ï¸ [æ…¢æŸ¥è¯¢å‘Šè­¦] æ‰¹æ¬¡ {batch_num} æŸ¥è¯¢è€—æ—¶ {batch_elapsed:.3f}s "
                                f"(æŸ¥è¯¢{len(batch_tids)}ä¸ªä¸»é¢˜ï¼Œè¿”å›{batch_result_count}æ¡è®°å½•)"
                            )

                        if total_batches > 1:
                            self.logger.debug(
                                f"âœ… æ‰¹æ¬¡ {batch_num} å®Œæˆ: è€—æ—¶{batch_elapsed:.3f}s, "
                                f"è¿”å›{batch_result_count}æ¡è®°å½•"
                            )

                    except SQLAlchemyError as e:
                        self.logger.error(f"æ‰¹æ¬¡ {batch_num} æ‰¹é‡æŸ¥è¯¢æ•°æ®åº“å‡ºé”™: {e}")
                        continue
                    except Exception as e:
                        self.logger.error(f"æ‰¹æ¬¡ {batch_num} æ‰¹é‡æŸ¥è¯¢æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
                        continue

                # è®°å½•æ€»æŸ¥è¯¢è€—æ—¶
                total_elapsed = time.time() - start_time

                # è®°å½•åˆ°ç›‘æ§ç³»ç»Ÿ
                monitor = get_monitor()
                monitor.record_query(total_elapsed, 'batch', batch_size, len(uncached_tids))

                # æŸ¥è¯¢æ€§èƒ½ç»Ÿè®¡
                avg_query_time = total_elapsed / query_count if query_count > 0 else 0

                self.logger.info(
                    f"ğŸ—„ï¸ [DBæ€§èƒ½ç»Ÿè®¡] æœªç¼“å­˜æŸ¥è¯¢: {len(uncached_tids)}ä¸ªä¸»é¢˜, "
                    f"åˆ†{query_count}æ‰¹, æ€»è€—æ—¶{total_elapsed:.3f}s, "
                    f"å¹³å‡æ¯æ‰¹{avg_query_time:.3f}s"
                )

                # æ•´ä½“æ…¢æŸ¥è¯¢å‘Šè­¦ï¼ˆ>2000msï¼‰
                if total_elapsed > 2.0:
                    self.logger.warning(
                        f"âš ï¸ [æ•´ä½“æ…¢æŸ¥è¯¢å‘Šè­¦] æ‰¹é‡æŸ¥è¯¢æ€»è€—æ—¶ {total_elapsed:.3f}s "
                        f"(æŸ¥è¯¢{len(uncached_tids)}ä¸ªä¸»é¢˜ï¼Œå»ºè®®ä¼˜åŒ–)"
                    )

                # æ¯1000ä¸ªä¸»é¢˜çš„æ€§èƒ½æŠ¥å‘Š
                if len(uncached_tids) >= 1000:
                    throughput = len(uncached_tids) / total_elapsed if total_elapsed > 0 else 0
                    self.logger.info(
                        f"ğŸ“Š [æ€§èƒ½æŠ¥å‘Š] æŸ¥è¯¢ååé‡: {throughput:.1f} ä¸»é¢˜/ç§’ "
                        f"({len(uncached_tids)}ä¸ªä¸»é¢˜ / {total_elapsed:.3f}s)"
                    )

        # ç¬¬ä¸‰æ­¥ï¼šæ±‡æ€»ç»Ÿè®¡ä¿¡æ¯
        cache_hit_rate = (cached_count / total_tids * 100) if total_tids > 0 else 0

        self.logger.info(
            f"ğŸ¯ [æŸ¥è¯¢ç­–ç•¥] ä½¿ç”¨ç­–ç•¥: {query_strategy}, "
            f"ç¼“å­˜å‘½ä¸­: {cached_count}/{total_tids} ({cache_hit_rate:.1f}%), "
            f"æ•°æ®åº“æŸ¥è¯¢: {db_query_count}æ‰¹æ¬¡, "
            f"æ‰¾åˆ°{len(result)}æ¡è®°å½•"
        )

        # ç¼“å­˜å‘½ä¸­ç‡ä½å‘Šè­¦ï¼ˆ<50%ï¼‰
        if use_cache and cache_hit_rate < 50:
            self.logger.warning(
                f"âš ï¸ [ç¼“å­˜å‘½ä¸­ç‡å‘Šè­¦] ç¼“å­˜å‘½ä¸­ç‡ {cache_hit_rate:.1f}% è¾ƒä½ "
                f"(å»ºè®®æ£€æŸ¥ç¼“å­˜é…ç½®æˆ–å¢åŠ ç¼“å­˜æ—¶é—´)"
            )

        return result

    # å…¶ä»–æ–¹æ³•ä¿æŒä¸å˜...
    def parse_replies(self, response):
        # ç«‹å³è®°å½•æ–¹æ³•è¢«è°ƒç”¨ï¼Œç”¨äºè°ƒè¯•
        self.logger.info(f"ğŸ¯ parse_repliesæ–¹æ³•è¢«è°ƒç”¨! URL: {response.url}, Status: {response.status}")
        
        # ã€è¯Šæ–­æ—¥å¿—ã€‘è®°å½•è°ƒåº¦é˜Ÿåˆ—çŠ¶æ€
        if hasattr(self.crawler.engine, 'scheduler') and hasattr(self.crawler.engine.scheduler, 'queue'):
            queue_size = len(self.crawler.engine.scheduler.queue)
            self.logger.info(f"ğŸ“Š [parse_repliesé˜Ÿåˆ—è¯Šæ–­] å½“å‰è°ƒåº¦é˜Ÿåˆ—é•¿åº¦: {queue_size}")

        tid = response.meta['tid']
        db_last_reply = response.meta.get('db_last_reply')
        current_page = response.meta.get('current_page', 'unknown')
        last_page = response.meta.get('last_page', 'unknown')

        self.logger.debug(f"ğŸ’¬ å¼€å§‹è§£æä¸»é¢˜ {tid} çš„å›å¤ (å½“å‰é¡µ: {current_page}/{last_page}, URL: {response.url})")

        meta={'tid': tid}

        if 'last_page' not in response.meta:
            last_page_link = response.xpath('//a[contains(@class, "invert") and @title="æœ€åé¡µ"]/@href').get()
            if last_page_link:
                last_page = int(parse_qs(last_page_link.split('?')[1]).get('page', [1])[0])
                #self.logger.info(f"æœ€åä¸€é¡µ{last_page}è·å–")
            else:
                page_links = [int(p.split('=')[-1]) for p in response.xpath('//a[contains(@href, "page=")]/@href').re(r'page=\d+')]
                last_page = max(page_links) if page_links else 1

            meta['last_page'] = last_page
            meta['current_page'] = last_page

        if 'current_page' in response.meta:
            meta['current_page'] = response.meta['current_page']

        new_page_flag=True

        replies = response.xpath('//*[@class="forumbox postbox"]')
        self.logger.debug(f"ğŸ“œ ä¸»é¢˜ {tid}: å½“å‰é¡µ {current_page}/{last_page} å…±æœ‰ {len(replies)} æ¡å›å¤")

        for idx, reply in enumerate(replies, 1):
            self.logger.debug(f"ğŸ“ ä¸»é¢˜ {tid}: å¼€å§‹å¤„ç†ç¬¬ {idx} æ¡å›å¤ (å½“å‰é¡µ {current_page}/{last_page})")
            post_id = reply.xpath('.//*[starts-with(@id, "postcontainer")]/a[1]/@id').get()

            # å®‰å…¨æ£€æŸ¥ï¼šå¦‚æœæ— æ³•è·å– post_idï¼Œè·³è¿‡è¯¥å›å¤
            if not post_id:
                self.logger.warning(f"æ— æ³•è·å– post_idï¼Œè·³è¿‡è¯¥å›å¤ (tid={tid})")
                continue

            # ç»Ÿä¸€ä½¿ç”¨çº¯æ•°å­—æ ¼å¼
            if post_id == 'pid0Anchor':
                # ä¸»æ¥¼ä½¿ç”¨ tid ä½œä¸º ridï¼Œçº¯æ•°å­—æ ¼å¼
                post_id = tid
            elif 'pid' in post_id and 'Anchor' in post_id:
                # æ™®é€šå›å¤ï¼šä» Anchor æ ¼å¼æå–çº¯æ•°å­—
                # ä¾‹å¦‚ï¼špid849526462Anchor â†’ 849526462
                post_id = post_id.replace('pid', '').replace('Anchor', '')
            elif post_id.isdigit():
                # å·²ç»æ˜¯çº¯æ•°å­—æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
                pass
            else:
                # å…¶ä»–æœªçŸ¥æ ¼å¼ï¼Œè®°å½•è­¦å‘Šå¹¶è·³è¿‡
                self.logger.warning(f"æœªçŸ¥çš„ post_id æ ¼å¼: {post_id} (tid={tid})")
                continue
                
            poster_href = reply.xpath('.//*[starts-with(@id, "postauthor")]/@href').get()
            poster_id = poster_href.split('uid=')[1].split('&')[0] if poster_href and 'uid=' in poster_href else ''
            poster_name = reply.xpath('.//*[starts-with(@id, "postauthor")]/text()').get()
            
            content = reply.xpath('.//*[starts-with(@id, "postcontent") '
                                 'and string-length(translate(substring(@id, 12), "0123456789", "")) = 0]/text()').get()
            
            recommendvalue = reply.xpath('.//span[contains(@class,"recommendvalue")]/text()').get('0')
            post_time = reply.xpath('.//*[starts-with(@id, "postdate")]/text()').get()

            # å¦‚æœå›å¤æ—¶é—´ä¸ºNoneï¼Œä½¿ç”¨å½“å‰æ—¶é—´
            if not post_time:
                post_time = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
                self.logger.debug(f"ğŸ•’ å›å¤ {post_id} æ— æ³•è·å–æ—¶é—´ï¼Œä½¿ç”¨å½“å‰æ—¶é—´: {post_time}")

            # å¦‚æœè®¾ç½®äº†æ•°æ®åº“æœ€åå›å¤æ—¶é—´ï¼Œä¸”å½“å‰å›å¤æ—¶é—´ä¸æ–°äºæ•°æ®åº“è®°å½•ï¼Œåˆ™è·³è¿‡
            if db_last_reply and not self.is_newer(post_time, db_last_reply):
                self.logger.debug(f"è·³è¿‡å›å¤ {post_id}ï¼Œå›å¤æ—¶é—´ {post_time} ä¸æ–°äºæ•°æ®åº“è®°å½• {db_last_reply}")
                new_page_flag=False
                continue
            
            parent_rid = None
            if reply.xpath('.//div[contains(@class, "quote")]'):
                quote_link = reply.xpath('.//a[contains(@title, "æ‰“å¼€é“¾æ¥")]/@href').get()
                if quote_link and 'pid=' in quote_link:
                    parent_rid = quote_link.split('pid=')[1].split('&')[0]
            
            # æ–°å¢å›¾ç‰‡URLæå–é€»è¾‘
            image_urls = []
            # æå–æ‰€æœ‰imgæ ‡ç­¾çš„srcå±æ€§ï¼ˆåŒ…å«data-srcorgå¤‡ç”¨ï¼‰
            for img in reply.xpath('.//img'):
                src = img.xpath('@src').get()
                if src and ('attachments' in src or 'smile' in src):
                    image_urls.append(src)

            reply_item = ReplyItem(
                rid=post_id,
                tid=tid,
                parent_rid=parent_rid,
                content=content,
                recommendvalue=recommendvalue,
                post_time=post_time,
                poster_id=poster_id,
                sampling_time=self._now_time(),
                image_urls=image_urls  # æ·»åŠ å›¾ç‰‡URLåˆ—è¡¨
            )
            self.logger.debug(f"âœ… ä¸»é¢˜ {tid}: æˆåŠŸæå–å›å¤ {post_id} (æ—¶é—´: {post_time}, ç”¨æˆ·: {poster_id}, æ¨èå€¼: {recommendvalue})")
            yield reply_item

            # åˆ›å»ºç”¨æˆ·ä¿¡æ¯ï¼ˆåªåŒ…å«åŸºæœ¬ä¿¡æ¯ï¼Œä¸å‘èµ·é¢å¤–è¯·æ±‚ï¼‰
            if poster_id:
                self.logger.debug(f"ğŸ‘¤ ä¸»é¢˜ {tid}: ä¸ºç”¨æˆ· {poster_id} ç”ŸæˆUserItem")
                user_item = UserItem(
                    uid=poster_id,
                    name=poster_name or '',
                    user_group='',
                    reg_date='',
                    prestige='',
                    history_re_num=''
                )
                yield user_item

        self.logger.debug(f"ğŸ“„ ä¸»é¢˜ {tid}: é¡µé¢ {current_page}/{last_page} è§£æå®Œæˆï¼Œå‡†å¤‡å¤„ç†ä¸Šä¸€é¡µ")
        # å¤„ç†ä¸Šä¸€é¡µ
        if new_page_flag and meta['current_page'] > 1:
            meta['current_page'] = meta['current_page'] - 1
            self.logger.debug(f"â¬…ï¸ ä¸»é¢˜ {tid}: ç¿»åˆ°ä¸Šä¸€é¡µ {meta['current_page']} é¡µ")
            yield Request(
                url=f"https://bbs.nga.cn/read.php?tid={tid}&page={meta['current_page']}",
                callback=self.parse_replies,
                meta=meta
            )
        else:
            self.logger.debug(f"âœ… ä¸»é¢˜ {tid}: æ‰€æœ‰å›å¤é¡µå¤„ç†å®Œæˆ")

    # å…¶ä»–æ–¹æ³•ä¿æŒä¸å˜...
    def parse_user(self, response):
        uid = response.meta['uid']
        user_group = response.xpath('//label[contains(text(), "ç”¨â€‚æˆ·â€‚ç»„")]/../span/span/text()').get('åŒ¿åç”¨æˆ·')
        reg_date = response.xpath('//label[contains(text(), "æ³¨å†Œæ—¥æœŸ")]/../span/text()').get()
        
        user_item=UserItem(
            uid=uid,
            user_group=user_group,
            reg_date=reg_date,
            prestige='',
            history_re_num='')
        yield user_item

    def is_newer(self, time1, time2):
        """æ¯”è¾ƒä¸¤ä¸ªæ—¶é—´å­—ç¬¦ä¸²ï¼Œåˆ¤æ–­time1æ˜¯å¦æ¯”time2æ–°"""
        try:
            # å¤„ç†NGAçš„æ—¶é—´æ ¼å¼å¯èƒ½ä¸ä¸€è‡´çš„æƒ…å†µ
            dt1 = self._parse_nga_time(time1)
            dt2 = self._parse_nga_time(time2)
            self.logger.debug(f"æ—¶é—´æ¯”è¾ƒ:  time1: {dt1}, time2: {dt2}ï¼Œç»“æœï¼š{dt1 >= dt2}")
            return dt1 >= dt2
        except Exception as e:
            self.logger.error(f"æ—¶é—´æ¯”è¾ƒé”™è¯¯: {e}, web_time: {time1}, db_time: {time2}")
            return True  # å¦‚æœè§£æå¤±è´¥ï¼Œé»˜è®¤å¤„ç†ä¸ºæ–°å›å¤

    def _parse_nga_time(self, time_str):
        """è§£æNGAçš„å„ç§æ—¶é—´æ ¼å¼"""
        if not time_str:
            return datetime.min
        
        # å°è¯•å¸¸è§æ ¼å¼ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
        formats = [
            '%Y-%m-%d %H:%M:%S',  # æ ‡å‡†æ ¼å¼ 2025-04-19 17:00:00
            '%y-%m-%d %H:%M:%S',  # ç®€å†™å¹´ä»½ 25-04-19 17:00:00
            '%d-%m-%y %H:%M:%S',  # æ—¥-æœˆ-å¹´ 19-04-25 17:00:00
            '%Y-%m-%d %H:%M',     # ç¼ºå°‘ç§’
            '%y-%m-%d %H:%M',     # ç®€å†™å¹´ä»½ç¼ºå°‘ç§’
            '%d-%m-%y %H:%M',     # æ—¥-æœˆ-å¹´ç¼ºå°‘ç§’
            '%m-%d %H:%M',        # ç¼ºå°‘å¹´å’Œç§’
            '%H:%M:%S',           # åªæœ‰æ—¶é—´
            '%H:%M'               # åªæœ‰å°æ—¶å’Œåˆ†é’Ÿ
        ]
        
        for fmt in formats:
            try:
                return datetime.strptime(time_str, fmt)
            except ValueError:
                continue
        
        # å¦‚æœéƒ½ä¸åŒ¹é…ï¼Œè¿”å›æœ€å°æ—¶é—´
        self.logger.warning(f"æ— æ³•è§£æçš„æ—¶é—´æ ¼å¼: {time_str}")
        return datetime.min


    def _is_nga_time_format(self, time_str):
        """æ£€æŸ¥å­—ç¬¦ä¸²æ˜¯å¦ä¸ºNGAæ—¶é—´æ ¼å¼"""
        if not time_str:
            return False
        import re
        # åŒ¹é… NGA æ—¶é—´æ ¼å¼: 25-11-30 15:59, 2025-11-30 15:59:30 ç­‰
        patterns = [
            r'\d{2,4}-\d{2}-\d{2}\s+\d{2}:\d{2}(?::\d{2})?',  # æ ‡å‡†æ ¼å¼
            r'\d{2,4}-\d{2}-\d{2}\s+\d{2}:\d{2}',             # ç®€åŒ–æ ¼å¼
        ]
        for pattern in patterns:
            if re.match(pattern, time_str.strip()):
                return True
        return False

    def _extract_time_from_text(self, text):
        """ä»æ–‡æœ¬ä¸­ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ—¶é—´"""
        if not text:
            return None
        import re
        # å¤šç§æ—¶é—´æ ¼å¼çš„æ­£åˆ™è¡¨è¾¾å¼
        patterns = [
            r'(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2})',  # 2025-11-30 15:59:30
            r'(\d{2}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2})',  # 25-11-30 15:59:30
            r'(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2})',        # 2025-11-30 15:59
            r'(\d{2}-\d{2}-\d{2}\s+\d{2}:\d{2})',        # 25-11-30 15:59
        ]
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(1)
        return None

    def _now_time(self):
        return time.strftime("%Y-%m-%d-%H-%M-%S", time.localtime())
