# # nga_spider.py
# ä»¥ä¸‹æ˜¯NGAçˆ¬è™«ä»£ç çš„åŠŸèƒ½å’Œç‰¹æ€§æ€»ç»“ï¼ŒåŸºäºè¿™äº›ä¿¡æ¯å¯ä»¥å®Œæ•´è¿˜åŸä»£ç ï¼š

# ### 1. æ ¸å¿ƒåŠŸèƒ½
# - **ä¸»é¢˜çˆ¬å–**ï¼šçˆ¬å–NGAè®ºå›æ°´åŒº(fid=-7)å‰10é¡µçš„ä¸»é¢˜åˆ—è¡¨
# - **å›å¤çˆ¬å–**ï¼šå¯¹æ¯ä¸ªä¸»é¢˜çˆ¬å–å…¶æ‰€æœ‰å›å¤å†…å®¹
# - **ç”¨æˆ·ä¿¡æ¯çˆ¬å–**ï¼šè·å–å‘å¸–ç”¨æˆ·çš„åŸºæœ¬ä¿¡æ¯
# - **å¢é‡çˆ¬å–**ï¼šé€šè¿‡æ¯”è¾ƒæ•°æ®åº“ä¸­çš„æœ€åå›å¤æ—¶é—´ï¼Œåªçˆ¬å–æ–°å†…å®¹

# ### 2. ä¸»è¦ç‰¹æ€§

# #### æ•°æ®åº“é›†æˆ
# - ä½¿ç”¨SQLAlchemy ORMè¿›è¡Œæ•°æ®åº“æ“ä½œ
# - æ”¯æŒä»å‘½ä»¤è¡Œä¼ å…¥æ•°æ®åº“URL(db_urlå‚æ•°)
# - ä½¿ç”¨scoped_sessionç¡®ä¿çº¿ç¨‹å®‰å…¨
# - è‡ªåŠ¨åˆå§‹åŒ–/å…³é—­æ•°æ®åº“è¿æ¥
# - ç¼“å­˜ä¸»é¢˜æœ€åå›å¤æ—¶é—´å‡å°‘æ•°æ®åº“æŸ¥è¯¢

# #### å¢é‡çˆ¬å–æœºåˆ¶
# - æ¯”è¾ƒç½‘é¡µæ—¶é—´ä¸æ•°æ®åº“æ—¶é—´(`is_newer`æ–¹æ³•)
# - è·³è¿‡å·²å­˜åœ¨çš„æ—§å›å¤
# - æ”¯æŒå¤šç§NGAæ—¶é—´æ ¼å¼è§£æ(`_parse_nga_time`æ–¹æ³•)

# #### æ•°æ®æå–
# - **ä¸»é¢˜ä¿¡æ¯**ï¼šæ ‡é¢˜ã€IDã€å‘å¸–äººã€å‘å¸–æ—¶é—´ã€å›å¤æ•°ã€æœ€åå›å¤æ—¶é—´ã€åˆ†åŒº
# - **å›å¤ä¿¡æ¯**ï¼šå†…å®¹ã€æ¨èå€¼ã€å›å¤æ—¶é—´ã€çˆ¶å›å¤ID
# - **ç”¨æˆ·ä¿¡æ¯**ï¼šç”¨æˆ·ç»„ã€æ³¨å†Œæ—¥æœŸç­‰

# #### æ€§èƒ½ä¼˜åŒ–
# - ä¸»é¢˜åˆ—è¡¨é¡µå¹¶è¡Œçˆ¬å–(10é¡µå¹¶å‘)
# - å›å¤åˆ†é¡µè‡ªåŠ¨å¤„ç†
# - æ•°æ®åº“æŸ¥è¯¢ç»“æœç¼“å­˜
# - è·³è¿‡æ— æ–°å›å¤çš„ä¸»é¢˜

# #### é”™è¯¯å¤„ç†
# - å…¨é¢çš„å¼‚å¸¸æ•è·(SQLAlchemyErrorç­‰)
# - æ—¶é—´è§£æå¤±è´¥å¤„ç†
# - æ•°æ®åº“è¿æ¥å¤±è´¥å¤„ç†
# - è¯¦ç»†çš„æ—¥å¿—è®°å½•

# #### ä»£ç ç»“æ„
# - ä½¿ç”¨Scrapyæ ‡å‡†Spiderç»“æ„
# - æ¨¡å—åŒ–è®¾è®¡(æ•°æ®åº“æ“ä½œåˆ†ç¦»)
# - æ¸…æ™°çš„å›è°ƒæ–¹æ³•é“¾:
#   `parse â†’ parse_topic_list â†’ parse_replies`

# ### 3. å…³é”®æ–¹æ³•

# 1. **æ•°æ®åº“ç›¸å…³**:
#    - `_init_db`: åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
#    - `get_last_reply_from_db`: æŸ¥è¯¢ä¸»é¢˜æœ€åå›å¤æ—¶é—´
#    - `close`: æ¸…ç†èµ„æº

# 2. **çˆ¬å–é€»è¾‘**:
#    - `parse`: å…¥å£ç‚¹ï¼Œç”Ÿæˆä¸»é¢˜åˆ—è¡¨é¡µè¯·æ±‚
#    - `parse_topic_list`: è§£æä¸»é¢˜åˆ—è¡¨
#    - `parse_replies`: è§£æå›å¤å†…å®¹åŠåˆ†é¡µ
#    - `parse_user`: è§£æç”¨æˆ·ä¿¡æ¯(å½“å‰è¢«æ³¨é‡Š)

# 3. **å·¥å…·æ–¹æ³•**:
#    - `is_newer`: æ—¶é—´æ¯”è¾ƒ
#    - `_parse_nga_time`: æ—¶é—´æ ¼å¼è§£æ
#    - `_now_time`: è·å–å½“å‰æ—¶é—´

# ### 4. æ•°æ®æ¨¡å‹
# ä½¿ç”¨ä¸‰ä¸ªScrapy Item:
# - `TopicItem`: å­˜å‚¨ä¸»é¢˜ä¿¡æ¯
# - `ReplyItem`: å­˜å‚¨å›å¤ä¿¡æ¯
# - `UserItem`: å­˜å‚¨ç”¨æˆ·ä¿¡æ¯

# ### 5. é…ç½®å‚æ•°
# - `name = 'nga'`
# - `allowed_domains = ['bbs.nga.cn']`
# - `start_urls`: æ°´åŒºé¦–é¡µ
# - æ”¯æŒä»å‘½ä»¤è¡Œä¼ å…¥`db_url`

# ### 6. æ€§èƒ½è€ƒè™‘
# - æ•°æ®åº“ä¼šè¯ç®¡ç†(scoped_session)
# - å‡å°‘ä¸å¿…è¦çš„è¯·æ±‚(é€šè¿‡æ—¶é—´æ¯”è¾ƒ)
# - å¹¶å‘çˆ¬å–å¤šä¸ªä¸»é¢˜é¡µ
# - ç¼“å­˜æœºåˆ¶å‡å°‘æ•°æ®åº“æŸ¥è¯¢

# ### 7. å¾…ä¼˜åŒ–ç‚¹
# - ç”¨æˆ·ä¿¡æ¯çˆ¬å–å½“å‰è¢«æ³¨é‡Š
# - å¯æ·»åŠ è¯·æ±‚å»¶è¿Ÿæ§åˆ¶
# - å¯å¢åŠ ä»£ç†æ”¯æŒ
# - å¯æ·»åŠ æ›´è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯

import scrapy
from scrapy import Request
from ..items import TopicItem, ReplyItem, UserItem
from urllib.parse import parse_qs, urljoin
import time
from datetime import datetime
from sqlalchemy.orm import scoped_session
from sqlalchemy.exc import SQLAlchemyError
from ..models import Base  # ç¡®ä¿å¯¼å…¥Base
import psutil
import os

class NgaSpider(scrapy.Spider):
    name = 'nga'
    allowed_domains = ['bbs.nga.cn']
    start_urls = ['https://bbs.nga.cn/thread.php?fid=-7']

    def __init__(self, *args, **kwargs):
        super(NgaSpider, self).__init__(*args, **kwargs)
        # ä¸å†åœ¨å¯åŠ¨æ—¶æ¸…ç©ºæ—¥å¿—æ–‡ä»¶ï¼Œè®©Scrapyçš„æ—¥å¿—è½®è½¬æœºåˆ¶å¤„ç†
        # è°ƒåº¦å™¨éœ€è¦è¯»å–æ—¥å¿—æ–‡ä»¶è·å–ç»Ÿè®¡ä¿¡æ¯ï¼Œæ¸…ç©ºä¼šå¯¼è‡´æ•°æ®ä¸¢å¤±

        # ç¼“å­˜ä¸»é¢˜çš„æœ€æ–°å›å¤æ—¶é—´ï¼Œå‡å°‘æ•°æ®åº“æŸ¥è¯¢
        self.topic_last_reply_cache = {}
        # æ•°æ®åº“ç›¸å…³å±æ€§
        self.db_session = None
        self.db_url = kwargs.get('db_url')  # å…è®¸ä»å‘½ä»¤è¡Œä¼ å…¥db_url
        self.process = psutil.Process(os.getpid())  # åˆå§‹åŒ–ç›‘æ§
    
    @classmethod
    def from_crawler(cls, crawler, *args, **kwargs):
        spider = super(NgaSpider, cls).from_crawler(crawler, *args, **kwargs)
        # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        spider._init_db()
        return spider
    
    def _init_db(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥"""
        from ..utils.db_utils import create_db_session
        try:
            # ä½¿ç”¨scoped_sessionåŒ…è£…ï¼Œç¡®ä¿çº¿ç¨‹å®‰å…¨
            session_factory = create_db_session(self.db_url)
            if session_factory is None:
                raise RuntimeError("æ— æ³•åˆ›å»ºæ•°æ®åº“ä¼šè¯å·¥å‚")
            
            self.db_session = scoped_session(lambda: session_factory)
            self.logger.info("æ•°æ®åº“è¿æ¥åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            self.logger.error(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def close(self, reason):
        """çˆ¬è™«å…³é—­æ—¶æ¸…ç†èµ„æº"""
        if hasattr(self, 'db_session') and self.db_session:
            try:
                self.db_session.remove()
                self.logger.info("æ•°æ®åº“ä¼šè¯å·²å…³é—­")
            except Exception as e:
                self.logger.error(f"å…³é—­æ•°æ®åº“ä¼šè¯æ—¶å‡ºé”™: {e}")
        self.logger.info(f"å…³é—­çˆ¬è™«æ–¹å¼: {reason}")
        #super().close(reason)
    
    def print_stats(self):
        """æ‰“å°è¿›åº¦å’Œæ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        cpu = self.process.cpu_percent(interval=1)
        mem = self.process.memory_info().rss / 1024 / 1024
        self.logger.debug(f"ğŸ“Š CPU: {cpu}% | Memory: {mem:.2f} MB")

        # è·å–æ•°æ®åº“ç»Ÿè®¡
        if self.db_session:
            try:
                from ..models import Topic, Reply, User
                topic_count = self.db_session.query(Topic).count()
                reply_count = self.db_session.query(Reply).count()
                user_count = self.db_session.query(User).count()
                self.logger.debug(f"ğŸ“ˆ DBç»Ÿè®¡: ä¸»é¢˜={topic_count}, å›å¤={reply_count}, ç”¨æˆ·={user_count}")
            except Exception as e:
                self.logger.debug(f"âš ï¸ è·å–æ•°æ®åº“ç»Ÿè®¡å¤±è´¥: {e}")

    def parse(self, response):
        # è§£æä¸»é¢˜åˆ—è¡¨é¡µ
        pageNum = 11
        self.logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–NGAè®ºå›æ°´åŒºï¼Œå…±éœ€çˆ¬å– {pageNum-1} é¡µä¸»é¢˜åˆ—è¡¨")
        for page in range(1, pageNum):  # çˆ¬å–å‰pageNumé¡µ
            self.logger.debug(f"ğŸ“„ ç”Ÿæˆç¬¬ {page} é¡µä¸»é¢˜åˆ—è¡¨é¡µè¯·æ±‚")
            yield Request(
                url=f"https://bbs.nga.cn/thread.php?fid=-7&page={page}",
                callback=self.parse_topic_list,
                meta={'page': page}
            )

    def parse_topic_list(self, response):
        """ä¸¤é˜¶æ®µä¸»é¢˜åˆ—è¡¨è§£æï¼šé˜¶æ®µ1-æ”¶é›†æ‰€æœ‰ä¸»é¢˜ä¿¡æ¯"""
        # è§£æä¸»é¢˜åˆ—è¡¨
        page = response.meta.get('page', 'unknown')
        self.logger.debug(f"ğŸ“ å¼€å§‹è§£æç¬¬ {page} é¡µä¸»é¢˜åˆ—è¡¨ (URL: {response.url})")

        rows = response.xpath('//*[contains(@class, "topicrow")]')
        self.logger.debug(f"ğŸ“Š ç¬¬ {page} é¡µä¸»é¢˜åˆ—è¡¨å…±æ‰¾åˆ° {len(rows)} ä¸ªä¸»é¢˜")

        # é˜¶æ®µ1: æ”¶é›†æ‰€æœ‰ä¸»é¢˜ä¿¡æ¯
        topics_data = self._collect_topics_from_page(rows, page)

        if not topics_data:
            self.logger.debug(f"âš ï¸ ç¬¬ {page} é¡µæ²¡æœ‰æ”¶é›†åˆ°æœ‰æ•ˆä¸»é¢˜")
            return

        # é˜¶æ®µ2: æ‰¹é‡æŸ¥è¯¢æ•°æ®åº“ä¿¡æ¯
        all_tids = list(topics_data.keys())
        self.logger.info(f"ğŸ—„ï¸ [DBè°ƒè¯•] ç¬¬{page}é¡µ: å‡†å¤‡æŸ¥è¯¢{len(all_tids)}ä¸ªä¸»é¢˜çš„æ•°æ®åº“è®°å½•")
        db_info = self.batch_query_topics_from_db(all_tids)
        self.logger.info(f"ğŸ—„ï¸ [DBè°ƒè¯•] ç¬¬{page}é¡µ: æ•°æ®åº“è¿”å›{len(db_info)}æ¡è®°å½•, æ–°ä¸»é¢˜æ•°: {len(all_tids) - len(db_info)}")

        # é˜¶æ®µ3: æ™ºèƒ½å†³ç­–å“ªäº›ä¸»é¢˜éœ€è¦çˆ¬å–å›å¤
        topics_to_crawl, topics_to_skip = self._decide_topics_to_crawl(topics_data, db_info)
        self.logger.info(f"ğŸ—„ï¸ [DBè°ƒè¯•] ç¬¬{page}é¡µå†³ç­–ç»“æœ: éœ€çˆ¬å–{len(topics_to_crawl)}ä¸ª, è·³è¿‡{len(topics_to_skip)}ä¸ª")

        # é˜¶æ®µ4: æ‰¹é‡ç”Ÿæˆæ•°æ®é¡¹å’Œè¯·æ±‚
        for item in self._process_topics_batch(topics_to_crawl, topics_to_skip, db_info):
            yield item

        self.logger.debug(f"ğŸ“„ ç¬¬ {page} é¡µå¤„ç†å®Œæˆ: æ€»è®¡{len(topics_data)}ä¸ªä¸»é¢˜, "
                        f"çˆ¬å–{len(topics_to_crawl)}ä¸ª, è·³è¿‡{len(topics_to_skip)}ä¸ª")

    def _collect_topics_from_page(self, rows, page):
        """é˜¶æ®µ1: ä»é¡µé¢æ”¶é›†æ‰€æœ‰ä¸»é¢˜çš„åŸºç¡€ä¿¡æ¯"""
        topics_data = {}
        idx = 0

        for idx, row in enumerate(rows, 1):
            self.logger.debug(f"ğŸ” æ”¶é›†ç¬¬ {page} é¡µç¬¬ {idx} ä¸ªä¸»é¢˜ä¿¡æ¯")

            # æå–åŸºç¡€ä¿¡æ¯
            topic_link = row.xpath('.//a[contains(@class, "topic")]/@href').get()
            if not topic_link or 'tid=' not in topic_link:
                continue

            tid = topic_link.split('tid=')[1].split('&')[0]
            title = row.xpath('.//a[contains(@class, "topic")]/text()').get()
            if title == 'å¸–å­å‘å¸ƒæˆ–å›å¤æ—¶é—´è¶…è¿‡é™åˆ¶':
                continue

            poster_id = row.xpath('.//*[@class="author"]/@title').re_first(r'ç”¨æˆ·ID (\d+)')
            poster_name = row.xpath('.//*[@class="author"]/text()').get()
            post_time = row.xpath('.//span[contains(@class, "postdate")]/@title').get()
            re_num = row.xpath('.//*[@class="replies"]/text()').get()

            # å¦‚æœä¸»é¢˜å‘å¸ƒæ—¶é—´ä¸ºNoneï¼Œä½¿ç”¨å½“å‰æ—¶é—´
            if not post_time:
                post_time = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
                self.logger.debug(f"ğŸ•’ ä¸»é¢˜ {tid} æ— æ³•è·å–å‘å¸ƒæ—¶é—´ï¼Œä½¿ç”¨å½“å‰æ—¶é—´: {post_time}")

            # æå–æœ€åå›å¤æ—¶é—´ï¼ˆå¤šç§æ–¹å¼ï¼‰
            last_reply_date = self._extract_last_reply_date(row)

            # è·å–åˆ†åŒºä¿¡æ¯
            partition = 'æ°´åŒº'
            partition_el = row.xpath('.//td[@class="c2"]/span[@class="titleadd2"]/a[@class="silver"]/text()')
            if partition_el:
                partition = partition_el.get()

            # å­˜å‚¨ä¸»é¢˜ä¿¡æ¯
            topics_data[tid] = {
                'title': title,
                'poster_id': poster_id,
                'poster_name': poster_name,
                'post_time': post_time,
                're_num': re_num,
                'last_reply_date': last_reply_date,
                'partition': partition,
                'row_index': idx,
                'page': page
            }

        self.logger.debug(f"ğŸ“‹ ç¬¬ {page} é¡µæ”¶é›†å®Œæˆï¼Œå…±æ”¶é›† {len(topics_data)} ä¸ªæœ‰æ•ˆä¸»é¢˜")
        return topics_data

    def _extract_last_reply_date(self, row):
        """æå–æœ€åå›å¤æ—¶é—´çš„å¤šç§æ–¹å¼"""
        last_reply_date = None

        # æ–¹å¼1: ä» .replydate çš„ title å±æ€§æå–
        last_reply_date = row.xpath('.//a[contains(@class, "replydate")]/@title').get()

        # æ–¹å¼2: ä» .replydate çš„æ–‡æœ¬å†…å®¹æå–ï¼ˆç›¸å¯¹æ—¶é—´ï¼‰
        if not last_reply_date:
            alt_date = row.xpath('.//a[contains(@class, "replydate")]/text()').get()
            if alt_date and alt_date not in ['åˆšæ‰', 'ä»Šå¤©', 'æ˜¨å¤©', 'å‰å¤©']:
                last_reply_date = alt_date

        # æ–¹å¼3: æŸ¥æ‰¾æ‰€æœ‰æœ‰titleå±æ€§çš„å…ƒç´ ï¼Œç­›é€‰å‡ºæ—¶é—´æ ¼å¼çš„
        if not last_reply_date:
            time_candidates = row.xpath('.//*[@title and string-length(@title) > 8]/@title').getall()
            for candidate in time_candidates:
                if self._is_nga_time_format(candidate):
                    last_reply_date = candidate
                    break

        # æ–¹å¼4: ä½¿ç”¨æ­£åˆ™ä»æ•´è¡Œæ–‡æœ¬ä¸­æå–æ—¶é—´
        if not last_reply_date:
            row_text = row.xpath('string(.)').get()
            last_reply_date = self._extract_time_from_text(row_text)

        # å¦‚æœç½‘é¡µæ—¶é—´ä¸ºNoneï¼Œä½¿ç”¨å½“å‰æ—¶é—´ä½œä¸ºfallback
        if not last_reply_date:
            last_reply_date = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())

        return last_reply_date

    def _decide_topics_to_crawl(self, topics_data, db_info):
        """é˜¶æ®µ3: æ™ºèƒ½å†³ç­–å“ªäº›ä¸»é¢˜éœ€è¦çˆ¬å–å›å¤"""
        topics_to_crawl = []
        topics_to_skip = []

        for tid, topic_info in topics_data.items():
            web_last_reply = topic_info['last_reply_date']
            db_topic_info = db_info.get(tid, {})
            db_last_reply = db_topic_info.get('last_reply_date')

            # æ›´æ–°ç¼“å­˜
            self.topic_last_reply_cache[tid] = db_last_reply

            # å†³ç­–é€»è¾‘ï¼šæ˜¯å¦éœ€è¦çˆ¬å–è¯¥ä¸»é¢˜çš„å›å¤
            should_crawl = self._should_crawl_topic_replies(tid, web_last_reply, db_last_reply, topic_info, db_topic_info)

            if should_crawl:
                topics_to_crawl.append((tid, topic_info, db_last_reply))
                self.logger.debug(f"âœ… ä¸»é¢˜ {tid} éœ€è¦çˆ¬å–å›å¤ (ç½‘é¡µ:{web_last_reply}, æ•°æ®åº“:{db_last_reply})")
            else:
                topics_to_skip.append((tid, topic_info, db_last_reply))
                self.logger.debug(f"â­ï¸  ä¸»é¢˜ {tid} è·³è¿‡å›å¤çˆ¬å– (ç½‘é¡µ:{web_last_reply}, æ•°æ®åº“:{db_last_reply})")

        return topics_to_crawl, topics_to_skip

    def _should_crawl_topic_replies(self, tid, web_last_reply, db_last_reply, topic_info, db_topic_info=None):
        """åˆ¤æ–­æ˜¯å¦éœ€è¦çˆ¬å–ä¸»é¢˜çš„å›å¤"""
        # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰è®°å½•ï¼Œéœ€è¦çˆ¬å–
        if not db_last_reply:
            return True

        # å¦‚æœç½‘é¡µæ—¶é—´æ¯”æ•°æ®åº“æ—¶é—´æ–°ï¼Œéœ€è¦çˆ¬å–
        if web_last_reply and self.is_newer(web_last_reply, db_last_reply):
            return True

        # å¦‚æœå›å¤æ•°é‡æœ‰å˜åŒ–ï¼Œå¯èƒ½éœ€è¦çˆ¬å–ï¼ˆå¯é€‰çš„å¯å‘å¼åˆ¤æ–­ï¼‰
        web_re_num = topic_info.get('re_num', '0') or '0'
        db_re_num = str(db_topic_info.get('re_num', 0)) if db_topic_info and db_topic_info.get('re_num') else '0'
        if web_re_num and db_re_num and web_re_num != db_re_num:
            self.logger.debug(f"ğŸ”¢ ä¸»é¢˜ {tid} å›å¤æ•°å˜åŒ–: ç½‘é¡µ{web_re_num} vs æ•°æ®åº“{db_re_num}")
            return True

        return False

    def _process_topics_batch(self, topics_to_crawl, topics_to_skip, db_info):
        """é˜¶æ®µ4: æ‰¹é‡å¤„ç†æ‰€æœ‰ä¸»é¢˜ï¼Œç”Ÿæˆæ•°æ®é¡¹å’Œè¯·æ±‚"""
        reply_requests_count = 0
        total_count = len(topics_to_crawl)

        # å¤„ç†éœ€è¦çˆ¬å–çš„ä¸»é¢˜
        # å…³é”®ä¿®å¤ï¼šæ·»åŠ å°å»¶è¿Ÿæ§åˆ¶ç”Ÿæˆé€Ÿåº¦ï¼Œé¿å…é˜Ÿåˆ—æ‹¥å¡
        # ç”Ÿæˆé€Ÿåº¦å¿…é¡» <= å¤„ç†é€Ÿåº¦ï¼Œå¦åˆ™ä¼šå †ç§¯
        for i, (tid, topic_info, db_last_reply) in enumerate(topics_to_crawl):
            # æ¯ç”Ÿæˆ8ä¸ªè¯·æ±‚æš‚åœ0.5ç§’ï¼ˆä¸å¹¶å‘æ•°åŒ¹é…ï¼‰ï¼Œè®©Scrapyæœ‰æ—¶é—´å¤„ç†
            if i > 0 and i % 8 == 0:
                self.logger.info(f"â±ï¸ [èŠ‚æµ] å·²ç”Ÿæˆ{i}/{total_count}ä¸ªè¯·æ±‚ï¼Œæš‚åœ0.5ç§’è®©è°ƒåº¦å™¨å¤„ç†...")
                time.sleep(0.5)
            # ç”ŸæˆTopicItem
            topic_item = TopicItem(
                tid=tid,
                title=topic_info['title'],
                poster_id=topic_info['poster_id'],
                post_time=topic_info['post_time'],
                re_num=topic_info['re_num'],
                sampling_time=self._now_time(),
                last_reply_date=topic_info['last_reply_date'],
                partition=topic_info['partition']
            )
            yield topic_item

            # ç”ŸæˆUserItem
            if topic_info['poster_id']:
                user_item = UserItem(
                    uid=topic_info['poster_id'],
                    name=topic_info['poster_name'] or '',
                    user_group='',
                    reg_date='',
                    prestige='',
                    history_re_num=''
                )
                yield user_item

            # ç”Ÿæˆå›å¤é¡µè¯·æ±‚ï¼ˆå¹¶å‘ç”± Scrapy çš„ CONCURRENT_REQUESTS æ§åˆ¶ï¼‰
            reply_request = Request(
                url=f"https://bbs.nga.cn/read.php?tid={tid}&page=999",
                callback=self.parse_replies,
                meta={'tid': tid, 'db_last_reply': db_last_reply},
                priority=100,
                dont_filter=False
            )
            self.logger.debug(f"ğŸ”„ æ­£åœ¨yieldè¯·æ±‚ {tid}...")
            yield reply_request
            reply_requests_count += 1
            self.logger.debug(f"âœ… æˆåŠŸyieldè¯·æ±‚ {tid}ï¼Œè®¡æ•°: {reply_requests_count}/{total_count}")
            self.logger.debug(f"ğŸš€ ä¸»é¢˜ {tid}: å·²ç”Ÿæˆå›å¤é¡µè¯·æ±‚ (ç¬¬{i+1}/{total_count}ä¸ª)")
            
            # ã€è¯Šæ–­æ—¥å¿—ã€‘æ¯ç”Ÿæˆ10ä¸ªè¯·æ±‚æ£€æŸ¥ä¸€æ¬¡é˜Ÿåˆ—çŠ¶æ€
            if (i + 1) % 10 == 0:
                if hasattr(self.crawler.engine, 'scheduler') and hasattr(self.crawler.engine.scheduler, 'queue'):
                    queue_size = len(self.crawler.engine.scheduler.queue)
                    self.logger.info(f"ğŸ“Š [ç”Ÿæˆè¯·æ±‚é˜Ÿåˆ—è¯Šæ–­] å·²ç”Ÿæˆ{i+1}ä¸ªè¯·æ±‚ï¼Œå½“å‰è°ƒåº¦é˜Ÿåˆ—é•¿åº¦: {queue_size}")

        self.logger.info(f"ğŸ—„ï¸ [DBè°ƒè¯•] æ‰¹å¤„ç†å®Œæˆ: ç”Ÿæˆ{reply_requests_count}ä¸ªå›å¤é¡µè¯·æ±‚, è·³è¿‡{len(topics_to_skip)}ä¸ªä¸»é¢˜")

        # é˜Ÿåˆ—çŠ¶æ€ç›‘æ§ - å…³é”®è°ƒè¯•ä¿¡æ¯
        if hasattr(self.crawler.engine, 'scheduler') and hasattr(self.crawler.engine.scheduler, 'queue'):
            queue_size = len(self.crawler.engine.scheduler.queue)
            self.logger.info(f"ğŸ“Š [é˜Ÿåˆ—ç›‘æ§] å½“å‰è°ƒåº¦é˜Ÿåˆ—é•¿åº¦: {queue_size}, ç”Ÿæˆè¯·æ±‚æ€»æ•°: {reply_requests_count}")
            if queue_size > 100:
                self.logger.warning(f"âš ï¸ [é˜Ÿåˆ—æ‹¥å¡] é˜Ÿåˆ—é•¿åº¦({queue_size})è¶…è¿‡100ï¼Œå¯èƒ½å¯¼è‡´å¤„ç†å»¶è¿Ÿï¼")
        else:
            self.logger.warning("âš ï¸ æ— æ³•è·å–è°ƒåº¦å™¨é˜Ÿåˆ—çŠ¶æ€")

        # å¤„ç†è·³è¿‡çš„ä¸»é¢˜ï¼ˆåªç”ŸæˆTopicItemï¼Œä¸ç”Ÿæˆè¯·æ±‚ï¼‰
        for tid, topic_info, db_last_reply in topics_to_skip:
            # å³ä½¿è·³è¿‡å›å¤çˆ¬å–ï¼Œä¹Ÿè¦æ›´æ–°ä¸»é¢˜ä¿¡æ¯ï¼ˆä¿æŒæ•°æ®æ–°é²œåº¦ï¼‰
            topic_item = TopicItem(
                tid=tid,
                title=topic_info['title'],
                poster_id=topic_info['poster_id'],
                post_time=topic_info['post_time'],
                re_num=topic_info['re_num'],
                sampling_time=self._now_time(),
                last_reply_date=topic_info['last_reply_date'],
                partition=topic_info['partition']
            )
            yield topic_item

            self.logger.debug(f"ğŸ“ ä¸»é¢˜ {tid}: å·²æ›´æ–°ä¸»é¢˜ä¿¡æ¯ï¼ˆè·³è¿‡å›å¤çˆ¬å–ï¼‰")

    def get_last_reply_from_db(self, tid):
        """ä»æ•°æ®åº“è·å–ä¸»é¢˜çš„æœ€åå›å¤æ—¶é—´"""
        if not hasattr(self, 'db_session') or not self.db_session:
            self.logger.error("æ•°æ®åº“ä¼šè¯æœªåˆå§‹åŒ–")
            return None

        try:
            from ..models import Topic  # å±€éƒ¨å¯¼å…¥é¿å…å¾ªç¯å¼•ç”¨
            topic = self.db_session.query(Topic).filter_by(tid=tid).first()
            return topic.last_reply_date if topic else None
        except SQLAlchemyError as e:
            self.logger.error(f"æŸ¥è¯¢æ•°æ®åº“å‡ºé”™: {e}")
            return None
        except Exception as e:
            self.logger.error(f"è·å–æœ€åå›å¤æ—¶é—´æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
            return None

    def batch_query_topics_from_db(self, tids):
        """æ‰¹é‡æŸ¥è¯¢æ•°æ®åº“ä¸­å¤šä¸ªä¸»é¢˜çš„ä¿¡æ¯

        Args:
            tids: ä¸»é¢˜IDåˆ—è¡¨

        Returns:
            dict: {tid: {'last_reply_date': str, 'post_time': str, 're_num': int}}
        """
        if not hasattr(self, 'db_session') or not self.db_session:
            self.logger.error("æ•°æ®åº“ä¼šè¯æœªåˆå§‹åŒ–")
            return {}

        if not tids:
            return {}

        try:
            from ..models import Topic  # å±€éƒ¨å¯¼å…¥é¿å…å¾ªç¯å¼•ç”¨
            # æ‰¹é‡æŸ¥è¯¢ä¸»é¢˜ä¿¡æ¯
            topics = self.db_session.query(Topic).filter(Topic.tid.in_(tids)).all()

            result = {}
            for topic in topics:
                result[topic.tid] = {
                    'last_reply_date': topic.last_reply_date,
                    'post_time': topic.post_time,
                    're_num': topic.re_num
                }

            self.logger.debug(f"ğŸ—„ï¸ æ‰¹é‡æŸ¥è¯¢æ•°æ®åº“: æŸ¥è¯¢{len(tids)}ä¸ªä¸»é¢˜ï¼Œæ‰¾åˆ°{len(result)}ä¸ªè®°å½•")
            return result

        except SQLAlchemyError as e:
            self.logger.error(f"æ‰¹é‡æŸ¥è¯¢æ•°æ®åº“å‡ºé”™: {e}")
            return {}
        except Exception as e:
            self.logger.error(f"æ‰¹é‡æŸ¥è¯¢æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
            return {}

    # å…¶ä»–æ–¹æ³•ä¿æŒä¸å˜...
    def parse_replies(self, response):
        # ç«‹å³è®°å½•æ–¹æ³•è¢«è°ƒç”¨ï¼Œç”¨äºè°ƒè¯•
        self.logger.info(f"ğŸ¯ parse_repliesæ–¹æ³•è¢«è°ƒç”¨! URL: {response.url}, Status: {response.status}")
        
        # ã€è¯Šæ–­æ—¥å¿—ã€‘è®°å½•è°ƒåº¦é˜Ÿåˆ—çŠ¶æ€
        if hasattr(self.crawler.engine, 'scheduler') and hasattr(self.crawler.engine.scheduler, 'queue'):
            queue_size = len(self.crawler.engine.scheduler.queue)
            self.logger.info(f"ğŸ“Š [parse_repliesé˜Ÿåˆ—è¯Šæ–­] å½“å‰è°ƒåº¦é˜Ÿåˆ—é•¿åº¦: {queue_size}")

        tid = response.meta['tid']
        db_last_reply = response.meta.get('db_last_reply')
        current_page = response.meta.get('current_page', 'unknown')
        last_page = response.meta.get('last_page', 'unknown')

        self.logger.debug(f"ğŸ’¬ å¼€å§‹è§£æä¸»é¢˜ {tid} çš„å›å¤ (å½“å‰é¡µ: {current_page}/{last_page}, URL: {response.url})")

        meta={'tid': tid}

        if 'last_page' not in response.meta:
            last_page_link = response.xpath('//a[contains(@class, "invert") and @title="æœ€åé¡µ"]/@href').get()
            if last_page_link:
                last_page = int(parse_qs(last_page_link.split('?')[1]).get('page', [1])[0])
                #self.logger.info(f"æœ€åä¸€é¡µ{last_page}è·å–")
            else:
                page_links = [int(p.split('=')[-1]) for p in response.xpath('//a[contains(@href, "page=")]/@href').re(r'page=\d+')]
                last_page = max(page_links) if page_links else 1

            meta['last_page'] = last_page
            meta['current_page'] = last_page

        if 'current_page' in response.meta:
            meta['current_page'] = response.meta['current_page']

        new_page_flag=True

        replies = response.xpath('//*[@class="forumbox postbox"]')
        self.logger.debug(f"ğŸ“œ ä¸»é¢˜ {tid}: å½“å‰é¡µ {current_page}/{last_page} å…±æœ‰ {len(replies)} æ¡å›å¤")

        for idx, reply in enumerate(replies, 1):
            self.logger.debug(f"ğŸ“ ä¸»é¢˜ {tid}: å¼€å§‹å¤„ç†ç¬¬ {idx} æ¡å›å¤ (å½“å‰é¡µ {current_page}/{last_page})")
            post_id = reply.xpath('.//*[starts-with(@id, "postcontainer")]/a[1]/@id').get()

            # å®‰å…¨æ£€æŸ¥ï¼šå¦‚æœæ— æ³•è·å– post_idï¼Œè·³è¿‡è¯¥å›å¤
            if not post_id:
                self.logger.warning(f"æ— æ³•è·å– post_idï¼Œè·³è¿‡è¯¥å›å¤ (tid={tid})")
                continue

            # ç»Ÿä¸€ä½¿ç”¨çº¯æ•°å­—æ ¼å¼
            if post_id == 'pid0Anchor':
                # ä¸»æ¥¼ä½¿ç”¨ tid ä½œä¸º ridï¼Œçº¯æ•°å­—æ ¼å¼
                post_id = tid
            elif 'pid' in post_id and 'Anchor' in post_id:
                # æ™®é€šå›å¤ï¼šä» Anchor æ ¼å¼æå–çº¯æ•°å­—
                # ä¾‹å¦‚ï¼špid849526462Anchor â†’ 849526462
                post_id = post_id.replace('pid', '').replace('Anchor', '')
            elif post_id.isdigit():
                # å·²ç»æ˜¯çº¯æ•°å­—æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
                pass
            else:
                # å…¶ä»–æœªçŸ¥æ ¼å¼ï¼Œè®°å½•è­¦å‘Šå¹¶è·³è¿‡
                self.logger.warning(f"æœªçŸ¥çš„ post_id æ ¼å¼: {post_id} (tid={tid})")
                continue
                
            poster_href = reply.xpath('.//*[starts-with(@id, "postauthor")]/@href').get()
            poster_id = poster_href.split('uid=')[1].split('&')[0] if poster_href and 'uid=' in poster_href else ''
            poster_name = reply.xpath('.//*[starts-with(@id, "postauthor")]/text()').get()
            
            content = reply.xpath('.//*[starts-with(@id, "postcontent") '
                                 'and string-length(translate(substring(@id, 12), "0123456789", "")) = 0]/text()').get()
            
            recommendvalue = reply.xpath('.//span[contains(@class,"recommendvalue")]/text()').get('0')
            post_time = reply.xpath('.//*[starts-with(@id, "postdate")]/text()').get()

            # å¦‚æœå›å¤æ—¶é—´ä¸ºNoneï¼Œä½¿ç”¨å½“å‰æ—¶é—´
            if not post_time:
                post_time = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
                self.logger.debug(f"ğŸ•’ å›å¤ {post_id} æ— æ³•è·å–æ—¶é—´ï¼Œä½¿ç”¨å½“å‰æ—¶é—´: {post_time}")

            # å¦‚æœè®¾ç½®äº†æ•°æ®åº“æœ€åå›å¤æ—¶é—´ï¼Œä¸”å½“å‰å›å¤æ—¶é—´ä¸æ–°äºæ•°æ®åº“è®°å½•ï¼Œåˆ™è·³è¿‡
            if db_last_reply and not self.is_newer(post_time, db_last_reply):
                self.logger.debug(f"è·³è¿‡å›å¤ {post_id}ï¼Œå›å¤æ—¶é—´ {post_time} ä¸æ–°äºæ•°æ®åº“è®°å½• {db_last_reply}")
                new_page_flag=False
                continue
            
            parent_rid = None
            if reply.xpath('.//div[contains(@class, "quote")]'):
                quote_link = reply.xpath('.//a[contains(@title, "æ‰“å¼€é“¾æ¥")]/@href').get()
                if quote_link and 'pid=' in quote_link:
                    parent_rid = quote_link.split('pid=')[1].split('&')[0]
            
            # æ–°å¢å›¾ç‰‡URLæå–é€»è¾‘
            image_urls = []
            # æå–æ‰€æœ‰imgæ ‡ç­¾çš„srcå±æ€§ï¼ˆåŒ…å«data-srcorgå¤‡ç”¨ï¼‰
            for img in reply.xpath('.//img'):
                src = img.xpath('@src').get()
                if src and ('attachments' in src or 'smile' in src):
                    image_urls.append(src)

            reply_item = ReplyItem(
                rid=post_id,
                tid=tid,
                parent_rid=parent_rid,
                content=content,
                recommendvalue=recommendvalue,
                post_time=post_time,
                poster_id=poster_id,
                sampling_time=self._now_time(),
                image_urls=image_urls  # æ·»åŠ å›¾ç‰‡URLåˆ—è¡¨
            )
            self.logger.debug(f"âœ… ä¸»é¢˜ {tid}: æˆåŠŸæå–å›å¤ {post_id} (æ—¶é—´: {post_time}, ç”¨æˆ·: {poster_id}, æ¨èå€¼: {recommendvalue})")
            yield reply_item

            # åˆ›å»ºç”¨æˆ·ä¿¡æ¯ï¼ˆåªåŒ…å«åŸºæœ¬ä¿¡æ¯ï¼Œä¸å‘èµ·é¢å¤–è¯·æ±‚ï¼‰
            if poster_id:
                self.logger.debug(f"ğŸ‘¤ ä¸»é¢˜ {tid}: ä¸ºç”¨æˆ· {poster_id} ç”ŸæˆUserItem")
                user_item = UserItem(
                    uid=poster_id,
                    name=poster_name or '',
                    user_group='',
                    reg_date='',
                    prestige='',
                    history_re_num=''
                )
                yield user_item

        self.logger.debug(f"ğŸ“„ ä¸»é¢˜ {tid}: é¡µé¢ {current_page}/{last_page} è§£æå®Œæˆï¼Œå‡†å¤‡å¤„ç†ä¸Šä¸€é¡µ")
        # å¤„ç†ä¸Šä¸€é¡µ
        if new_page_flag and meta['current_page'] > 1:
            meta['current_page'] = meta['current_page'] - 1
            self.logger.debug(f"â¬…ï¸ ä¸»é¢˜ {tid}: ç¿»åˆ°ä¸Šä¸€é¡µ {meta['current_page']} é¡µ")
            yield Request(
                url=f"https://bbs.nga.cn/read.php?tid={tid}&page={meta['current_page']}",
                callback=self.parse_replies,
                meta=meta
            )
        else:
            self.logger.debug(f"âœ… ä¸»é¢˜ {tid}: æ‰€æœ‰å›å¤é¡µå¤„ç†å®Œæˆ")

    # å…¶ä»–æ–¹æ³•ä¿æŒä¸å˜...
    def parse_user(self, response):
        uid = response.meta['uid']
        user_group = response.xpath('//label[contains(text(), "ç”¨â€‚æˆ·â€‚ç»„")]/../span/span/text()').get('åŒ¿åç”¨æˆ·')
        reg_date = response.xpath('//label[contains(text(), "æ³¨å†Œæ—¥æœŸ")]/../span/text()').get()
        
        user_item=UserItem(
            uid=uid,
            user_group=user_group,
            reg_date=reg_date,
            prestige='',
            history_re_num='')
        yield user_item

    def is_newer(self, time1, time2):
        """æ¯”è¾ƒä¸¤ä¸ªæ—¶é—´å­—ç¬¦ä¸²ï¼Œåˆ¤æ–­time1æ˜¯å¦æ¯”time2æ–°"""
        try:
            # å¤„ç†NGAçš„æ—¶é—´æ ¼å¼å¯èƒ½ä¸ä¸€è‡´çš„æƒ…å†µ
            dt1 = self._parse_nga_time(time1)
            dt2 = self._parse_nga_time(time2)
            self.logger.debug(f"æ—¶é—´æ¯”è¾ƒ:  time1: {dt1}, time2: {dt2}ï¼Œç»“æœï¼š{dt1 >= dt2}")
            return dt1 >= dt2
        except Exception as e:
            self.logger.error(f"æ—¶é—´æ¯”è¾ƒé”™è¯¯: {e}, web_time: {time1}, db_time: {time2}")
            return True  # å¦‚æœè§£æå¤±è´¥ï¼Œé»˜è®¤å¤„ç†ä¸ºæ–°å›å¤

    def _parse_nga_time(self, time_str):
        """è§£æNGAçš„å„ç§æ—¶é—´æ ¼å¼"""
        if not time_str:
            return datetime.min
        
        # å°è¯•å¸¸è§æ ¼å¼ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
        formats = [
            '%Y-%m-%d %H:%M:%S',  # æ ‡å‡†æ ¼å¼ 2025-04-19 17:00:00
            '%y-%m-%d %H:%M:%S',  # ç®€å†™å¹´ä»½ 25-04-19 17:00:00
            '%d-%m-%y %H:%M:%S',  # æ—¥-æœˆ-å¹´ 19-04-25 17:00:00
            '%Y-%m-%d %H:%M',     # ç¼ºå°‘ç§’
            '%y-%m-%d %H:%M',     # ç®€å†™å¹´ä»½ç¼ºå°‘ç§’
            '%d-%m-%y %H:%M',     # æ—¥-æœˆ-å¹´ç¼ºå°‘ç§’
            '%m-%d %H:%M',        # ç¼ºå°‘å¹´å’Œç§’
            '%H:%M:%S',           # åªæœ‰æ—¶é—´
            '%H:%M'               # åªæœ‰å°æ—¶å’Œåˆ†é’Ÿ
        ]
        
        for fmt in formats:
            try:
                return datetime.strptime(time_str, fmt)
            except ValueError:
                continue
        
        # å¦‚æœéƒ½ä¸åŒ¹é…ï¼Œè¿”å›æœ€å°æ—¶é—´
        self.logger.warning(f"æ— æ³•è§£æçš„æ—¶é—´æ ¼å¼: {time_str}")
        return datetime.min


    def _is_nga_time_format(self, time_str):
        """æ£€æŸ¥å­—ç¬¦ä¸²æ˜¯å¦ä¸ºNGAæ—¶é—´æ ¼å¼"""
        if not time_str:
            return False
        import re
        # åŒ¹é… NGA æ—¶é—´æ ¼å¼: 25-11-30 15:59, 2025-11-30 15:59:30 ç­‰
        patterns = [
            r'\d{2,4}-\d{2}-\d{2}\s+\d{2}:\d{2}(?::\d{2})?',  # æ ‡å‡†æ ¼å¼
            r'\d{2,4}-\d{2}-\d{2}\s+\d{2}:\d{2}',             # ç®€åŒ–æ ¼å¼
        ]
        for pattern in patterns:
            if re.match(pattern, time_str.strip()):
                return True
        return False

    def _extract_time_from_text(self, text):
        """ä»æ–‡æœ¬ä¸­ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ—¶é—´"""
        if not text:
            return None
        import re
        # å¤šç§æ—¶é—´æ ¼å¼çš„æ­£åˆ™è¡¨è¾¾å¼
        patterns = [
            r'(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2})',  # 2025-11-30 15:59:30
            r'(\d{2}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2})',  # 25-11-30 15:59:30
            r'(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2})',        # 2025-11-30 15:59
            r'(\d{2}-\d{2}-\d{2}\s+\d{2}:\d{2})',        # 25-11-30 15:59
        ]
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(1)
        return None

    def _now_time(self):
        return time.strftime("%Y-%m-%d-%H-%M-%S", time.localtime())
